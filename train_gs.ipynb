{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Config and Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "def info(type, value, tb):\n",
    "    if hasattr(sys, 'ps1') or not sys.stderr.isatty():\n",
    "    # we are in interactive mode or we don't have a tty-like\n",
    "    # device, so we call the default hook\n",
    "        sys.__excepthook__(type, value, tb)\n",
    "    else:\n",
    "        import traceback, pdb\n",
    "        # we are NOT in interactive mode, print the exception...\n",
    "        traceback.print_exception(type, value, tb)\n",
    "        print\n",
    "        # ...then start the debugger in post-mortem mode.\n",
    "        # pdb.pm() # deprecated\n",
    "        pdb.post_mortem(tb) # more \"modern\"\n",
    "\n",
    "sys.excepthook = info\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "from typing import List\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "from omegaconf import DictConfig, OmegaConf, ListConfig\n",
    "import torch\n",
    "import run_seed_fn\n",
    "from helpers.utils import create_obs_config\n",
    "import lightning as L\n",
    "\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize(config_path=\"conf/\"):\n",
    "    cfg = compose(config_name='config.yaml')\n",
    "    method_cfg = compose(config_name='method/ManiGaussian_BC.yaml')\n",
    "    cfg.method = method_cfg.method\n",
    "    # print(cfg.method)\n",
    "    print(OmegaConf.to_yaml(cfg))\n",
    "    cfg_yaml = OmegaConf.to_yaml(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.rlbench.demo_path = 'data/train_data'\n",
    "cfg.rlbench.tasks = [\n",
    "    # 'open_drawer',\n",
    "    # 'close_jar',\n",
    "    # 'meat_off_grill',\n",
    "    # 'push_buttons',\n",
    "    # 'put_item_in_drawer',\n",
    "    # 'reach_and_drag',\n",
    "    'slide_block_to_color_target',\n",
    "    # 'stack_blocks',\n",
    "    # 'sweep_to_dustpan_of_size',\n",
    "    # 'turn_tap'\n",
    "    ]\n",
    "cfg.method.use_fabric = False\n",
    "cfg.framework.use_wandb = False\n",
    "cfg.method.use_wandb = False\n",
    "cfg.method.neural_renderer.use_dynamic_field = False\n",
    "cfg.method.neural_renderer.foundation_model_name = 'None' # diffusion\n",
    "lambda_arm = 0.4\n",
    "lambda_object = 0.4\n",
    "lambda_bg = 0.2\n",
    "# cfg.method.neural_renderer.mlp.max_sh_degree = 1\n",
    "# cfg.method.neural_renderer.mlp.d_hidden = 1024\n",
    "# cfg.method.neural_renderer.mlp.n_blocks = 5\n",
    "# cfg.method.neural_renderer.lambda_nerf = 0.01\n",
    "# cfg.method.neural_renderer.lambda_embed = 0.5\n",
    "\n",
    "cfg.method.neural_renderer.lambda_l1  = 0.8\n",
    "cfg.method.neural_renderer.lambda_ssim  = 0.2\n",
    "# cfg.method.optimizer = 'adam'\n",
    "# cfg.method.demo_augmentation_every_n = 5\n",
    "cfg.method.lr = 0.0005\n",
    "# cfg.method.lr_scheduler = False\n",
    "# cfg.replay.batch_size = 1\n",
    "\n",
    "cfg.rlbench.demos = 10\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# cfg.method.neural_renderer.visdom = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['MASTER_ADDR'] = cfg.ddp.master_addr\n",
    "os.environ['MASTER_PORT'] = str(cfg.ddp.master_port)\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "print(\"device available: \", torch.cuda.device_count())\n",
    "\n",
    "cfg.rlbench.cameras = cfg.rlbench.cameras \\\n",
    "    if isinstance(cfg.rlbench.cameras, ListConfig) else [cfg.rlbench.cameras]\n",
    "obs_config = create_obs_config(cfg.rlbench.cameras,\n",
    "                                cfg.rlbench.camera_resolution,\n",
    "                                cfg.method.name,\n",
    "                                use_depth=cfg.method.use_depth,\n",
    "                                )\n",
    "multi_task = len(cfg.rlbench.tasks) > 1\n",
    "\n",
    "cwd = os.getcwd()\n",
    "logging.info('CWD:' + os.getcwd())\n",
    "\n",
    "if cfg.framework.start_seed >= 0:\n",
    "    # seed specified\n",
    "    start_seed = cfg.framework.start_seed\n",
    "elif cfg.framework.start_seed == -1 and \\\n",
    "        len(list(filter(lambda x: 'seed' in x, os.listdir(cwd)))) > 0:\n",
    "    # unspecified seed; use largest existing seed plus one\n",
    "    largest_seed =  max([int(n.replace('seed', ''))\n",
    "                            for n in list(filter(lambda x: 'seed' in x, os.listdir(cwd)))])\n",
    "    start_seed = largest_seed + 1\n",
    "else:\n",
    "    # start with seed 0\n",
    "    start_seed = 0\n",
    "\n",
    "seed_folder = os.path.join(os.getcwd(), 'seed%d' % start_seed)\n",
    "os.makedirs(seed_folder, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(seed_folder, 'config.yaml'), 'w') as f:\n",
    "    f.write(cfg_yaml)\n",
    "\n",
    "# check if previous checkpoints already exceed the number of desired training iterations\n",
    "# if so, exit the script\n",
    "weights_folder = os.path.join(seed_folder, 'weights')\n",
    "if os.path.isdir(weights_folder) and len(os.listdir(weights_folder)) > 0:\n",
    "    weights = os.listdir(weights_folder)\n",
    "    latest_weight = sorted(map(int, weights))[-1]\n",
    "    if latest_weight >= cfg.framework.training_iterations:\n",
    "        logging.info('Agent was already trained for %d iterations. Exiting.' % latest_weight)\n",
    "        sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adapted from ARM\n",
    "# Source: https://github.com/stepjam/ARM\n",
    "# License: https://github.com/stepjam/ARM/LICENSE\n",
    "\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from rlbench.backend.observation import Observation\n",
    "from rlbench.observation_config import ObservationConfig\n",
    "import rlbench.utils as rlbench_utils\n",
    "from rlbench.demo import Demo\n",
    "from yarr.replay_buffer.prioritized_replay_buffer import ObservationElement\n",
    "from yarr.replay_buffer.replay_buffer import ReplayElement, ReplayBuffer\n",
    "from yarr.replay_buffer.uniform_replay_buffer import UniformReplayBuffer\n",
    "from yarr.replay_buffer.task_uniform_replay_buffer import TaskUniformReplayBuffer\n",
    "from yarr.replay_buffer.uniform_replay_buffer_single_process import UniformReplayBufferSingleProcess\n",
    "from helpers import demo_loading_utils, utils\n",
    "# from helpers.preprocess_agent import PreprocessAgent\n",
    "from helpers.clip.core.clip import tokenize\n",
    "from helpers.language_model import create_language_model\n",
    "\n",
    "# from agents.manigaussian_bc.perceiver_lang_io import PerceiverVoxelLangEncoder\n",
    "# from agents.manigaussian_bc.qattention_manigaussian_bc_agent import QAttentionPerActBCAgent\n",
    "# from agents.manigaussian_bc.qattention_stack_agent import QAttentionStackAgent\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import multiprocessing as mp\n",
    "from torch.multiprocessing import Process, Value, Manager\n",
    "from omegaconf import DictConfig\n",
    "from termcolor import colored, cprint\n",
    "from lightning.fabric import Fabric\n",
    "\n",
    "\n",
    "REWARD_SCALE = 100.0\n",
    "LOW_DIM_SIZE = 4\n",
    "\n",
    "def create_replay(batch_size: int, \n",
    "                  timesteps: int,\n",
    "                  prioritisation: bool, \n",
    "                  task_uniform: bool,\n",
    "                  save_dir: str, \n",
    "                  cameras: list,\n",
    "                  voxel_sizes,\n",
    "                  image_size=[128, 128],\n",
    "                  replay_size=3e5,\n",
    "                  single_process=False,\n",
    "                  cfg=None,):\n",
    "\n",
    "    trans_indicies_size = 3 * len(voxel_sizes)\n",
    "    rot_and_grip_indicies_size = (3 + 1)\n",
    "    gripper_pose_size = 7\n",
    "    ignore_collisions_size = 1\n",
    "    max_token_seq_len = 77\n",
    "    lang_feat_dim = 1024\n",
    "    lang_emb_dim = cfg.method.language_model_dim\n",
    "    cprint(f\"[create_replay] lang_emb_dim: {lang_emb_dim}\", \"green\")\n",
    "\n",
    "    num_view_for_nerf = cfg.rlbench.num_view_for_nerf\n",
    "    \n",
    "    # low_dim_state\n",
    "    observation_elements = []\n",
    "    observation_elements.append(\n",
    "        ObservationElement('low_dim_state', (LOW_DIM_SIZE,), np.float32))\n",
    "\n",
    "    # rgb, depth, point cloud, intrinsics, extrinsics\n",
    "    for cname in cameras:\n",
    "        observation_elements.append(\n",
    "            ObservationElement('%s_rgb' % cname, (3, *image_size,), np.float32))\n",
    "        observation_elements.append(\n",
    "            ObservationElement('%s_depth' % cname, (1, *image_size,), np.float32))\n",
    "        observation_elements.append(\n",
    "            ObservationElement('%s_point_cloud' % cname, (3, *image_size),\n",
    "                               np.float32))  # see pyrep/objects/vision_sensor.py on how pointclouds are extracted from depth frames\n",
    "        observation_elements.append(\n",
    "            ObservationElement('%s_mask' % cname, (1, *image_size), np.float32))\n",
    "        observation_elements.append(\n",
    "            ObservationElement('%s_camera_extrinsics' % cname, (4, 4,), np.float32))\n",
    "        observation_elements.append(\n",
    "            ObservationElement('%s_camera_intrinsics' % cname, (3, 3,), np.float32))\n",
    "\n",
    "    # for nerf img, exs, ins\n",
    "    observation_elements.append(\n",
    "        ObservationElement('nerf_multi_view_rgb', (num_view_for_nerf,), np.object_))\n",
    "    observation_elements.append(\n",
    "        ObservationElement('nerf_multi_view_depth', (num_view_for_nerf,), np.object_))\n",
    "    observation_elements.append(\n",
    "        ObservationElement('nerf_multi_view_camera', (num_view_for_nerf,), np.object_))\n",
    "    observation_elements.append(\n",
    "        ObservationElement('nerf_multi_view_mask', (num_view_for_nerf,), np.object_))\n",
    "    \n",
    "    # for next nerf\n",
    "    observation_elements.append(\n",
    "        ObservationElement('nerf_next_multi_view_rgb', (num_view_for_nerf,), np.object_))\n",
    "    observation_elements.append(\n",
    "        ObservationElement('nerf_next_multi_view_depth', (num_view_for_nerf,), np.object_))\n",
    "    observation_elements.append(\n",
    "        ObservationElement('nerf_next_multi_view_camera', (num_view_for_nerf,), np.object_))\n",
    "    observation_elements.append(\n",
    "        ObservationElement('nerf_next_multi_view_mask', (num_view_for_nerf,), np.object_))\n",
    "    \n",
    "    # discretized translation, discretized rotation, discrete ignore collision, 6-DoF gripper pose, and pre-trained language embeddings\n",
    "    observation_elements.extend([\n",
    "        ReplayElement('trans_action_indicies', (trans_indicies_size,),\n",
    "                      np.int32),\n",
    "        ReplayElement('rot_grip_action_indicies', (rot_and_grip_indicies_size,),\n",
    "                      np.int32),\n",
    "        ReplayElement('ignore_collisions', (ignore_collisions_size,),\n",
    "                      np.int32),\n",
    "        ReplayElement('gripper_pose', (gripper_pose_size,),\n",
    "                      np.float32),\n",
    "        ReplayElement('lang_goal_emb', (lang_feat_dim,),\n",
    "                      np.float32),\n",
    "        ReplayElement('lang_token_embs', (max_token_seq_len, lang_emb_dim,),\n",
    "                      np.float32), # extracted from CLIP's language encoder\n",
    "        ReplayElement('task', (),\n",
    "                      str),\n",
    "        ReplayElement('lang_goal', (1,),\n",
    "                      object),  # language goal string for debugging and visualization\n",
    "    ])\n",
    "\n",
    "    extra_replay_elements = [\n",
    "        ReplayElement('demo', (), np.bool),\n",
    "    ]\n",
    "    if not single_process:  # default: False\n",
    "        replay_buffer = TaskUniformReplayBuffer(\n",
    "            save_dir=save_dir,\n",
    "            batch_size=batch_size,\n",
    "            timesteps=timesteps,\n",
    "            replay_capacity=int(replay_size),\n",
    "            action_shape=(8,),\n",
    "            action_dtype=np.float32,\n",
    "            reward_shape=(),\n",
    "            reward_dtype=np.float32,\n",
    "            update_horizon=1,\n",
    "            observation_elements=observation_elements,\n",
    "            extra_replay_elements=extra_replay_elements,\n",
    "        )\n",
    "    else:\n",
    "        replay_buffer = UniformReplayBufferSingleProcess(\n",
    "            save_dir=save_dir,\n",
    "            batch_size=batch_size,\n",
    "            timesteps=timesteps,\n",
    "            replay_capacity=int(replay_size),\n",
    "            action_shape=(8,),\n",
    "            action_dtype=np.float32,\n",
    "            reward_shape=(),\n",
    "            reward_dtype=np.float32,\n",
    "            update_horizon=1,\n",
    "            observation_elements=observation_elements,\n",
    "            extra_replay_elements=extra_replay_elements\n",
    "        )\n",
    "    return replay_buffer\n",
    "\n",
    "\n",
    "def _get_action(\n",
    "        obs_tp1: Observation,\n",
    "        obs_tm1: Observation,\n",
    "        rlbench_scene_bounds: List[float], # metric 3D bounds of the scene\n",
    "        voxel_sizes: List[int],\n",
    "        bounds_offset: List[float],\n",
    "        rotation_resolution: int,\n",
    "        crop_augmentation: bool):\n",
    "    '''\n",
    "    obs_tp1: current observation\n",
    "    obs_tm1: previous observation\n",
    "    '''\n",
    "    quat = utils.normalize_quaternion(obs_tp1.gripper_pose[3:])\n",
    "    if quat[-1] < 0:\n",
    "        quat = -quat\n",
    "    disc_rot = utils.quaternion_to_discrete_euler(quat, rotation_resolution)\n",
    "    disc_rot = utils.correct_rotation_instability(disc_rot, rotation_resolution)\n",
    "\n",
    "    attention_coordinate = obs_tp1.gripper_pose[:3]\n",
    "    trans_indicies, attention_coordinates = [], []\n",
    "    bounds = np.array(rlbench_scene_bounds)\n",
    "    ignore_collisions = int(obs_tm1.ignore_collisions)\n",
    "    for depth, vox_size in enumerate(voxel_sizes): # only single voxelization-level is used in PerAct\n",
    "        if depth > 0:\n",
    "            if crop_augmentation:\n",
    "                shift = bounds_offset[depth - 1] * 0.75\n",
    "                attention_coordinate += np.random.uniform(-shift, shift, size=(3,))\n",
    "            bounds = np.concatenate([attention_coordinate - bounds_offset[depth - 1],\n",
    "                                     attention_coordinate + bounds_offset[depth - 1]])\n",
    "        index = utils.point_to_voxel_index(\n",
    "            obs_tp1.gripper_pose[:3], vox_size, bounds)\n",
    "        trans_indicies.extend(index.tolist())\n",
    "        res = (bounds[3:] - bounds[:3]) / vox_size\n",
    "        attention_coordinate = bounds[:3] + res * index\n",
    "        attention_coordinates.append(attention_coordinate)\n",
    "\n",
    "    rot_and_grip_indicies = disc_rot.tolist()\n",
    "    grip = float(obs_tp1.gripper_open)\n",
    "    rot_and_grip_indicies.extend([int(obs_tp1.gripper_open)])\n",
    "    return trans_indicies, rot_and_grip_indicies, ignore_collisions, np.concatenate(\n",
    "        [obs_tp1.gripper_pose, np.array([grip])]), attention_coordinates\n",
    "\n",
    "\n",
    "def _add_keypoints_to_replay(\n",
    "        cfg: DictConfig,\n",
    "        task: str,\n",
    "        replay: ReplayBuffer,\n",
    "        inital_obs: Observation,\n",
    "        demo: Demo,\n",
    "        episode_keypoints: List[int],\n",
    "        cameras: List[str],\n",
    "        rlbench_scene_bounds: List[float],\n",
    "        voxel_sizes: List[int],\n",
    "        bounds_offset: List[float],\n",
    "        rotation_resolution: int,\n",
    "        crop_augmentation: bool,\n",
    "        description: str = '',\n",
    "        language_model = None,\n",
    "        device = 'cpu'):\n",
    "    prev_action = None\n",
    "    obs = inital_obs    # initial observation is 0\n",
    "\n",
    "   \n",
    "    for k, keypoint in enumerate(episode_keypoints):    # demo[-1].nerf_multi_view_rgb is None\n",
    "        obs_tp1 = demo[keypoint]    # e.g, 44\n",
    "\n",
    "        obs_tm1 = demo[max(0, keypoint - 1)]    # previous observation, e.g., 43\n",
    "\n",
    "        trans_indicies, rot_grip_indicies, ignore_collisions, action, attention_coordinates = _get_action(\n",
    "            obs_tp1, obs_tm1, rlbench_scene_bounds, voxel_sizes, bounds_offset,\n",
    "            rotation_resolution, crop_augmentation)\n",
    "\n",
    "        terminal = (k == len(episode_keypoints) - 1)\n",
    "        reward = float(terminal) * REWARD_SCALE if terminal else 0\n",
    "\n",
    "        obs_dict = utils.extract_obs(obs, t=k, prev_action=prev_action,\n",
    "                                     cameras=cameras, episode_length=cfg.rlbench.episode_length,\n",
    "                                     next_obs=obs_tp1 if not terminal else obs_tm1,\n",
    "                                     )\n",
    "        # FIXME: better way to use the last sample for next frame prediction?\n",
    "        sentence_emb, token_embs = language_model.extract(description)\n",
    "\n",
    "        obs_dict['lang_goal_emb'] = sentence_emb[0].float().detach().cpu().numpy()\n",
    "        obs_dict['lang_token_embs'] = token_embs[0].float().detach().cpu().numpy()\n",
    "        obs_dict['lang_goal'] = np.array([description], dtype=object) # add this for usage in diffusion model\n",
    "\n",
    "        prev_action = np.copy(action)\n",
    "\n",
    "        others = {'demo': True}\n",
    "        final_obs = {\n",
    "            'trans_action_indicies': trans_indicies,\n",
    "            'rot_grip_action_indicies': rot_grip_indicies,\n",
    "            'gripper_pose': obs_tp1.gripper_pose,\n",
    "            'task': task,\n",
    "            'lang_goal': np.array([description], dtype=object),\n",
    "        }\n",
    "\n",
    "        others.update(final_obs)\n",
    "        others.update(obs_dict)\n",
    "\n",
    "        timeout = False\n",
    "        replay.add(action, reward, terminal, timeout, **others)\n",
    "        obs = obs_tp1\n",
    "        # break\n",
    "\n",
    "    \n",
    "    # final step\n",
    "    obs_dict_tp1 = utils.extract_obs(obs_tp1, t=k + 1, prev_action=prev_action,\n",
    "                                     cameras=cameras, episode_length=cfg.rlbench.episode_length,\n",
    "                                     next_obs=obs_tp1,  \n",
    "                                     )\n",
    "    # nerf_multi_view_rgb is None\n",
    "    obs_dict_tp1['lang_goal_emb'] = sentence_emb[0].float().detach().cpu().numpy()\n",
    "    obs_dict_tp1['lang_token_embs'] = token_embs[0].float().detach().cpu().numpy()\n",
    "    obs_dict_tp1['lang_goal'] = np.array([description], dtype=object) # add this for usage in diffusion model\n",
    "\n",
    "    obs_dict_tp1.pop('wrist_world_to_cam', None)\n",
    "    obs_dict_tp1.update(final_obs)\n",
    "    # check nerf data here. find None\n",
    "    replay.add_final(**obs_dict_tp1)\n",
    "\n",
    "\n",
    "def fill_replay(cfg: DictConfig,\n",
    "                obs_config: ObservationConfig,\n",
    "                rank: int,\n",
    "                replay: ReplayBuffer,\n",
    "                task: str,\n",
    "                num_demos: int,\n",
    "                demo_augmentation: bool,\n",
    "                demo_augmentation_every_n: int,\n",
    "                cameras: List[str],\n",
    "                rlbench_scene_bounds: List[float],  # AKA: DEPTH0_BOUNDS\n",
    "                voxel_sizes: List[int],\n",
    "                bounds_offset: List[float],\n",
    "                rotation_resolution: int,\n",
    "                crop_augmentation: bool,\n",
    "                language_model = None,\n",
    "                device = 'cpu',\n",
    "                keypoint_method = 'heuristic'):\n",
    "    logging.getLogger().setLevel(cfg.framework.logging_level)        \n",
    "\n",
    "    logging.debug('Filling %s replay ...' % task)\n",
    "    for d_idx in range(num_demos):\n",
    "        # load demo from disk\n",
    "        demo = rlbench_utils.get_stored_demos(\n",
    "            amount=1, image_paths=False,\n",
    "            dataset_root=cfg.rlbench.demo_path,\n",
    "            variation_number=-1, task_name=task,\n",
    "            obs_config=obs_config,\n",
    "            random_selection=False,\n",
    "            from_episode_number=d_idx)\n",
    "        \n",
    "        if demo == -1:\n",
    "            continue\n",
    "        \n",
    "        demo = demo[0]\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        descs = demo._observations[0].misc['descriptions']\n",
    "        # print(descs)\n",
    "\n",
    "        # extract keypoints (a.k.a keyframes)\n",
    "        episode_keypoints = demo_loading_utils.keypoint_discovery(demo, method=keypoint_method)\n",
    "\n",
    "        if rank == 0:   # always 0\n",
    "            logging.info(f\"Loading Demo({d_idx}) - found {len(episode_keypoints)} keypoints - {task}\")\n",
    "\n",
    "        for i in range(len(demo) - 1):\n",
    "            if not demo_augmentation and i > 0:\n",
    "                break\n",
    "            if i % demo_augmentation_every_n != 0:\n",
    "                continue\n",
    "\n",
    "            obs = demo[i]\n",
    "           \n",
    "\n",
    "            desc = descs[0]\n",
    "            # if our starting point is past one of the keypoints, then remove it\n",
    "            while len(episode_keypoints) > 0 and i >= episode_keypoints[0]:\n",
    "                episode_keypoints = episode_keypoints[1:]\n",
    "            if len(episode_keypoints) == 0:\n",
    "                break\n",
    "            \n",
    "            \n",
    "            _add_keypoints_to_replay(\n",
    "                cfg, task, replay, obs, demo, episode_keypoints, cameras,\n",
    "                rlbench_scene_bounds, voxel_sizes, bounds_offset,\n",
    "                rotation_resolution, crop_augmentation, description=desc,\n",
    "                language_model=language_model, device=device)\n",
    "            \n",
    "            # break\n",
    "    logging.debug('Replay %s filled with demos.' % task)\n",
    "\n",
    "\n",
    "def fill_multi_task_replay(cfg: DictConfig,\n",
    "                           obs_config: ObservationConfig,\n",
    "                           rank: int,   # non-sense\n",
    "                           replay: ReplayBuffer,\n",
    "                           tasks: List[str],\n",
    "                           num_demos: int,\n",
    "                           demo_augmentation: bool,\n",
    "                           demo_augmentation_every_n: int,\n",
    "                           cameras: List[str],\n",
    "                           rlbench_scene_bounds: List[float],\n",
    "                           voxel_sizes: List[int],\n",
    "                           bounds_offset: List[float],\n",
    "                           rotation_resolution: int,\n",
    "                           crop_augmentation: bool,\n",
    "                           keypoint_method = 'heuristic',\n",
    "                           fabric: Fabric = None):\n",
    "    manager = Manager()\n",
    "    store = manager.dict()\n",
    "\n",
    "    # create a MP dict for storing indicies\n",
    "    # TODO(mohit): this shouldn't be initialized here\n",
    "    if hasattr(replay, '_task_idxs'):\n",
    "        del replay._task_idxs\n",
    "    task_idxs = manager.dict()\n",
    "\n",
    "    replay._task_idxs = task_idxs\n",
    "    replay._create_storage(store)\n",
    "    replay.add_count = Value('i', 0)\n",
    "\n",
    "    # fill replay buffer in parallel across tasks\n",
    "    max_parallel_processes = cfg.replay.max_parallel_processes\n",
    "    processes = []\n",
    "    n = np.arange(len(tasks))\n",
    "    split_n = utils.split_list(n, max_parallel_processes)\n",
    "    \n",
    "    device = fabric.device if fabric is not None else None\n",
    "    language_model = create_language_model(name=cfg.method.language_model, device=device)\n",
    "\n",
    "    for split in split_n:\n",
    "        for e_idx, task_idx in enumerate(split):\n",
    "            task = tasks[int(task_idx)]\n",
    "            model_device = torch.device('cuda:%s' % (e_idx % torch.cuda.device_count())\n",
    "                                        if torch.cuda.is_available() else 'cpu')    # NOT USED\n",
    "            \n",
    "            # print(cameras)\n",
    "            fill_replay(cfg,\n",
    "                        obs_config,\n",
    "                        rank,\n",
    "                        replay,\n",
    "                        task,\n",
    "                        num_demos,\n",
    "                        demo_augmentation,\n",
    "                        demo_augmentation_every_n,\n",
    "                        cameras,\n",
    "                        rlbench_scene_bounds,\n",
    "                        voxel_sizes,\n",
    "                        bounds_offset,\n",
    "                        rotation_resolution,\n",
    "                        crop_augmentation,\n",
    "                        language_model,\n",
    "                        model_device,\n",
    "                        keypoint_method)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import logging\n",
    "from typing import List\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "from rlbench import CameraConfig, ObservationConfig\n",
    "from yarr.replay_buffer.wrappers.pytorch_replay_buffer import PyTorchReplayBuffer\n",
    "from yarr.runners.offline_train_runner import OfflineTrainRunner\n",
    "from yarr.utils.stat_accumulator import SimpleAccumulator\n",
    "\n",
    "from helpers.custom_rlbench_env import CustomRLBenchEnv, CustomMultiTaskRLBenchEnv\n",
    "import torch.distributed as dist\n",
    "\n",
    "from termcolor import cprint\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "rank = 0\n",
    "cams = cfg.rlbench.cameras\n",
    "seed = 0\n",
    "world_size = cfg.ddp.num_devices\n",
    "fabric = None\n",
    "task = cfg.rlbench.tasks[0]\n",
    "tasks = cfg.rlbench.tasks\n",
    "\n",
    "replay_path = os.path.join(cfg.replay.path, 'seed%d' % seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "replay_buffer = create_replay(\n",
    "    cfg.replay.batch_size, \n",
    "    cfg.replay.timesteps,\n",
    "    cfg.replay.prioritisation,\n",
    "    cfg.replay.task_uniform,\n",
    "    replay_path if cfg.replay.use_disk else None,\n",
    "    cams, \n",
    "    cfg.method.voxel_sizes,\n",
    "    cfg.rlbench.camera_resolution,\n",
    "    single_process = True,\n",
    "    cfg=cfg)\n",
    "\n",
    "fill_multi_task_replay(\n",
    "    cfg, obs_config, 0,\n",
    "    replay_buffer, tasks, cfg.rlbench.demos,\n",
    "    cfg.method.demo_augmentation, cfg.method.demo_augmentation_every_n,\n",
    "    cams, cfg.rlbench.scene_bounds,\n",
    "    cfg.method.voxel_sizes, cfg.method.bounds_offset,\n",
    "    cfg.method.rotation_resolution, cfg.method.crop_augmentation,\n",
    "    keypoint_method=cfg.method.keypoint_method,\n",
    "    fabric=fabric,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "wrapped_replay = PyTorchReplayBuffer(replay_buffer, num_workers=cfg.framework.num_workers)\n",
    "stat_accum = SimpleAccumulator(eval_video_fps=30)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "weightsdir = os.path.join(cwd, 'seed%d' % seed, 'weights')  # load from the last checkpoint\n",
    "\n",
    "logdir = os.path.join(cwd, 'seed%d' % seed)\n",
    "\n",
    "cprint(f'Project path: {weightsdir}', 'cyan')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from yarr.agents.agent import Agent, ActResult, ScalarSummary, \\\n",
    "    HistogramSummary, ImageSummary, Summary\n",
    "from termcolor import colored, cprint\n",
    "import io\n",
    "\n",
    "from helpers.utils import visualise_voxel\n",
    "from voxel.voxel_grid import VoxelGrid\n",
    "from voxel.augmentation import apply_se3_augmentation_with_camera_pose\n",
    "from helpers.clip.core.clip import build_model, load_clip\n",
    "import PIL.Image as Image\n",
    "import transformers\n",
    "from helpers.optim.lamb import Lamb\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# from agents.manigaussian_bc.neural_rendering import NeuralRenderer\n",
    "# from agents.manigaussian_bc.utils import visualize_pcd\n",
    "from helpers.language_model import create_language_model\n",
    "from helpers.network_utils import DenseBlock, SpatialSoftmax3D, Conv3DBlock, \\\n",
    "            Conv3DUpsampleBlock, MultiLayer3DEncoderShallow\n",
    "import wandb\n",
    "import visdom\n",
    "from lightning.fabric import Fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural_rendering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast as autocast\n",
    "import torch.autograd.profiler as profiler\n",
    "from scipy.spatial.transform import Rotation\n",
    "import os\n",
    "import os.path as osp\n",
    "import warnings\n",
    "from termcolor import colored, cprint\n",
    "\n",
    "from agents.manigaussian_bc.utils import PositionalEncoding, visualize_pcd\n",
    "from agents.manigaussian_bc.resnetfc import ResnetFC\n",
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import visdom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class GSPointCloudRegresser(nn.Module):\n",
    "    def __init__(self, cfg, out_channels, bias, scale):\n",
    "        '''\n",
    "        for weight initialization\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.cfg = cfg\n",
    "        self.activation = torch.nn.functional.softplus\n",
    "        self.out = nn.Linear(\n",
    "            in_features=sum(out_channels),\n",
    "            out_features=sum(out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.out(self.activation(x, beta=100))\n",
    "\n",
    "class GeneralizableGSEmbedNet(nn.Module):\n",
    "    def __init__(self, cfg, with_gs_render=True):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.with_gs_render = with_gs_render\n",
    "\n",
    "        self.coordinate_bounds = cfg.coordinate_bounds # default: [-0.3, -0.5, 0.6, 0.7, 0.5, 1.6]\n",
    "        print(colored(f\"[GeneralizableNeRFEmbedNet] coordinate_bounds: {self.coordinate_bounds}\", \"red\"))\n",
    "    \n",
    "        self.use_xyz = cfg.use_xyz\n",
    "        d_in = 3 if self.use_xyz else 1\n",
    "\n",
    "        self.use_code = cfg.use_code\n",
    "        if self.use_code and d_in > 0:\n",
    "            # Positional encoding for x,y,z\n",
    "            self.code = PositionalEncoding.from_conf(cfg[\"code\"], d_in=d_in)\n",
    "            d_in = self.code.d_out  # 39\n",
    "\n",
    "        self.d_in = d_in\n",
    "\n",
    "        self.image_shape = (cfg.image_height, cfg.image_width)\n",
    "        self.num_objs = 0\n",
    "        self.num_views_per_obj = 1\n",
    "\n",
    "        split_dimensions, scale_inits, bias_inits = self._get_splits_and_inits(cfg)\n",
    "\n",
    "        # backbone\n",
    "        self.d_latent = d_latent = cfg.d_latent # 128\n",
    "        self.d_lang = d_lang = cfg.d_lang   # 128\n",
    "        self.d_out = sum(split_dimensions)\n",
    "\n",
    "        self.encoder = ResnetFC(\n",
    "                d_in=d_in, # xyz t\n",
    "                d_latent=d_latent,  # volumetric representation\n",
    "                d_lang=d_lang, \n",
    "                d_out=self.d_out, \n",
    "                d_hidden=cfg.mlp.d_hidden, \n",
    "                n_blocks=cfg.mlp.n_blocks, \n",
    "                combine_layer=cfg.mlp.combine_layer,\n",
    "                beta=cfg.mlp.beta, use_spade=cfg.mlp.use_spade,\n",
    "            )\n",
    "        \n",
    "        self.gs_parm_regresser = GSPointCloudRegresser(\n",
    "            cfg,\n",
    "            split_dimensions,\n",
    "            scale=scale_inits,\n",
    "            bias=bias_inits,\n",
    "            )\n",
    "        self.scaling_activation = torch.exp\n",
    "        # self.scaling_activation = torch.nn.functional.softplus\n",
    "        self.opacity_activation = torch.sigmoid\n",
    "        self.rotation_activation = torch.nn.functional.normalize    # [B, N, 4]\n",
    "        self.max_sh_degree = cfg.mlp.max_sh_degree\n",
    "\n",
    "        # we move xyz, rot\n",
    "        self.use_dynamic_field = cfg.use_dynamic_field\n",
    "        self.warm_up = cfg.next_mlp.warm_up\n",
    "        self.use_action = cfg.next_mlp.use_action\n",
    "        cprint(f\"[GeneralizableGSEmbedNet] Using dynamic field: {self.use_dynamic_field}\", \"red\")\n",
    "        if self.use_dynamic_field:\n",
    "            self.use_semantic_feature = (cfg.foundation_model_name == 'diffusion')\n",
    "            cprint(f\"[GeneralizableGSEmbedNet] Using action input: {self.use_action}\", \"red\")\n",
    "            cprint(f\"[GeneralizableGSEmbedNet] Using semantic feature: {self.use_semantic_feature}\", \"red\")\n",
    "            next_d_in = self.d_out + self.d_in\n",
    "            next_d_in = next_d_in + 8 if self.use_action else next_d_in  # action: 8 dim\n",
    "            next_d_in = next_d_in if self.use_semantic_feature else next_d_in - 3\n",
    "            self.gs_deformation_field = ResnetFC(\n",
    "                    d_in=next_d_in, # all things despite volumetric representation (26 + 39 + 8 -3 = 70)\n",
    "                    d_latent=self.d_latent,\n",
    "                    d_lang=self.d_lang,\n",
    "                    d_out=3 + 4,    # xyz, rot\n",
    "                    d_hidden=cfg.next_mlp.d_hidden, \n",
    "                    n_blocks=cfg.next_mlp.n_blocks, \n",
    "                    combine_layer=cfg.next_mlp.combine_layer,\n",
    "                    beta=cfg.next_mlp.beta, use_spade=cfg.next_mlp.use_spade,\n",
    "                )\n",
    "\n",
    "    def _get_splits_and_inits(self, cfg):\n",
    "        '''Gets channel split dimensions and last layer initialization\n",
    "        Credit: https://github.com/szymanowiczs/splatter-image/blob/main/scene/gaussian_predictor.py\n",
    "        '''\n",
    "        split_dimensions = []\n",
    "        scale_inits = []\n",
    "        bias_inits = []\n",
    "        split_dimensions = split_dimensions + [3, 1, 3, 4, 3, 3]\n",
    "        scale_inits = scale_inits + [\n",
    "            cfg.mlp.xyz_scale,\n",
    "            cfg.mlp.opacity_scale,\n",
    "            cfg.mlp.scale_scale,\n",
    "            1.0,    # rotation\n",
    "            5.0,    # feature_dc\n",
    "            1.0,    # feature\n",
    "            ]\n",
    "        bias_inits = [\n",
    "            cfg.mlp.xyz_bias, \n",
    "            cfg.mlp.opacity_bias,\n",
    "            np.log(cfg.mlp.scale_bias),\n",
    "            0.0,\n",
    "            0.0,\n",
    "            0.0,\n",
    "            ]\n",
    "        if cfg.mlp.max_sh_degree != 0:    # default: 1\n",
    "            sh_num = (self.cfg.mlp.max_sh_degree + 1) ** 2 - 1    # 3\n",
    "            sh_num_rgb = sh_num * 3\n",
    "            split_dimensions.append(sh_num_rgb)\n",
    "            scale_inits.append(0.0)\n",
    "            bias_inits.append(0.0)\n",
    "        self.split_dimensions_with_offset = split_dimensions\n",
    "        return split_dimensions, scale_inits, bias_inits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def world_to_canonical(self, xyz):\n",
    "        \"\"\"\n",
    "        :param xyz (B, N, 3) or (B, 3, N)\n",
    "        :return (B, N, 3) or (B, 3, N)\n",
    "\n",
    "        transform world coordinate to canonical coordinate with bounding box [0, 1]\n",
    "        \"\"\"\n",
    "        xyz = xyz.clone()\n",
    "        bb_min = self.coordinate_bounds[:3]\n",
    "        bb_max = self.coordinate_bounds[3:]\n",
    "        bb_min = torch.tensor(bb_min, device=xyz.device).unsqueeze(0).unsqueeze(0) if xyz.shape[-1] == 3 \\\n",
    "            else torch.tensor(bb_min, device=xyz.device).unsqueeze(-1).unsqueeze(0)\n",
    "        bb_max = torch.tensor(bb_max, device=xyz.device).unsqueeze(0).unsqueeze(0) if xyz.shape[-1] == 3 \\\n",
    "            else torch.tensor(bb_max, device=xyz.device).unsqueeze(-1).unsqueeze(0)\n",
    "        xyz -= bb_min\n",
    "        xyz /= (bb_max - bb_min)\n",
    "\n",
    "        return xyz\n",
    "\n",
    "    def sample_in_canonical_voxel(self, xyz, voxel_feat):   # USED\n",
    "        \"\"\"\n",
    "        :param xyz (B, 3)\n",
    "        :param self.voxel_feat: [B, 128, 20, 20, 20]\n",
    "        :return (B, Feat)\n",
    "        \"\"\"\n",
    "        xyz_voxel_space = xyz.clone()\n",
    "\n",
    "        xyz_voxel_space = xyz_voxel_space * 2 - 1.0 # [0,1]->[-1,1]\n",
    "\n",
    "        # unsqueeze the point cloud to also have 5 dim\n",
    "        xyz_voxel_space = xyz_voxel_space.unsqueeze(1).unsqueeze(1)\n",
    "        # xyz_voxel_space: [bs, 1, 1, N, 3]\n",
    "        # print(f'xyz_voxel_space:{xyz_voxel_space.shape}')\n",
    "        # print(f'voxel_feat:{voxel_feat.shape}')\n",
    "        # sample in voxel space\n",
    "        # get the voxel feat correspond to xyz coordinate\n",
    "        point_feature = F.grid_sample(voxel_feat, xyz_voxel_space, align_corners=True, mode='bilinear')\n",
    "        # [bs, 128, 1, 1, N]\n",
    "        # squeeze back to point cloud shape \n",
    "        point_feature = point_feature.squeeze(2).squeeze(2).permute(0, 2, 1) \n",
    "        # [bs, N, 128]\n",
    "\n",
    "        return point_feature\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        SB is batch size\n",
    "        N is batch of points\n",
    "        NS is number of input views\n",
    "\n",
    "        Predict gaussian parameter maps\n",
    "        \"\"\"\n",
    "        # print('model embed')\n",
    "        # print(data['xyz'].shape)\n",
    "        SB, N, _ = data['xyz'].shape # (1,16384,3)\n",
    "        # print(f\"data['xyz']: {data['xyz'].shape}\")\n",
    "        \n",
    "        NS = self.num_views_per_obj # 1\n",
    "\n",
    "        canon_xyz = self.world_to_canonical(data['xyz'])    # [1,N,3], min:-2.28, max:1.39\n",
    "        # print(f\"canon_xyz: {canon_xyz.shape}\")\n",
    "        # volumetric sampling\n",
    "        point_latent = self.sample_in_canonical_voxel(canon_xyz, data['dec_fts']) # [bs, N, 128]->[bs, 128, N]\n",
    "        point_latent = point_latent.reshape(-1, self.d_latent)  # (SB * NS * B, latent)  [N, 128]\n",
    "        # print(f\"point_latent: {point_latent.shape}\")\n",
    "        if self.use_xyz:    # True\n",
    "            z_feature = canon_xyz.reshape(-1, 3)  # (SB*B, 3)\n",
    "\n",
    "        if self.use_code:    # True\n",
    "            # Positional encoding (no viewdirs)\n",
    "            z_feature = self.code(z_feature)    # [N, 39]\n",
    "\n",
    "        # add timestep\n",
    "        # print(f'point_latent: {point_latent.shape}')\n",
    "        # print(f'z_feature: {z_feature.shape}')\n",
    "        # print(f\"data['time']: {data['time'].shape} {data['time']}\")\n",
    "        # latent = torch.cat((point_latent, z_feature, data['time'].repeat(16384,1)), dim=-1) # [N, 128+39+1] \n",
    "        latent = torch.cat((point_latent, z_feature), dim=-1) # [N, 128+39] \n",
    "\n",
    "\n",
    "        # print(f'latent before encoder: {latent.shape}')\n",
    "        # Camera frustum culling stuff, currently disabled\n",
    "        combine_index = None\n",
    "        dim_size = None\n",
    "        # backbone\n",
    "        latent, _ = self.encoder(\n",
    "            latent,\n",
    "            combine_inner_dims=(self.num_views_per_obj, N),\n",
    "            combine_index=combine_index,\n",
    "            dim_size=dim_size,\n",
    "            language_embed=data['lang'],\n",
    "            batch_size=SB,\n",
    "            )   # 26\n",
    "\n",
    "        latent = latent.reshape(-1, N, self.d_out)  # [1, N, d_out]\n",
    "        # print(f'latent after encoder: {latent.shape}')\n",
    "        ## regress gaussian parms\n",
    "        split_network_outputs = self.gs_parm_regresser(latent) # [1, N, (3, 1, 3, 4, 3, 9)]\n",
    "        # split_network_outputs = latent\n",
    "        # print(f'split_network_outputs: {split_network_outputs.shape}')\n",
    "        # print(f'self.split_dimensions_with_offset: {self.split_dimensions_with_offset}')\n",
    "        \n",
    "        split_network_outputs = split_network_outputs.split(self.split_dimensions_with_offset, dim=-1) #[3, 1, 3, 4, 3, 3, 9]\n",
    "        \n",
    "        xyz_maps, opacity_maps, scale_maps, rot_maps, features_dc_maps, feature_maps = split_network_outputs[:6]\n",
    "\n",
    "        # xyz_maps: torch.Size([1, 16384, 3])\n",
    "        # opacity_maps: torch.Size([1, 16384, 1])\n",
    "        # scale_maps: torch.Size([1, 16384, 3])\n",
    "        # rot_maps: torch.Size([1, 16384, 4])\n",
    "        # features_dc_maps: torch.Size([1, 16384, 3])\n",
    "        # feature_maps: torch.Size([1, 16384, 3])\n",
    "        # features_rest_maps: torch.Size([1, 16384, 9])\n",
    "        if self.max_sh_degree > 0:\n",
    "            features_rest_maps = split_network_outputs[6]\n",
    "\n",
    "\n",
    "        # spherical function head\n",
    "        features_dc_maps = features_dc_maps.unsqueeze(2) #.transpose(2, 1).contiguous().unsqueeze(2) # [B, H*W, 1, 3]\n",
    "        features_rest_maps = features_rest_maps.reshape(*features_rest_maps.shape[:2], -1, 3) # [B, H*W, 3, 3]\n",
    "        sh_out = torch.cat([features_dc_maps, features_rest_maps], dim=2)  # [B, H*W, 4, 3]\n",
    "\n",
    "        scale_maps = self.scaling_activation(scale_maps)    # exp\n",
    "        scale_maps = torch.clamp_max(scale_maps, 0.05)\n",
    "\n",
    "        data['xyz_maps'] = data['xyz'] + xyz_maps   # [B, N, 3]\n",
    "        data['sh_maps'] = sh_out    # [B, N, 4, 3]\n",
    "        data['rot_maps'] = self.rotation_activation(rot_maps, dim=-1)\n",
    "        data['scale_maps'] = scale_maps\n",
    "        data['opacity_maps'] = self.opacity_activation(opacity_maps)\n",
    "        data['feature_maps'] = feature_maps # [B, N, 3]\n",
    "\n",
    "        # Dynamic Modeling: predict next gaussian maps\n",
    "        if self.use_dynamic_field: #and data['step'] >= self.warm_up:\n",
    "\n",
    "            if not self.use_semantic_feature:\n",
    "                # dyna_input: (d_latent, d_in)\n",
    "                dyna_input = torch.cat((\n",
    "                    point_latent,   # [N, 128]\n",
    "                    data['xyz_maps'].detach().reshape(N, 3), \n",
    "                    features_dc_maps.detach().reshape(N, 3),\n",
    "                    features_rest_maps.detach().reshape(N, 9),\n",
    "                    data['rot_maps'].detach().reshape(N, 4),\n",
    "                    data['scale_maps'].detach().reshape(N, 3),\n",
    "                    data['opacity_maps'].detach().reshape(N, 1),\n",
    "                    # d_in:\n",
    "                    z_feature,\n",
    "                ), dim=-1) # no batch dim\n",
    "            else:\n",
    "                dyna_input = torch.cat((\n",
    "                    point_latent,   # [N, 128]\n",
    "                    data['xyz_maps'].detach().reshape(N, 3), \n",
    "                    features_dc_maps.detach().reshape(N, 3),\n",
    "                    features_rest_maps.detach().reshape(N, 9),\n",
    "                    data['rot_maps'].detach().reshape(N, 4),\n",
    "                    data['scale_maps'].detach().reshape(N, 3),\n",
    "                    data['opacity_maps'].detach().reshape(N, 1),\n",
    "                    data['feature_maps'].detach().reshape(N, 3),\n",
    "                    # d_in:\n",
    "                    z_feature,  \n",
    "                ), dim=-1) # no batch dim\n",
    "\n",
    "            # voxel embedding, stop gradient (gaussian xyz), (128+39)+3=170\n",
    "            if self.use_action:\n",
    "                dyna_input = torch.cat((dyna_input, data['action'].repeat(N, 1)), dim=-1)   # action detach\n",
    "\n",
    "            # print('----------gs_deformation_field------------')\n",
    "            # print('Input----------------------------')\n",
    "            # print(f'dyna_input: {dyna_input.shape}')\n",
    "            # print(f'combine_inner_dims: {(self.num_views_per_obj, N)}')\n",
    "            # print(f'combine_index: {combine_index}')\n",
    "            # print(f'dim_size: {dim_size}')\n",
    "            # print(f'batch_size: {SB}')\n",
    "            # Input----------------------------\n",
    "            # dyna_input: torch.Size([16384, 201])\n",
    "            # combine_inner_dims: (1, 16384)\n",
    "            # combine_index: None\n",
    "            # dim_size: None\n",
    "            # batch_size: 1\n",
    "            # Output-----------------------------\n",
    "            # next_xyz_maps: torch.Size([1, 16384, 3])\n",
    "            # next_rot_maps: torch.Size([1, 16384, 4])\n",
    "            next_split_network_outputs, _ = self.gs_deformation_field( # delta xyz,rot\n",
    "                dyna_input,\n",
    "                combine_inner_dims=(self.num_views_per_obj, N),\n",
    "                combine_index=combine_index,\n",
    "                dim_size=dim_size,\n",
    "                language_embed=data['lang'],\n",
    "                batch_size=SB,\n",
    "                )\n",
    "            \n",
    "\n",
    "            next_xyz_maps, next_rot_maps = next_split_network_outputs.split([3, 4], dim=-1)\n",
    "            # print('Output-----------------------------')\n",
    "            # print(f'next_xyz_maps: {next_xyz_maps.shape}')\n",
    "            # print(f'next_rot_maps: {next_rot_maps.shape}')\n",
    "            # next_xyz_maps: torch.Size([1, 16384, 3])\n",
    "            # next_rot_maps: torch.Size([1, 16384, 4])\n",
    "\n",
    "\n",
    "            data['next']['xyz_maps'] = data['xyz_maps'].detach() + next_xyz_maps\n",
    "            data['next']['sh_maps'] = data['sh_maps'].detach()\n",
    "            data['next']['rot_maps'] = self.rotation_activation(data['rot_maps'].detach() + next_rot_maps, dim=-1)\n",
    "            data['next']['scale_maps'] = data['scale_maps'].detach()\n",
    "            data['next']['opacity_maps'] = data['opacity_maps'].detach()\n",
    "            data['next']['feature_maps'] = data['feature_maps'].detach()\n",
    "\n",
    "        return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralRenderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd.profiler as profiler\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from termcolor import colored, cprint\n",
    "from dotmap import DotMap\n",
    "\n",
    "# import agents.manigaussian_bc.utils as utils\n",
    "# from agents.manigaussian_bc.models_embed import GeneralizableGSEmbedNet\n",
    "from agents.manigaussian_bc.loss import l1_loss, l2_loss, cosine_loss, ssim\n",
    "from agents.manigaussian_bc.graphics_utils import getWorld2View2, getProjectionMatrix, focal2fov\n",
    "from agents.manigaussian_bc.gaussian_renderer import render\n",
    "\n",
    "import visdom\n",
    "import logging\n",
    "import einops\n",
    "\n",
    "\n",
    "def PSNR_torch(img1, img2, max_val=1, mask=None):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return torch.tensor(100.0).to(img1.device)\n",
    "    PIXEL_MAX = max_val\n",
    "    return 20 * torch.log10(PIXEL_MAX / torch.sqrt(mse))\n",
    "\n",
    "\n",
    "class NeuralRenderer(nn.Module):\n",
    "    \"\"\"\n",
    "    take a voxel, camera pose, and camera intrinsics as input,\n",
    "    and output a rendered image\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        super(NeuralRenderer, self).__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.coordinate_bounds = cfg.coordinate_bounds # bounds of voxel grid\n",
    "        self.W = cfg.image_width\n",
    "        self.H = cfg.image_height\n",
    "        self.bg_color = cfg.dataset.bg_color\n",
    "\n",
    "        self.znear = cfg.dataset.znear\n",
    "        self.zfar = cfg.dataset.zfar\n",
    "        self.trans = cfg.dataset.trans # default: [0, 0, 0]\n",
    "        self.scale = cfg.dataset.scale\n",
    "\n",
    "        # gs regressor\n",
    "        self.gs_model = GeneralizableGSEmbedNet(cfg, with_gs_render=True)\n",
    "        print(colored(\"[NeuralRenderer] GeneralizableGSEmbedNet is build\", \"cyan\"))\n",
    "\n",
    "        self.model_name = cfg.foundation_model_name\n",
    "        self.d_embed = cfg.d_embed\n",
    "        self.loss_embed_fn = cfg.loss_embed_fn\n",
    "        # print(f'model name: {self.model_name}')\n",
    "        if self.model_name == \"diffusion\":\n",
    "            try:\n",
    "                from odise.modeling.meta_arch.ldm import LdmFeatureExtractor\n",
    "            except:\n",
    "                from odise.modeling.meta_arch.ldm import LdmFeatureExtractor\n",
    "                \n",
    "            import torchvision.transforms as T\n",
    "            self.feature_extractor = LdmFeatureExtractor(\n",
    "                            encoder_block_indices=(5, 7),\n",
    "                            unet_block_indices=(2, 5, 8, 11),\n",
    "                            decoder_block_indices=(2, 5),\n",
    "                            steps=(0,),\n",
    "                            captioner=None,\n",
    "                        )\n",
    "            self.diffusion_preprocess = T.Resize(512, antialias=True)\n",
    "            cprint(\"diffusion feature dims: \"+str(self.feature_extractor.feature_dims), \"yellow\")\n",
    "        elif self.model_name == \"dinov2\":\n",
    "            from agents.manigaussian_bc.dino_extractor import VitExtractor\n",
    "            import torchvision.transforms as T\n",
    "            self.feature_extractor = VitExtractor(\n",
    "                model_name='dinov2_vitl14',\n",
    "            )\n",
    "            self.dino_preprocess = T.Compose([\n",
    "                T.Resize(224 * 8, antialias=True),  # must be a multiple of 14\n",
    "            ])\n",
    "            cprint(\"dinov2 feature dims: \"+str(self.feature_extractor.feature_dims), \"yellow\")\n",
    "        else:\n",
    "            cprint(f\"foundation model {self.model_name} is not implemented\", \"yellow\")\n",
    "\n",
    "        self.lambda_embed = cfg.lambda_embed\n",
    "        print(colored(f\"[NeuralRenderer] foundation model {self.model_name} is build. loss weight: {self.lambda_embed}\", \"cyan\"))\n",
    "\n",
    "        self.lambda_rgb = 1.0 if cfg.lambda_rgb is None else cfg.lambda_rgb\n",
    "        print(colored(f\"[NeuralRenderer] rgb loss weight: {self.lambda_rgb}\", \"cyan\"))\n",
    "\n",
    "        self.use_dynamic_field = cfg.use_dynamic_field\n",
    "\n",
    "    def _embed_loss_fn(self, render_embed, gt_embed):\n",
    "        \"\"\"\n",
    "        render_embed: [bs, h, w, 3]\n",
    "        gt_embed: [bs, h, w, 3]\n",
    "        \"\"\"\n",
    "        if self.loss_embed_fn == \"l2_norm\":\n",
    "            # label normalization\n",
    "            MIN_DENOMINATOR = 1e-12\n",
    "            gt_embed = (gt_embed - gt_embed.min()) / (gt_embed.max() - gt_embed.min() + MIN_DENOMINATOR)\n",
    "            loss_embed = l2_loss(render_embed, gt_embed)\n",
    "        elif self.loss_embed_fn == \"l2\":\n",
    "            loss_embed = l2_loss(render_embed, gt_embed)\n",
    "        elif self.loss_embed_fn == \"cosine\":\n",
    "            loss_embed = cosine_loss(render_embed, gt_embed)\n",
    "        else:\n",
    "            cprint(f\"loss_embed_fn {self.loss_embed_fn} is not implemented\", \"yellow\")\n",
    "        return loss_embed\n",
    "\n",
    "    def _save_gradient(self, name):\n",
    "        \"\"\"\n",
    "        for debugging language feature rendering\n",
    "        \"\"\"\n",
    "        def hook(grad):\n",
    "            print(f\"name={name}, grad={grad}\")\n",
    "            return grad\n",
    "        return hook\n",
    "\n",
    "    def extract_foundation_model_feature(self, gt_rgb, lang_goal):\n",
    "        \"\"\"\n",
    "        we use the last layer of the diffusion feature extractor\n",
    "        since we reshape 128x128 img to 512x512, the last layer's feature is just 128x128\n",
    "        thus, no need to resize the feature map\n",
    "        lang_goal: numpy.ndarray, [bs, 1, 1]\n",
    "        \"\"\"\n",
    "        # print(f'gt_rgb: {gt_rgb}')\n",
    "        # print(f'lang_goal: {lang_goal}')\n",
    "        if self.model_name == \"diffusion\":\n",
    "            \"\"\"\n",
    "            we support multiple captions for batched input here\n",
    "            \"\"\"\n",
    "            if lang_goal.shape[0] > 1:\n",
    "                caption = ['a robot arm ' + cap.item() for cap in lang_goal]\n",
    "            else:\n",
    "                caption = \"a robot arm \" + lang_goal.item()\n",
    "            batched_input = {'img': self.diffusion_preprocess(gt_rgb.permute(0, 3, 1, 2)), 'caption': caption}\n",
    "            feature_list, lang_embed = self.feature_extractor(batched_input) # list of visual features, and 77x768 language embedding\n",
    "            # print(feature_list)\n",
    "            used_feature_idx = -1  \n",
    "            gt_embed = feature_list[used_feature_idx]   # [bs,512,128,128]\n",
    "\n",
    "            # NOTE: dimensionality reduction with PCA, which is used to satisfy the output dimension of the Gaussian Renderer\n",
    "            bs = gt_rgb.shape[0]\n",
    "            A = gt_embed.reshape(bs, 512, -1).permute(0, 2, 1)  # [bs, 128*128, 512]\n",
    "            gt_embed_list = []\n",
    "            for i in range(bs):\n",
    "                U, S, V = torch.pca_lowrank(A[i], q=np.maximum(6, self.d_embed))\n",
    "                reconstructed_embed = torch.matmul(A[i], V[:, :self.d_embed])\n",
    "                gt_embed_list.append(reconstructed_embed)\n",
    "\n",
    "            gt_embed = torch.stack(gt_embed_list, dim=0).permute(0, 2, 1).reshape(bs, self.d_embed, 128, 128)\n",
    "            return gt_embed\n",
    "        \n",
    "        elif self.model_name == \"dinov2\":\n",
    "            batched_input = self.dino_preprocess(gt_rgb.permute(0, 3, 1, 2))    # resize\n",
    "            feature = self.feature_extractor(batched_input)\n",
    "            gt_embed = F.interpolate(feature, size=(128, 128), mode='bilinear', align_corners=False)    # [b, 1024, 128, 128]\n",
    "\n",
    "            # NOTE: dimensionality reduction with PCA, which is used to satisfy the output dimension of the Gaussian Renderer\n",
    "            bs = gt_rgb.shape[0]\n",
    "            A = gt_embed.reshape(bs, 1024, -1).permute(0, 2, 1)  # [bs, 128*128, 1024]\n",
    "            gt_embed_list = []\n",
    "            for i in range(bs):\n",
    "                U, S, V = torch.pca_lowrank(A[i], q=np.maximum(6, self.d_embed))\n",
    "                reconstructed_embed = torch.matmul(A[i], V[:, :self.d_embed])\n",
    "                gt_embed_list.append(reconstructed_embed)\n",
    "            gt_embed = torch.stack(gt_embed_list, dim=0).permute(0, 2, 1).reshape(bs, self.d_embed, 128, 128)\n",
    "            return gt_embed\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def encode_data(self, pcd, dec_fts, lang, \n",
    "                    rgb=None, depth=None, mask=None,\n",
    "                    tgt_pose=None, tgt_intrinsic=None, \n",
    "                    next_tgt_pose=None, next_tgt_intrinsic=None,\n",
    "                    focal=None, c=None, lang_goal=None, action=None, step=None, proprio = None,\n",
    "                    mask_value=None):\n",
    "        '''prepare data dict'''\n",
    "        bs = pcd.shape[0]\n",
    "        data = {}\n",
    "        # format input\n",
    "        data['img'] = rgb\n",
    "        data['dec_fts'] = dec_fts\n",
    "        data['depth'] = depth\n",
    "        data['lang'] = lang\n",
    "        data['action'] = action\n",
    "        data['step'] = step\n",
    "\n",
    "        # novel pose\n",
    "        data['novel_view'] = {}\n",
    "        data['intr'] = tgt_intrinsic\n",
    "        data['extr'] = tgt_pose\n",
    "        \n",
    "        data['xyz'] = einops.rearrange(pcd, 'b c h w -> b (h w) c')\n",
    "\n",
    "        pcd = einops.rearrange(pcd, 'b c h w -> b h w c')\n",
    "        mask = mask.reshape(-1,128,128)\n",
    "\n",
    "    \n",
    "        # arm_mask_value = [31,34,35,39,40,41,42,43,44,45,46]\n",
    "        object_mask = torch.zeros(1,128,128).to(device=device)\n",
    "\n",
    "        # mask 1,3,128,128\n",
    "        # for obj in mask_value:\n",
    "        #     _mask = mask.clone()\n",
    "        #     _mask[_mask!=obj] = 0\n",
    "        #     _mask[_mask==obj] = 255\n",
    "        #     object_mask+=_mask\n",
    "            \n",
    "        # object_mask = object_mask==255\n",
    "        # data['xyz'] = pcd[:,object_mask[0],:]\n",
    "        \n",
    "        \n",
    "        # add timestep\n",
    "        # proprio (B,4)\n",
    "        data['time'] = proprio[...,-1] # get only time step\n",
    "\n",
    "        # use extrinsic pose to generate gaussain parameters\n",
    "        if data['intr'] is not None:\n",
    "            data_novel = self.get_novel_calib(data)\n",
    "            data['novel_view'].update(data_novel)\n",
    "\n",
    "        if self.use_dynamic_field:\n",
    "            data['next'] = {\n",
    "                'extr': next_tgt_pose,\n",
    "                'intr': next_tgt_intrinsic,\n",
    "                'novel_view': {},\n",
    "            }\n",
    "            if data['next']['intr'] is not None:\n",
    "                data_novel = self.get_novel_calib(data['next'])\n",
    "                data['next']['novel_view'].update(data_novel)\n",
    "\n",
    "        return data\n",
    "    def sim_to_colmap(self,extrinsics):\n",
    "        R = extrinsics[:3, :3]\n",
    "        t = extrinsics[:3, 3]\n",
    "\n",
    "        # Invert the rotation matrix (transpose, since it's orthogonal)\n",
    "        R_inv = R.T\n",
    "        # R_inv = np.linalg.inv(R)\n",
    "        # print(R_inv)\n",
    "        # print(R.T)\n",
    "        # # Compute the new translation vector\n",
    "        # # t_inv = -np.dot(R_inv, t)\n",
    "        # print(t)\n",
    "        R_flip = Rotation.from_euler('z', 180, degrees=True).as_matrix()\n",
    "        R_inv = R_flip @ R_inv\n",
    "        \n",
    "        t_inv = -R_inv @ t\n",
    "        # print('t_inv')\n",
    "        # print(t_inv)\n",
    "        # print('RT')\n",
    "        # print(-R.T@t)\n",
    "\n",
    "\n",
    "\n",
    "        # Form the inverted transformation matrix\n",
    "        extrinsics = np.eye(4)\n",
    "        # print(extrinsics)\n",
    "        extrinsics[:3, :3] = R_inv\n",
    "        extrinsics[:3, 3] = t_inv\n",
    "        \n",
    "        return extrinsics\n",
    "    def get_novel_calib(self, data):\n",
    "        \"\"\"\n",
    "        get readable camera state for gaussian renderer from gt_pose\n",
    "        :param data: dict\n",
    "        :param data['intr']: intrinsic matrix\n",
    "        :param data['extr']: c2w matrix\n",
    "\n",
    "        :return: dict\n",
    "        \"\"\"\n",
    "\n",
    "        bs = data['intr'].shape[0]\n",
    "        device = data['intr'].device\n",
    "        fovx_list, fovy_list, world_view_transform_list, full_proj_transform_list, camera_center_list = [], [], [], [], []\n",
    "        for i in range(bs):\n",
    "            intr = data['intr'][i, ...].cpu().numpy()\n",
    "            extr = data['extr'][i, ...].cpu().numpy()\n",
    "            extr = np.linalg.inv(extr)  # the saved extrinsic is actually cam2world matrix, so turn it to world2cam matrix\n",
    "            # extr = self.sim_to_colmap(extr)  # colmap's extrinsic is\n",
    "            width, height = self.W, self.H\n",
    "            R = np.array(extr[:3, :3], np.float32).reshape(3, 3).transpose(1, 0)    # inverse\n",
    "            T = np.array(extr[:3, 3], np.float32)\n",
    "            FovX = focal2fov(intr[0, 0], width)\n",
    "            FovY = focal2fov(intr[1, 1], height)\n",
    "            # print(f\"FovX:{FovX}\")\n",
    "            # print(f\"FovY:{FovY}\")\n",
    "            projection_matrix = getProjectionMatrix(znear=self.znear, zfar=self.zfar, K=intr, h=height, w=width).transpose(0, 1)\n",
    "            world_view_transform = torch.tensor(getWorld2View2(R, T, np.array(self.trans), self.scale)).transpose(0, 1) # [4, 4], w2c\n",
    "            full_proj_transform = (world_view_transform.unsqueeze(0).bmm(projection_matrix.unsqueeze(0))).squeeze(0)    # [4, 4]\n",
    "            camera_center = world_view_transform.inverse()[3, :3]   # inverse is c2w\n",
    "\n",
    "            fovx_list.append(FovX)\n",
    "            fovy_list.append(FovY)\n",
    "            world_view_transform_list.append(world_view_transform.unsqueeze(0))\n",
    "            full_proj_transform_list.append(full_proj_transform.unsqueeze(0))\n",
    "            camera_center_list.append(camera_center.unsqueeze(0))\n",
    "\n",
    "        novel_view_data = {\n",
    "            'FovX': torch.FloatTensor(np.array(fovx_list)).to(device),\n",
    "            'FovY': torch.FloatTensor(np.array(fovy_list)).to(device),\n",
    "            'width': torch.tensor([width] * bs).to(device),\n",
    "            'height': torch.tensor([height] * bs).to(device),\n",
    "            'world_view_transform': torch.concat(world_view_transform_list).to(device),\n",
    "            'full_proj_transform': torch.concat(full_proj_transform_list).to(device),\n",
    "            'camera_center': torch.concat(camera_center_list).to(device),\n",
    "        }\n",
    "\n",
    "        return novel_view_data\n",
    "\n",
    "    def forward(self,rgb=None, depth=None, pcd=None, mask=None,\n",
    "                dec_fts=None, language=None,  \n",
    "                gt_rgb=None, gt_mask=None, gt_pose=None, gt_intrinsic=None, \n",
    "                next_gt_pose=None, next_gt_intrinsic=None, next_gt_rgb=None, next_gt_mask=None,\n",
    "                camera_intrinsics=None, camera_extrinsics=None, \n",
    "                focal=None, c=None, lang_goal=None, gt_depth=None,\n",
    "                step=None, action=None, proprio = None,\n",
    "                training=True):\n",
    "        '''\n",
    "        main forward function\n",
    "        Return:\n",
    "        :loss_dict: dict, loss values\n",
    "        :ret_dict: dict, rendered images\n",
    "        '''\n",
    "\n",
    "        # print(f'gt_mask:{gt_mask}')\n",
    "\n",
    "        \n",
    "        bs = rgb.shape[0]\n",
    "\n",
    "\n",
    "        data = self.encode_data(\n",
    "            rgb=rgb, depth=depth, pcd=pcd, mask=mask, focal=focal, c=c, lang_goal=None, tgt_pose=gt_pose, tgt_intrinsic=gt_intrinsic,\n",
    "            dec_fts=dec_fts, lang=language, next_tgt_pose=next_gt_pose, next_tgt_intrinsic=next_gt_intrinsic, \n",
    "            action=action, step=step, proprio = proprio,\n",
    "        )\n",
    "\n",
    "        render_novel = None\n",
    "        next_render_novel = None\n",
    "        render_embed = None\n",
    "        gt_embed = None\n",
    "\n",
    "        # create gt feature from foundation models\n",
    "        with torch.no_grad():\n",
    "            gt_embed = self.extract_foundation_model_feature(gt_rgb, lang_goal)\n",
    "\n",
    "        # if gt_rgb is not None:\n",
    "        if training:\n",
    "            # Gaussian Generator\n",
    "            data = self.gs_model(data)\n",
    "            # print('before render')\n",
    "            # for k,v in data.items():\n",
    "            #     if isinstance(v,torch.Tensor):\n",
    "            #         print(k,v.shape)\n",
    "            #     elif isinstance(v,dict):\n",
    "            #         print(k,v.keys())\n",
    "            \n",
    "            \n",
    "            # Gaussian Render\n",
    "            data = self.pts2render(data, bg_color=self.bg_color) # default: [0, 0, 0]\n",
    "\n",
    "            # Loss\n",
    "            render_novel = data['novel_view']['img_pred'].permute(0, 2, 3, 1)   # [1, 128, 128, 3]\n",
    "            # print(f'render_novel: {render_novel.shape}')\n",
    "            # plt.imshow(render_novel[0].detach().cpu().numpy())\n",
    "            # plt.show()\n",
    "            # if self.cfg.visdom:\n",
    "            #     vis = visdom.Visdom()\n",
    "            rgb_vis = data['img'][0].detach().cpu().numpy() * 0.5 + 0.5\n",
    "            # plt.imshow(rgb_vis)\n",
    "            # plt.show()\n",
    "            # print(f'rgb_vis: {rgb_vis.shape}')\n",
    "            # plt.imshow(rgb_vis.transpose(1,2,0))\n",
    "            # plt.show()\n",
    "                # vis.image(rgb_vis, win='front_rgb', opts=dict(title='front_rgb'))\n",
    "\n",
    "            depth_vis = data['depth'][0].detach().cpu().numpy()#/255.0\n",
    "            # print(f'depth_vis: {depth_vis.shape}')\n",
    "            # plt.imshow(depth_vis.transpose(1,2,0))\n",
    "            # plt.show()\n",
    "                # convert 128x128 0-255 depth map to 3x128x128 0-1 colored map\n",
    "                # vis.image(depth_vis, win='front_depth', opts=dict(title='front_depth'))\n",
    "                # vis.image(render_novel[0].permute(2, 0, 1).detach().cpu().numpy(), win='render_novel', opts=dict(title='render_novel'))\n",
    "                # vis.image(gt_rgb[0].permute(2, 0, 1).detach().cpu().numpy(), win='gt_novel', opts=dict(title='gt_novel'))\n",
    "\n",
    "            Ll1 = l2_loss(render_novel, gt_rgb)\n",
    "            # Ll1 = l2_loss(render_novel, gt_rgb)\n",
    "            Lssim = 1.0 - ssim(render_novel, gt_rgb)\n",
    "            # Lssim = 0.\n",
    "            psnr = PSNR_torch(render_novel, gt_rgb)\n",
    "\n",
    "            loss = 0.\n",
    "            \n",
    "            # print(f'render_novel: {render_novel.shape}')\n",
    "            # print(f'gt_rgb: {gt_rgb.shape}')\n",
    "            # print(f'gt_mask: {gt_mask.shape}')\n",
    "            \n",
    "            # gt_arm = gt_rgb.clone()\n",
    "            # gt_bg = gt_rgb.clone()\n",
    "            # gt_object = gt_rgb.clone()\n",
    "            \n",
    "            gt_arm_mask = gt_mask[...,0:1].repeat(1,1,1,3) # 1,128,128,3\n",
    "            gt_bg_mask = gt_mask[...,1:2].repeat(1,1,1,3)\n",
    "            gt_object_mask = gt_mask[...,2:3].repeat(1,1,1,3)\n",
    "            \n",
    "            # render_novel[gt_arm_mask!=1.0] = 0\n",
    "            # gt_rgb[gt_arm_mask!=1.0] = 0\n",
    "            # render_arm = render_novel.clone()\n",
    "            # render_bg = render_novel.clone()\n",
    "            # render_object = render_novel.clone()\n",
    "\n",
    "            # gt_arm[gt_arm_mask!=1.0] = 0\n",
    "            # gt_bg[gt_bg_mask!=1] = 0\n",
    "            # gt_object[gt_object_mask!=1] = 0\n",
    "                        \n",
    "            # render_arm[gt_arm_mask!=1.0] = 0\n",
    "            # render_bg[gt_bg_mask!=1] = 0\n",
    "            # render_object[gt_object_mask!=1] = 0\n",
    "            \n",
    "            Ll1_arm = l1_loss(render_novel[gt_arm_mask==1.0], gt_rgb[gt_arm_mask==1.0])\n",
    "            Ll1_bg = l1_loss(render_novel[gt_bg_mask==1.0], gt_rgb[gt_bg_mask==1.0])\n",
    "            Ll1_objetc = l1_loss(render_novel[gt_object_mask==1.0], gt_rgb[gt_object_mask==1.0])\n",
    "            \n",
    "            \n",
    "            # plt.imshow(render_arm.cpu().detach().numpy()[0])\n",
    "            # plt.show()\n",
    "            # plt.imshow(gt_arm.cpu().detach().numpy()[0])\n",
    "            # plt.show()\n",
    "            # loss_rgb_arm = render_novel[]\n",
    "            # loss_rgb_bg = render_novel[]\n",
    "            # loss_rgb_object = render_novel[]\n",
    "            \n",
    "            loss_rgb = self.cfg.lambda_l1 * Ll1 + self.cfg.lambda_ssim * Lssim\n",
    "            # loss_rgb = lambda_arm * Ll1_arm + lambda_bg*Ll1_bg + lambda_object*Ll1_objetc\n",
    "            # loss_rgb = Ll1_arm\n",
    "\n",
    "            loss += loss_rgb\n",
    "\n",
    "            if gt_embed is not None:\n",
    "                gt_embed = gt_embed.permute(0, 2, 3, 1) # channel last\n",
    "                render_embed = data['novel_view']['embed_pred'].permute(0, 2, 3, 1)\n",
    "\n",
    "                # DEBUG gradient\n",
    "                # render_embed_grad = render_embed.register_hook(self._save_gradient('render_embed'))\n",
    "\n",
    "                loss_embed = self._embed_loss_fn(render_embed, gt_embed)\n",
    "                loss += self.cfg.lambda_embed * loss_embed\n",
    "            else:\n",
    "                loss_embed = torch.tensor(0.)\n",
    "\n",
    "            # next frame prediction\n",
    "            if self.use_dynamic_field and (next_gt_rgb is not None) and ('xyz_maps' in data['next']):\n",
    "                data['next'] = self.pts2render(data['next'], bg_color=self.bg_color)\n",
    "                next_render_novel = data['next']['novel_view']['img_pred'].permute(0, 2, 3, 1)\n",
    "                # loss_dyna = l1_loss(next_render_novel, next_gt_rgb)\n",
    "                \n",
    "                next_gt_arm_mask = next_gt_mask[...,0:1].repeat(1,1,1,3) # 1,128,128,3\n",
    "                next_gt_bg_mask = next_gt_mask[...,1:2].repeat(1,1,1,3)\n",
    "                next_gt_object_mask = next_gt_mask[...,2:3].repeat(1,1,1,3)\n",
    "                Ll1_arm_dyna = l2_loss(next_render_novel[next_gt_arm_mask==1.0], next_gt_rgb[next_gt_arm_mask==1.0])\n",
    "                Ll1_bg_dyna = l2_loss(next_render_novel[next_gt_bg_mask==1.0], next_gt_rgb[next_gt_bg_mask==1.0])\n",
    "                Ll1_objetc_dyna = l2_loss(next_render_novel[next_gt_object_mask==1.0], next_gt_rgb[next_gt_object_mask==1.0])\n",
    "                \n",
    "                # loss_dyna = lambda_arm * Ll1_arm_dyna + lambda_bg*Ll1_bg_dyna + lambda_object*Ll1_objetc_dyna\n",
    "                \n",
    "                loss_dyna = l2_loss(next_render_novel, next_gt_rgb)\n",
    "                lambda_dyna = self.cfg.lambda_dyna if step >= self.cfg.next_mlp.warm_up else 0.\n",
    "                loss += lambda_dyna * loss_dyna\n",
    "\n",
    "                loss_reg = torch.tensor(0.)\n",
    "                # TODO: regularization on deformation\n",
    "                # if self.cfg.lambda_reg > 0:\n",
    "                #     loss_reg = l2_loss(data['next']['xyz_maps'], data['xyz_maps'].detach())\n",
    "                #     lambda_reg = self.cfg.lambda_reg if step >= self.cfg.next_mlp.warm_up else 0.\n",
    "                #     loss += lambda_reg * loss_reg\n",
    "\n",
    "                # TODO: local rigid loss\n",
    "\n",
    "            else:\n",
    "                loss_dyna = torch.tensor(0.)\n",
    "                loss_reg = torch.tensor(0.)\n",
    "\n",
    "            loss_dict = {\n",
    "                'loss': loss,\n",
    "                'loss_rgb': loss_rgb.item(),\n",
    "                'loss_embed': loss_embed.item(),\n",
    "                'loss_dyna': loss_dyna.item(),\n",
    "                'loss_reg': loss_reg.item(),\n",
    "                'l1': Ll1.item(),\n",
    "                'psnr': psnr.item(),\n",
    "                }\n",
    "        else:\n",
    "            # no ground-truth given, rendering (inference)\n",
    "            with torch.no_grad():\n",
    "                # Gaussian Generator\n",
    "                data = self.gs_model(data)\n",
    "                # Gaussian Render\n",
    "                data = self.pts2render(data, bg_color=self.bg_color) # default: [0, 0, 0]\n",
    "                render_novel = data['novel_view']['img_pred'].permute(0, 2, 3, 1) # channel last\n",
    "                render_embed = data['novel_view']['embed_pred'].permute(0, 2, 3, 1)\n",
    "                \n",
    "                if self.use_dynamic_field and 'xyz_maps' in data['next']:\n",
    "                    data['next'] = self.pts2render(data['next'], bg_color=self.bg_color)\n",
    "                    next_render_novel = data['next']['novel_view']['img_pred'].permute(0, 2, 3, 1)\n",
    "\n",
    "                loss_dict = {\n",
    "                    'loss': 0.,\n",
    "                    'loss_rgb': 0.,\n",
    "                    'loss_embed': 0.,\n",
    "                    'loss_dyna': 0.,\n",
    "                    'loss_reg': 0.,\n",
    "                    'l1': 0.,\n",
    "                    'psnr': 0.,\n",
    "                }\n",
    "\n",
    "        # get Gaussian embedding\n",
    "        ret_dict = DotMap(render_novel=render_novel, next_render_novel=next_render_novel,\n",
    "                          render_embed=render_embed, gt_embed=gt_embed)\n",
    "\n",
    "        return loss_dict, ret_dict\n",
    "    \n",
    "    def pts2render(self, data: dict, bg_color=[0,0,0]):\n",
    "        '''use render function in GS'''\n",
    "        bs = data['intr'].shape[0]\n",
    "        assert bs == 1, \"batch size should be 1\"\n",
    "        i = 0\n",
    "        xyz_i = data['xyz_maps'][i, :, :]\n",
    "        feature_i = data['sh_maps'][i, :, :, :] # [16384, 4, 3]\n",
    "        rot_i = data['rot_maps'][i, :, :]\n",
    "        scale_i = data['scale_maps'][i, :, :]\n",
    "        opacity_i = data['opacity_maps'][i, :, :]\n",
    "        feature_language_i = data['feature_maps'][i, :, :]\n",
    "\n",
    "        # print('----------render--------')\n",
    "        # print('Input-------------------')\n",
    "        # print(f'data: {data}')\n",
    "        # print(f'xyz_i: {xyz_i.shape}')\n",
    "        # print(f'rot_i: {rot_i.shape}')\n",
    "        # print(f'scale_i: {scale_i.shape}')\n",
    "        # print(f'opacity_i: {opacity_i.shape}')\n",
    "        # print(f'bg_color: {bg_color}')\n",
    "        # print(f'features_color: {feature_i.shape}')\n",
    "        # print(f'features_language: {feature_language_i.shape}')\n",
    "\n",
    "        render_return_dict = render(\n",
    "            data, i, xyz_i, rot_i, scale_i, opacity_i, \n",
    "            bg_color=bg_color, pts_rgb=None, features_color=feature_i, features_language=feature_language_i\n",
    "            )\n",
    "\n",
    "        data['novel_view']['img_pred'] = render_return_dict['render'].unsqueeze(0)\n",
    "        data['novel_view']['embed_pred'] = render_return_dict['render_embed'].unsqueeze(0)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QFunction(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 perceiver_encoder: nn.Module,\n",
    "                 voxelizer: VoxelGrid,\n",
    "                 bounds_offset: float,\n",
    "                 rotation_resolution: float,\n",
    "                 device,\n",
    "                 training,\n",
    "                 use_ddp=True,  # default: True\n",
    "                 cfg=None,\n",
    "                 fabric=None,):\n",
    "        super(QFunction, self).__init__()\n",
    "        self._rotation_resolution = rotation_resolution\n",
    "        self._voxelizer = voxelizer\n",
    "        self._bounds_offset = bounds_offset\n",
    "        self._qnet = perceiver_encoder.to(device)\n",
    "        self._coord_trans = torch.diag(torch.tensor([1, 1, 1, 1], dtype=torch.float32)).to(device)\n",
    "        \n",
    "        # self.encoder_3d = MultiLayer3DEncoderShallow(in_channels=self.init_dim, out_channels=self.im_channels)\n",
    "\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        if cfg.use_neural_rendering:\n",
    "            self._neural_renderer = NeuralRenderer(cfg.neural_renderer).to(device)\n",
    "            if training and use_ddp:\n",
    "                self._neural_renderer = fabric.setup(self._neural_renderer)\n",
    "        else:\n",
    "            self._neural_renderer = None\n",
    "        print(colored(f\"[NeuralRenderer]: {cfg.use_neural_rendering}\", \"cyan\"))\n",
    "        \n",
    "        # distributed training\n",
    "        if training and use_ddp:\n",
    "            print(colored(f\"[QFunction] use DDP: True\", \"cyan\"))\n",
    "            self._qnet = fabric.setup(self._qnet)\n",
    "        \n",
    "        self.device = device\n",
    "\n",
    "    def _argmax_3d(self, tensor_orig):\n",
    "        b, c, d, h, w = tensor_orig.shape  # c will be one\n",
    "        idxs = tensor_orig.view(b, c, -1).argmax(-1)\n",
    "        indices = torch.cat([((idxs // h) // d), (idxs // h) % w, idxs % w], 1)\n",
    "        return indices\n",
    "\n",
    "\n",
    "    def choose_highest_action(self, q_trans, q_rot_grip, q_collision):\n",
    "        coords = self._argmax_3d(q_trans)\n",
    "        rot_and_grip_indicies = None\n",
    "        ignore_collision = None\n",
    "        if q_rot_grip is not None:\n",
    "            q_rot = torch.stack(torch.split(\n",
    "                q_rot_grip[:, :-2],\n",
    "                int(360 // self._rotation_resolution),\n",
    "                dim=1), dim=1)\n",
    "            rot_and_grip_indicies = torch.cat(\n",
    "                [q_rot[:, 0:1].argmax(-1),\n",
    "                 q_rot[:, 1:2].argmax(-1),\n",
    "                 q_rot[:, 2:3].argmax(-1),\n",
    "                 q_rot_grip[:, -2:].argmax(-1, keepdim=True)], -1)\n",
    "            ignore_collision = q_collision[:, -2:].argmax(-1, keepdim=True)\n",
    "        return coords, rot_and_grip_indicies, ignore_collision\n",
    "    \n",
    "\n",
    "    def forward(self, rgb_pcd, depth, proprio, pcd, mask, camera_extrinsics, camera_intrinsics, lang_goal_emb, lang_token_embs,\n",
    "                bounds=None, prev_bounds=None, prev_layer_voxel_grid=None,\n",
    "                use_neural_rendering=False, nerf_target_rgb=None, nerf_target_depth=None, nerf_target_mask=None,\n",
    "                nerf_target_pose=None, nerf_target_camera_intrinsic=None,\n",
    "                lang_goal=None,\n",
    "                nerf_next_target_rgb=None, nerf_next_target_mask=None, nerf_next_target_pose=None, nerf_next_target_depth=None,\n",
    "                nerf_next_target_camera_intrinsic=None,\n",
    "                gt_embed=None, step=None, action=None):\n",
    "        '''\n",
    "        Return Q-functions and neural rendering loss\n",
    "        '''\n",
    "        # rgb_pcd:<class 'list'>\n",
    "        # depth:<class 'list'>\n",
    "        # proprio:<class 'torch.Tensor'>\n",
    "        # pcd:<class 'list'>\n",
    "        # camera_intrinsics:<class 'list'>\n",
    "        # lang_goal_emb:<class 'torch.Tensor'>\n",
    "        # lang_token_embs:<class 'torch.Tensor'>\n",
    "        # bounds:<class 'torch.Tensor'>\n",
    "        # use_neural_rendering:<class 'bool'>\n",
    "        # nerf_target_rgb:<class 'torch.Tensor'>\n",
    "        # nerf_target_depth:<class 'torch.Tensor'>\n",
    "        # nerf_target_pose:<class 'torch.Tensor'>\n",
    "        # nerf_target_camera_intrinsic:<class 'torch.Tensor'>\n",
    "        # lang_goal:<class 'numpy.ndarray'>\n",
    "        # nerf_next_target_rgb:<class 'torch.Tensor'>\n",
    "        # nerf_next_target_pose:<class 'torch.Tensor'>\n",
    "        # nerf_next_target_camera_intrinsic:<class 'torch.Tensor'>\n",
    "        # step:<class 'int'>\n",
    "        # action:<class 'torch.Tensor'>\n",
    "        # print(f'rgb_pcd:{type(rgb_pcd)}, {len(rgb_pcd)}')\n",
    "        # print(f'depth:{type(depth)}, {len(depth)}')\n",
    "        # print(f'proprio:{type(proprio)}, {proprio.shape} {proprio}') # low_dim_state\n",
    "        # print(f'pcd:{type(pcd)}, {len(pcd)}')\n",
    "        # print(f'camera_intrinsics:{type(camera_intrinsics)}, {len(camera_intrinsics)}, {camera_intrinsics[0]}')\n",
    "        # print(f'lang_goal_emb:{type(lang_goal_emb)}, {lang_goal_emb.shape}')\n",
    "        # print(f'lang_token_embs:{type(lang_token_embs)}, {lang_token_embs.shape}')\n",
    "        # print(f'bounds:{type(bounds)}, {bounds.shape}')\n",
    "\n",
    "        # print(f'use_neural_rendering:{type(use_neural_rendering)}, {use_neural_rendering}')\n",
    "        # print(f'nerf_target_rgb:{type(nerf_target_rgb)}, {nerf_target_rgb.shape}')\n",
    "        # print(f'nerf_target_depth:{type(nerf_target_depth)}, {nerf_target_depth.shape}')\n",
    "        # print(f'nerf_target_pose:{type(nerf_target_pose)}, {nerf_target_pose.shape}')\n",
    "        # print(f'nerf_target_camera_intrinsic:{type(nerf_target_camera_intrinsic)}, {nerf_target_camera_intrinsic.shape}')\n",
    "\n",
    "        # print(f'lang_goal:{type(lang_goal)}, {lang_goal}')\n",
    "        # print(f'nerf_next_target_rgb:{type(nerf_next_target_rgb)}, {nerf_next_target_rgb.shape}')\n",
    "        # print(f'nerf_next_target_pose:{type(nerf_next_target_pose)}, {nerf_next_target_pose.shape}')\n",
    "        # print(f'nerf_next_target_camera_intrinsic:{type(nerf_next_target_camera_intrinsic)}, {nerf_next_target_camera_intrinsic.shape}')\n",
    "\n",
    "        # print(f'step:{type(step)}, {step}')\n",
    "        # print(f'action:{type(action)}, {action.shape}')\n",
    "\n",
    "\n",
    "        b = rgb_pcd[0][0].shape[0]\n",
    "\n",
    "        pcd_flat = torch.cat(\n",
    "            [p.permute(0, 2, 3, 1).reshape(b, -1, 3) for p in pcd], 1)  # [1, 16384, 3]\n",
    "        \n",
    "        # flatten RGBs and Pointclouds\n",
    "        rgb = [rp[0] for rp in rgb_pcd]\n",
    "        feat_size = rgb[0].shape[1] # 3\n",
    "\n",
    "        flat_imag_features = torch.cat(\n",
    "            [p.permute(0, 2, 3, 1).reshape(b, -1, feat_size) for p in rgb], 1)  # [1, 16384, 3]\n",
    "\n",
    "        # construct voxel grid\n",
    "        voxel_grid, voxel_density = self._voxelizer.coords_to_bounding_voxel_grid(\n",
    "            pcd_flat, coord_features=flat_imag_features, coord_bounds=bounds, return_density=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        # swap to channels fist\n",
    "        voxel_grid = voxel_grid.permute(0, 4, 1, 2, 3).detach() # Bx10x100x100x100\n",
    "\n",
    "        # batch bounds if necessary\n",
    "        if bounds.shape[0] != b:\n",
    "            bounds = bounds.repeat(b, 1)\n",
    "\n",
    "\n",
    "        # print(f'--------PerceiverIO:-------------')\n",
    "        # print('Input:---------------')\n",
    "        # print(f'proprio:{type(proprio)}, {proprio.shape} {proprio}') # low_dim_state\n",
    "        # print(f'voxel_grid: {voxel_grid.shape}')\n",
    "        # print(f'lang_goal_emb:{type(lang_goal_emb)}, {lang_goal_emb.shape}')\n",
    "        # print(f'lang_token_embs:{type(lang_token_embs)}, {lang_token_embs.shape}')\n",
    "        # print(f'bounds:{type(bounds)}, {bounds.shape}')\n",
    "        # forward pass\n",
    "        q_trans, \\\n",
    "        q_rot_and_grip,\\\n",
    "        q_ignore_collisions,\\\n",
    "        voxel_grid_feature, \\\n",
    "        multi_scale_voxel_list, \\\n",
    "        lang_embedd = self._qnet(voxel_grid,  # [1,10,100,100,100]\n",
    "                                proprio, # [1,4]\n",
    "                                lang_goal_emb, # [1,1024]\n",
    "                                lang_token_embs, # [1,77,512]\n",
    "                                None,\n",
    "                                bounds, # [1,6]\n",
    "                                None)\n",
    "        \n",
    "        # voxel_grid_feature, _ = self.encoder_3d(voxel_grid) # [B,10,100,100,100] -> [B,128,100,100,100]\n",
    "        # print('Output:---------------')\n",
    "        # print(f'q_trans: {q_trans.shape}')\n",
    "        # print(f'q_rot_and_grip: {q_rot_and_grip.shape}')\n",
    "        # print(f'q_ignore_collisions: {q_ignore_collisions.shape}, {q_ignore_collisions}')\n",
    "        # print(f'voxel_grid_feature: {voxel_grid_feature.shape}')\n",
    "        # print(f'lang_embedd: {lang_embedd.shape}')\n",
    "\n",
    "        # neural rendering as an auxiliary loss\n",
    "        rendering_loss_dict = {}\n",
    "        if use_neural_rendering:    # train default: True; eval default: False\n",
    "            # prepare nerf rendering\n",
    "            focal = camera_intrinsics[0][:, 0, 0]  # [SB]\n",
    "            cx = 128 / 2\n",
    "            cy = 128 / 2\n",
    "            c = torch.tensor([cx, cy], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            if nerf_target_rgb is not None:\n",
    "                gt_rgb = nerf_target_rgb    # [1,128,128,3]\n",
    "                gt_mask = nerf_target_mask\n",
    "                gt_pose = nerf_target_pose @ self._coord_trans # remember to do this\n",
    "                gt_depth = nerf_target_depth\n",
    "\n",
    "                # We only use the front camera\n",
    "                rgb_0 = rgb[0]\n",
    "                depth_0 = depth[0]\n",
    "                pcd_0 = pcd[0] # 1,3,128,128\n",
    "                mask_0 = mask[0] # 1,1,128,128\n",
    "\n",
    "\n",
    "                # print(f'--------_neural_renderer:-------------')\n",
    "                # print('Input:---------------')\n",
    "                # print(f'rgb: {rgb_0.shape}')\n",
    "                # print(f'pcd: {pcd_0.shape}')\n",
    "                # print(f'mask_0: {mask_0.shape}')\n",
    "                # plt.imshow(pcd_0[0].permute(1,2,0).cpu())\n",
    "                # plt.show()\n",
    "                # plt.imshow(mask_0[0].permute(1,2,0).cpu())\n",
    "                # plt.show()\n",
    "                # print(f'depth: {depth_0.shape}')\n",
    "                # print(f'language: {lang_embedd.shape}')\n",
    "                # print(f'dec_fts: {voxel_grid_feature.shape}')\n",
    "                # print(f'gt_rgb: {gt_rgb.shape}')\n",
    "                # print(f'gt_depth: {gt_depth.shape}')\n",
    "                # print(f'focal: {focal.shape}')\n",
    "                # print(f'c: {c.shape}')\n",
    "                # print(f'gt_pose: {gt_pose.shape}')\n",
    "                # print(f'gt_intrinsic: {nerf_target_camera_intrinsic.shape}')\n",
    "                # print(f'lang_goal: {lang_goal}')\n",
    "                # print(f'next_gt_pose: {nerf_next_target_pose.shape}')\n",
    "                # print(f'next_gt_intrinsic: {nerf_next_target_camera_intrinsic.shape}')\n",
    "                # print(f'next_gt_rgb: {nerf_next_target_rgb.shape}')\n",
    "                # print(f'step: {step}')\n",
    "                # print(f'action: {action.shape}')\n",
    "                # render loss\n",
    "                rendering_loss_dict, _ = self._neural_renderer(\n",
    "                    rgb=rgb_0, pcd=pcd_0, depth=depth_0, mask=mask_0,\\\n",
    "                    language=lang_embedd, \\\n",
    "                    dec_fts=voxel_grid_feature, \\\n",
    "                    gt_rgb=gt_rgb, gt_mask = gt_mask, gt_depth=gt_depth, \\\n",
    "                    focal=focal, c=c, \\\n",
    "                    gt_pose=gt_pose, gt_intrinsic=nerf_target_camera_intrinsic, \\\n",
    "                    lang_goal=lang_goal, \n",
    "                    next_gt_pose=nerf_next_target_pose, next_gt_intrinsic=nerf_next_target_camera_intrinsic, \n",
    "                    next_gt_rgb=nerf_next_target_rgb, next_gt_mask=nerf_next_target_mask,\n",
    "                    step=step, action=action, proprio = proprio,\n",
    "                    training=True,\n",
    "                    )\n",
    "                \n",
    "                # print('Output:---------------')\n",
    "                # print(f'rendering_loss_dict: {rendering_loss_dict}')\n",
    "\n",
    "            else:\n",
    "                # # if we do not have additional multi-view data, we use input view as reconstruction target\n",
    "                rendering_loss_dict = {\n",
    "                    'loss': 0.,\n",
    "                    'loss_rgb': 0.,\n",
    "                    'loss_embed': 0.,\n",
    "                    'l1': 0.,\n",
    "                    'psnr': 0.,\n",
    "                    }\n",
    "\n",
    "        return q_trans, q_rot_and_grip, q_ignore_collisions, voxel_grid, rendering_loss_dict\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def render(self, rgb_pcd, proprio, pcd, mask,\n",
    "               lang_goal_emb, lang_token_embs,\n",
    "                tgt_pose, tgt_intrinsic,\n",
    "               camera_extrinsics=None, camera_intrinsics=None,\n",
    "                depth=None, bounds=None, prev_bounds=None, prev_layer_voxel_grid=None,\n",
    "                nerf_target_rgb=None, lang_goal=None,\n",
    "                nerf_next_target_rgb=None, nerf_next_target_depth=None,\n",
    "                nerf_next_target_pose=None, nerf_next_target_camera_intrinsic=None, \n",
    "                action=None, step=None,\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Render the novel view and the next novel view during the training process\n",
    "        \"\"\"\n",
    "        # rgb_pcd will be list of [rgb, pcd]\n",
    "        b = rgb_pcd[0][0].shape[0]\n",
    "        pcd_flat = torch.cat(\n",
    "            [p.permute(0, 2, 3, 1).reshape(b, -1, 3) for p in pcd], 1)\n",
    "\n",
    "        # flatten RGBs and Pointclouds\n",
    "        rgb = [rp[0] for rp in rgb_pcd]\n",
    "        feat_size = rgb[0].shape[1]\n",
    "        flat_imag_features = torch.cat(\n",
    "            [p.permute(0, 2, 3, 1).reshape(b, -1, feat_size) for p in rgb], 1)\n",
    "\n",
    "        # construct voxel grid\n",
    "        voxel_grid, voxel_density = self._voxelizer.coords_to_bounding_voxel_grid(\n",
    "            pcd_flat, coord_features=flat_imag_features, coord_bounds=bounds, return_density=True)\n",
    "\n",
    "        # swap to channels fist\n",
    "        voxel_grid = voxel_grid.permute(0, 4, 1, 2, 3).detach()\n",
    "\n",
    "        # batch bounds if necessary\n",
    "        if bounds.shape[0] != b:\n",
    "            bounds = bounds.repeat(b, 1)\n",
    "\n",
    "        # forward pass\n",
    "        q_trans, \\\n",
    "        q_rot_and_grip,\\\n",
    "        q_ignore_collisions,\\\n",
    "        voxel_grid_feature,\\\n",
    "        multi_scale_voxel_list,\\\n",
    "        lang_embedd = self._qnet(voxel_grid, \n",
    "                                        proprio,\n",
    "                                        lang_goal_emb, \n",
    "                                        lang_token_embs,\n",
    "                                        prev_layer_voxel_grid,\n",
    "                                        bounds, \n",
    "                                        prev_bounds)\n",
    "\n",
    "        # prepare nerf rendering\n",
    "        # We only use the front camera\n",
    "        _, ret_dict = self._neural_renderer(\n",
    "                pcd=pcd[0],\n",
    "                rgb=rgb[0],\n",
    "                mask=mask[0],\n",
    "                dec_fts=voxel_grid_feature,\n",
    "                language=lang_embedd,\n",
    "                gt_pose=tgt_pose,\n",
    "                gt_intrinsic=tgt_intrinsic,\n",
    "                # for providing gt embed\n",
    "                gt_rgb=nerf_target_rgb,\n",
    "                lang_goal=lang_goal,\n",
    "                next_gt_rgb=nerf_next_target_rgb,\n",
    "                next_gt_pose=nerf_next_target_pose,\n",
    "                next_gt_intrinsic=nerf_next_target_camera_intrinsic,\n",
    "                step=step,\n",
    "                action=action,\n",
    "                proprio = proprio,\n",
    "                training=False,\n",
    "                )\n",
    "\n",
    "        return ret_dict.render_novel, ret_dict.next_render_novel, ret_dict.render_embed, ret_dict.gt_embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qattention manigaussian bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "NAME = 'QAttentionAgent'\n",
    "\n",
    "\n",
    "def visualize_feature_map_by_clustering(features, num_cluster=4, return_cluster_center=False):\n",
    "    from sklearn.cluster import KMeans\n",
    "    features = features.cpu().detach().numpy()\n",
    "    B, D, H, W = features.shape\n",
    "    features_1d = features.reshape(B, D, H*W).transpose(0, 2, 1).reshape(-1, D)\n",
    "    kmeans = KMeans(n_clusters=num_cluster, random_state=0, n_init=10).fit(features_1d)\n",
    "    labels = kmeans.labels_\n",
    "    labels = labels.reshape(H, W)\n",
    "\n",
    "    cluster_colors = [\n",
    "        np.array([255, 0, 0]),   # red\n",
    "        np.array([0, 255, 0]),       # green\n",
    "        np.array([0, 0, 255]),      # blue\n",
    "        np.array([255, 255, 0]),   # yellow\n",
    "        np.array([255, 0, 255]),  # magenta\n",
    "        np.array([0, 255, 255]),    # cyan\n",
    "    ]\n",
    "\n",
    "    segmented_img = np.zeros((H, W, 3))\n",
    "    for i in range(num_cluster):\n",
    "        segmented_img[labels==i] = cluster_colors[i]\n",
    "        \n",
    "    if return_cluster_center:\n",
    "        cluster_centers = []\n",
    "        for i in range(num_cluster):\n",
    "            cluster_pixels = np.argwhere(labels == i)\n",
    "            cluster_center = cluster_pixels.mean(axis=0)\n",
    "            cluster_centers.append(cluster_center)\n",
    "        return labels, cluster_centers\n",
    "        \n",
    "    return segmented_img\n",
    " \n",
    "def visualize_feature_map_by_normalization(features):\n",
    "    '''\n",
    "    Normalize feature map to [0, 1] for plt.show()\n",
    "    :features: (B, 3, H, W)\n",
    "    Return: (H, W, 3)\n",
    "    '''\n",
    "    MIN_DENOMINATOR = 1e-12\n",
    "    features = features[0].cpu().detach().numpy()\n",
    "    features = features.transpose(1, 2, 0)  # [H, W, 3]\n",
    "    features = features / (np.linalg.norm(features, axis=-1, keepdims=True) + MIN_DENOMINATOR)\n",
    "    return features\n",
    " \n",
    "def PSNR_torch(img1, img2, max_val=1):\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    PIXEL_MAX = max_val\n",
    "    return 20 * torch.log10(PIXEL_MAX / torch.sqrt(mse))\n",
    "\n",
    "\n",
    "def parse_camera_file(file_path):\n",
    "    \"\"\"\n",
    "    Parse our camera format.\n",
    "\n",
    "    The format is (*.txt):\n",
    "    \n",
    "    4x4 matrix (camera extrinsic)\n",
    "    space\n",
    "    3x3 matrix (camera intrinsic)\n",
    "\n",
    "    focal is extracted from the intrinsc matrix\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    camera_extrinsic = []\n",
    "    for x in lines[0:4]:\n",
    "        camera_extrinsic += [float(y) for y in x.split()]\n",
    "    camera_extrinsic = np.array(camera_extrinsic).reshape(4, 4)\n",
    "\n",
    "    camera_intrinsic = []\n",
    "    for x in lines[5:8]:\n",
    "        camera_intrinsic += [float(y) for y in x.split()]\n",
    "    camera_intrinsic = np.array(camera_intrinsic).reshape(3, 3)\n",
    "\n",
    "    focal = camera_intrinsic[0, 0]\n",
    "\n",
    "    return camera_extrinsic, camera_intrinsic, focal\n",
    "\n",
    "def parse_img_file(file_path, mask_gt_rgb=False, bg_color=[0,0,0,255]):\n",
    "    \"\"\"\n",
    "    return np.array of RGB image with range [0, 1]\n",
    "    \"\"\"\n",
    "    rgb = Image.open(file_path).convert('RGB')\n",
    "    rgb = np.asarray(rgb).astype(np.float32) / 255.0    # [0, 1]\n",
    "    return rgb\n",
    "\n",
    "def parse_mask_file(file_path):\n",
    "    \"\"\"\n",
    "    return np.array (128,128,num_of_object) mask with range [0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    mask = Image.open(file_path)# .convert('RGB')\n",
    "    mask = np.asarray(mask)\n",
    "    # plt.imshow(mask)\n",
    "    # plt.show()\n",
    "    mask_value = mask.reshape(-1)\n",
    "    # print(f'mask_value: {set(mask_value)}')\n",
    "    arm_mask_value = [31,34,35,39,40,41,42,43,44,45,46]\n",
    "    bg_mask_value = [10,48,52,53,54,55]\n",
    "    object_mask_value = [item for item in set(mask_value) if item not in arm_mask_value + bg_mask_value]\n",
    "    object_mask_value.remove(0) # remove 0 value\n",
    "    \n",
    "    arm_mask = group_mask(mask,arm_mask_value)[:,:,0:1] # [...,0:1]  (128,128,3) -> (128,128,1) need only R channel\n",
    "    bg_mask = group_mask(mask,bg_mask_value)[:,:,0:1]\n",
    "    object_mask = group_mask(mask,object_mask_value)[:,:,0:1]\n",
    "    \n",
    "    # fig, ax = plt.subplots(1,3,figsize=(15,15))\n",
    "    # ax[0].set_title('arm_mask')\n",
    "    # ax[0].imshow(arm_mask)\n",
    "    # ax[1].set_title('bg_mask')\n",
    "    # ax[1].imshow(bg_mask)\n",
    "    # ax[2].set_title('object_mask')\n",
    "    # ax[2].imshow(object_mask)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    return np.concatenate([arm_mask,bg_mask,object_mask],axis=-1) # return (128,128,num_of_object) mask\n",
    "\n",
    "\n",
    "def group_mask(mask, mask_value):\n",
    "    '''\n",
    "    group the mask by value, for example [31,34,35,39,40,41,42,43,44,45,46] are consider as robot arm,\n",
    "    group these values into one mask\n",
    "    '''\n",
    "    total_mask = np.zeros([128,128,3]).astype(np.uint8)\n",
    "    \n",
    "    for obj in mask_value:\n",
    "        _mask = mask.copy()\n",
    "        _mask[_mask!=obj] = 0\n",
    "        _mask[_mask==obj] = 255\n",
    "        total_mask+=_mask\n",
    "        \n",
    "    # plt.imshow(total_mask)\n",
    "    # plt.show()\n",
    "    return total_mask.astype(np.float32) / 255.0\n",
    "\n",
    "def parse_depth_file(file_path):\n",
    "    \"\"\"\n",
    "    return np.array of depth image\n",
    "    \"\"\"\n",
    "    depth = Image.open(file_path).convert('L')\n",
    "    depth = np.asarray(depth).astype(np.float32)\n",
    "    return depth\n",
    "\n",
    "\n",
    "class QAttentionPerActBCAgent(Agent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 layer: int,\n",
    "                 coordinate_bounds: list,\n",
    "                 perceiver_encoder: nn.Module,\n",
    "                 camera_names: list,\n",
    "                 batch_size: int,\n",
    "                 voxel_size: int,\n",
    "                 bounds_offset: float,\n",
    "                 voxel_feature_size: int,\n",
    "                 image_crop_size: int,\n",
    "                 num_rotation_classes: int,\n",
    "                 rotation_resolution: float,\n",
    "                 lr: float = 0.0001,\n",
    "                 lr_scheduler: bool = False,\n",
    "                 training_iterations: int = 100000,\n",
    "                 num_warmup_steps: int = 20000,\n",
    "                 trans_loss_weight: float = 1.0,\n",
    "                 rot_loss_weight: float = 1.0,\n",
    "                 grip_loss_weight: float = 1.0,\n",
    "                 collision_loss_weight: float = 1.0,\n",
    "                 include_low_dim_state: bool = False,\n",
    "                 image_resolution: list = None,\n",
    "                 lambda_weight_l2: float = 0.0,\n",
    "                 transform_augmentation: bool = True,   # True\n",
    "                 transform_augmentation_xyz: list = [0.0, 0.0, 0.0],\n",
    "                 transform_augmentation_rpy: list = [0.0, 0.0, 180.0],\n",
    "                 transform_augmentation_rot_resolution: int = 5,\n",
    "                 optimizer_type: str = 'adam',\n",
    "                 num_devices: int = 1,\n",
    "                 cfg = None,\n",
    "                 ):\n",
    "        self._layer = layer\n",
    "        self._coordinate_bounds = coordinate_bounds\n",
    "        self._perceiver_encoder = perceiver_encoder\n",
    "        self._voxel_feature_size = voxel_feature_size\n",
    "        self._bounds_offset = bounds_offset\n",
    "        self._image_crop_size = image_crop_size\n",
    "        self._lr = lr\n",
    "        self._lr_scheduler = lr_scheduler\n",
    "        self._training_iterations = training_iterations\n",
    "        self._num_warmup_steps = num_warmup_steps\n",
    "        self._trans_loss_weight = trans_loss_weight\n",
    "        self._rot_loss_weight = rot_loss_weight\n",
    "        self._grip_loss_weight = grip_loss_weight\n",
    "        self._collision_loss_weight = collision_loss_weight\n",
    "        self._include_low_dim_state = include_low_dim_state\n",
    "        self._image_resolution = image_resolution or [128, 128]\n",
    "        self._voxel_size = voxel_size\n",
    "        self._camera_names = camera_names\n",
    "        self._num_cameras = len(camera_names)\n",
    "        self._batch_size = batch_size\n",
    "        self._lambda_weight_l2 = lambda_weight_l2\n",
    "        self._transform_augmentation = transform_augmentation\n",
    "        self._transform_augmentation_xyz = torch.from_numpy(np.array(transform_augmentation_xyz))\n",
    "        self._transform_augmentation_rpy = transform_augmentation_rpy\n",
    "        self._transform_augmentation_rot_resolution = transform_augmentation_rot_resolution\n",
    "        self._optimizer_type = optimizer_type\n",
    "        self._num_devices = num_devices\n",
    "        self._num_rotation_classes = num_rotation_classes\n",
    "        self._rotation_resolution = rotation_resolution\n",
    "\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.use_neural_rendering = self.cfg.use_neural_rendering\n",
    "        print(colored(f\"use_neural_rendering: {self.use_neural_rendering}\", \"red\"))\n",
    "\n",
    "        if self.use_neural_rendering:\n",
    "            print(colored(f\"[agent] nerf weight step: {self.cfg.neural_renderer.lambda_nerf}\", \"red\"))\n",
    "\n",
    "        self._cross_entropy_loss = nn.CrossEntropyLoss(reduction='mean')  # if batch size>1\n",
    "        \n",
    "        self._mask_gt_rgb = cfg.neural_renderer.dataset.mask_gt_rgb\n",
    "        print(colored(f\"[NeuralRenderer] mask_gt_rgb: {self._mask_gt_rgb}\", \"cyan\"))\n",
    "        \n",
    "        self._name = NAME + '_layer' + str(self._layer)\n",
    "\n",
    "    def build(self, training: bool, device: torch.device = None, use_ddp=True, fabric: Fabric = None):\n",
    "        self._training = training\n",
    "        self._device = device\n",
    "\n",
    "        if device is None:\n",
    "            device = torch.device('cpu')\n",
    "\n",
    "        print(f\"device: {device}\")\n",
    "        self._voxelizer = VoxelGrid(\n",
    "            coord_bounds=self._coordinate_bounds.cpu() if isinstance(self._coordinate_bounds, torch.Tensor) else self._coordinate_bounds,\n",
    "            voxel_size=self._voxel_size,\n",
    "            device=device,  # 0\n",
    "            batch_size=self._batch_size if training else 1,\n",
    "            feature_size=self._voxel_feature_size,\n",
    "            max_num_coords=np.prod(self._image_resolution) * self._num_cameras,\n",
    "        )\n",
    "\n",
    "        self._q = QFunction(self._perceiver_encoder,\n",
    "                            self._voxelizer,\n",
    "                            self._bounds_offset,\n",
    "                            self._rotation_resolution,\n",
    "                            device,\n",
    "                            training,\n",
    "                            use_ddp,\n",
    "                            self.cfg,\n",
    "                            fabric=fabric).to(device).train(training)\n",
    "\n",
    "        grid_for_crop = torch.arange(0,\n",
    "                                     self._image_crop_size,\n",
    "                                     device=device).unsqueeze(0).repeat(self._image_crop_size, 1).unsqueeze(-1)\n",
    "        self._grid_for_crop = torch.cat([grid_for_crop.transpose(1, 0),\n",
    "                                         grid_for_crop], dim=2).unsqueeze(0)\n",
    "\n",
    "        self._coordinate_bounds = torch.tensor(self._coordinate_bounds,\n",
    "                                               device=device).unsqueeze(0)\n",
    "\n",
    "        if self._training:\n",
    "            # optimizer\n",
    "            if self._optimizer_type == 'lamb':\n",
    "                self._optimizer = Lamb(\n",
    "                    self._q.parameters(),\n",
    "                    lr=self._lr,\n",
    "                    weight_decay=self._lambda_weight_l2,\n",
    "                    betas=(0.9, 0.999),\n",
    "                    adam=False,\n",
    "                )\n",
    "            elif self._optimizer_type == 'adam':\n",
    "                self._optimizer = torch.optim.Adam(\n",
    "                    self._q.parameters(),\n",
    "                    lr=self._lr,\n",
    "                    weight_decay=self._lambda_weight_l2,\n",
    "                )\n",
    "            else:\n",
    "                raise Exception('Unknown optimizer type')\n",
    "            \n",
    "            # DDP optimizer\n",
    "            try:\n",
    "                self._optimizer = fabric.setup_optimizers(self._optimizer)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "            # learning rate scheduler\n",
    "            if self._lr_scheduler:\n",
    "                self._scheduler = transformers.get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "                    self._optimizer,\n",
    "                    num_warmup_steps=self._num_warmup_steps,\n",
    "                    num_training_steps=self._training_iterations,\n",
    "                    num_cycles=self._training_iterations // 10000,\n",
    "                )\n",
    "\n",
    "            # one-hot zero tensors\n",
    "            self._action_trans_one_hot_zeros = torch.zeros((self._batch_size,\n",
    "                                                            1,\n",
    "                                                            self._voxel_size,\n",
    "                                                            self._voxel_size,\n",
    "                                                            self._voxel_size),\n",
    "                                                            dtype=int,\n",
    "                                                            device=device)\n",
    "            self._action_rot_x_one_hot_zeros = torch.zeros((self._batch_size,\n",
    "                                                            self._num_rotation_classes),\n",
    "                                                            dtype=int,\n",
    "                                                            device=device)\n",
    "            self._action_rot_y_one_hot_zeros = torch.zeros((self._batch_size,\n",
    "                                                            self._num_rotation_classes),\n",
    "                                                            dtype=int,\n",
    "                                                            device=device)\n",
    "            self._action_rot_z_one_hot_zeros = torch.zeros((self._batch_size,\n",
    "                                                            self._num_rotation_classes),\n",
    "                                                            dtype=int,\n",
    "                                                            device=device)\n",
    "            self._action_grip_one_hot_zeros = torch.zeros((self._batch_size,\n",
    "                                                           2),\n",
    "                                                           dtype=int,\n",
    "                                                           device=device)\n",
    "            self._action_ignore_collisions_one_hot_zeros = torch.zeros((self._batch_size,\n",
    "                                                                        2),\n",
    "                                                                        dtype=int,\n",
    "                                                                        device=device)\n",
    "\n",
    "            # print total params\n",
    "            total_num_params  = 0\n",
    "            total_size_params = 0\n",
    "            for name, p in self._q.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    print(name)\n",
    "                    total_num_params+= p.numel()\n",
    "                    total_size_params += p.numel()* p.element_size()\n",
    "                \n",
    "            logging.info('# Q Params: %d M' % (total_num_params/1e6))\n",
    "            logging.info('# total_size_params: %d M' % (total_size_params/1e6))\n",
    "            \n",
    "        else:\n",
    "            for param in self._q.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            self.language_model =  create_language_model(self.cfg.language_model)\n",
    "            self._voxelizer.to(device)\n",
    "            self._q.to(device)\n",
    "\n",
    "    def _preprocess_inputs(self, replay_sample, sample_id=None):\n",
    "        obs = []\n",
    "        depths = []\n",
    "        pcds = []\n",
    "        exs = []\n",
    "        ins = []\n",
    "        masks = []\n",
    "        self._crop_summary = []\n",
    "        # print(f'_camera_names: {self._camera_names}')\n",
    "        for n in self._camera_names:    # default: [front,left_shoulder,right_shoulder,wrist] or [front]\n",
    "            if sample_id is not None:   # default: None\n",
    "                rgb = replay_sample['%s_rgb' % n][sample_id:sample_id+1]\n",
    "                depth = replay_sample['%s_depth' % n][sample_id:sample_id+1]\n",
    "                pcd = replay_sample['%s_point_cloud' % n][sample_id:sample_id+1]\n",
    "                mask = replay_sample['%s_mask' % n][sample_id:sample_id+1]\n",
    "                \n",
    "                extin = replay_sample['%s_camera_extrinsics' % n][sample_id:sample_id+1]\n",
    "                intin = replay_sample['%s_camera_intrinsics' % n][sample_id:sample_id+1]\n",
    "            else:\n",
    "                rgb = replay_sample['%s_rgb' % n]\n",
    "                depth = replay_sample['%s_depth' % n]\n",
    "                pcd = replay_sample['%s_point_cloud' % n]\n",
    "                extin = replay_sample['%s_camera_extrinsics' % n]\n",
    "                intin = replay_sample['%s_camera_intrinsics' % n]\n",
    "                mask = replay_sample['%s_mask' % n]\n",
    "                \n",
    "            obs.append([rgb, pcd])\n",
    "            depths.append(depth)\n",
    "            pcds.append(pcd)\n",
    "            masks.append(mask)\n",
    "            exs.append(extin)\n",
    "            ins.append(intin)\n",
    "        return obs, depths, pcds, masks, exs, ins\n",
    "\n",
    "    def _act_preprocess_inputs(self, observation):\n",
    "        obs, depths, pcds, exs, ins = [], [], [], [], []\n",
    "        for n in self._camera_names:\n",
    "            rgb = observation['%s_rgb' % n]\n",
    "            # [-1,1] to [0,1]\n",
    "            # rgb = (rgb + 1) / 2\n",
    "            depth = observation['%s_depth' % n]\n",
    "            pcd = observation['%s_point_cloud' % n]\n",
    "            extin = observation['%s_camera_extrinsics' % n].squeeze(0)\n",
    "            intin = observation['%s_camera_intrinsics' % n].squeeze(0)\n",
    "\n",
    "            obs.append([rgb, pcd])\n",
    "            depths.append(depth)\n",
    "            pcds.append(pcd)\n",
    "            exs.append(extin)\n",
    "            ins. append(intin)\n",
    "        return obs, depths, pcds, exs, ins\n",
    "\n",
    "    def _get_value_from_voxel_index(self, q, voxel_idx):\n",
    "        b, c, d, h, w = q.shape\n",
    "        q_trans_flat = q.view(b, c, d * h * w)\n",
    "        flat_indicies = (voxel_idx[:, 0] * d * h + voxel_idx[:, 1] * h + voxel_idx[:, 2])[:, None].int()\n",
    "        highest_idxs = flat_indicies.unsqueeze(-1).repeat(1, c, 1)\n",
    "        chosen_voxel_values = q_trans_flat.gather(2, highest_idxs)[..., 0]  # (B, trans + rot + grip)\n",
    "        return chosen_voxel_values\n",
    "\n",
    "    def _get_value_from_rot_and_grip(self, rot_grip_q, rot_and_grip_idx):\n",
    "        q_rot = torch.stack(torch.split(\n",
    "            rot_grip_q[:, :-2], int(360 // self._rotation_resolution),\n",
    "            dim=1), dim=1)  # B, 3, 72\n",
    "        q_grip = rot_grip_q[:, -2:]\n",
    "        rot_and_grip_values = torch.cat(\n",
    "            [q_rot[:, 0].gather(1, rot_and_grip_idx[:, 0:1]),\n",
    "             q_rot[:, 1].gather(1, rot_and_grip_idx[:, 1:2]),\n",
    "             q_rot[:, 2].gather(1, rot_and_grip_idx[:, 2:3]),\n",
    "             q_grip.gather(1, rot_and_grip_idx[:, 3:4])], -1)\n",
    "        return rot_and_grip_values\n",
    "\n",
    "    def _celoss(self, pred, labels):\n",
    "        return self._cross_entropy_loss(pred, labels.argmax(-1))\n",
    "\n",
    "    def _softmax_q_trans(self, q):\n",
    "        q_shape = q.shape\n",
    "        return F.softmax(q.reshape(q_shape[0], -1), dim=1).reshape(q_shape)\n",
    "\n",
    "    def _softmax_q_rot_grip(self, q_rot_grip):\n",
    "        q_rot_x_flat = q_rot_grip[:, 0*self._num_rotation_classes: 1*self._num_rotation_classes]\n",
    "        q_rot_y_flat = q_rot_grip[:, 1*self._num_rotation_classes: 2*self._num_rotation_classes]\n",
    "        q_rot_z_flat = q_rot_grip[:, 2*self._num_rotation_classes: 3*self._num_rotation_classes]\n",
    "        q_grip_flat  = q_rot_grip[:, 3*self._num_rotation_classes:]\n",
    "\n",
    "        q_rot_x_flat_softmax = F.softmax(q_rot_x_flat, dim=1)\n",
    "        q_rot_y_flat_softmax = F.softmax(q_rot_y_flat, dim=1)\n",
    "        q_rot_z_flat_softmax = F.softmax(q_rot_z_flat, dim=1)\n",
    "        q_grip_flat_softmax = F.softmax(q_grip_flat, dim=1)\n",
    "\n",
    "        return torch.cat([q_rot_x_flat_softmax,\n",
    "                          q_rot_y_flat_softmax,\n",
    "                          q_rot_z_flat_softmax,\n",
    "                          q_grip_flat_softmax], dim=1)\n",
    "\n",
    "    def _softmax_ignore_collision(self, q_collision):\n",
    "        q_collision_softmax = F.softmax(q_collision, dim=1)\n",
    "        return q_collision_softmax\n",
    "\n",
    "    def save_gradient(self, module, grad_input, grad_output):\n",
    "        \"\"\"\n",
    "        for grad-cam\n",
    "        \"\"\"\n",
    "        self.gradient = grad_output[0]\n",
    "\n",
    "    def save_feature(self, module, input, output):\n",
    "        \"\"\"\n",
    "        for grad-cam\n",
    "        \"\"\"\n",
    "        if isinstance(output, tuple):\n",
    "            self.feature = output[0].detach()\n",
    "        else:\n",
    "            self.feature = output.detach()\n",
    "            \n",
    "    def sim_to_colmap(self,extrinsics):\n",
    "        R = extrinsics[:3, :3]\n",
    "        t = extrinsics[:3, 3]\n",
    "\n",
    "        # Invert the rotation matrix (transpose, since it's orthogonal)\n",
    "        R_inv = R.T\n",
    "\n",
    "        # Compute the new translation vector\n",
    "        # t_inv = -np.dot(R_inv, t)\n",
    "        R_flip = Rotation.from_euler('z', 180, degrees=True).as_matrix()\n",
    "        R_inv = R_flip @ R_inv\n",
    "\n",
    "        t_inv = -R_inv @ t\n",
    "\n",
    "\n",
    "        # Form the inverted transformation matrix\n",
    "        extrinsics = np.eye(4)\n",
    "        # print(extrinsics)\n",
    "        extrinsics[:3, :3] = R_inv\n",
    "        extrinsics[:3, 3] = t_inv\n",
    "        \n",
    "        return extrinsics\n",
    "\n",
    "    def update(self, step: int, replay_sample: dict, fabric: Fabric = None) -> dict:\n",
    "        '''\n",
    "        : replay_sample.action: [bs, 8]\n",
    "        : replay_sample['trans_action_indicies']: [bs, 3]\n",
    "        '''\n",
    "        action_gt = replay_sample['action'] # [bs, 8]\n",
    "        action_trans = replay_sample['trans_action_indicies'][:, self._layer * 3:self._layer * 3 + 3].int()\n",
    "        action_rot_grip = replay_sample['rot_grip_action_indicies'].int()\n",
    "        action_gripper_pose = replay_sample['gripper_pose']\n",
    "        action_ignore_collisions = replay_sample['ignore_collisions'].int()\n",
    "        lang_goal_emb = replay_sample['lang_goal_emb'].float()\n",
    "        lang_token_embs = replay_sample['lang_token_embs'].float()\n",
    "        prev_layer_voxel_grid = replay_sample.get('prev_layer_voxel_grid', None)\n",
    "        prev_layer_bounds = replay_sample.get('prev_layer_bounds', None)\n",
    "        lang_goal = replay_sample['lang_goal']\n",
    "\n",
    "        # obs_dict['lang_goal_emb'] = sentence_emb[0].float().detach().cpu().numpy()\n",
    "        # obs_dict['lang_token_embs'] = token_embs[0].float().detach().cpu().numpy()\n",
    "        # obs_dict['lang_goal'] = np.array([description], dtype=object) # add this for usage in diffusion model\n",
    "\n",
    "        device = self._device\n",
    "\n",
    "        # get rank by device id\n",
    "        rank = self._q.device\n",
    "\n",
    "        obs, depth, pcd, mask, extrinsics, intrinsics = self._preprocess_inputs(replay_sample)\n",
    "        # batch size\n",
    "        bs = pcd[0].shape[0]\n",
    "\n",
    "        # for nerf multi-view training\n",
    "        nerf_multi_view_rgb_path = replay_sample['nerf_multi_view_rgb'] # only succeed to get path sometime\n",
    "        nerf_multi_view_depth_path = replay_sample['nerf_multi_view_depth']\n",
    "        nerf_multi_view_camera_path = replay_sample['nerf_multi_view_camera']\n",
    "        nerf_multi_view_mask_path = replay_sample['nerf_multi_view_mask']\n",
    "\n",
    "        nerf_next_multi_view_rgb_path = replay_sample['nerf_next_multi_view_rgb']\n",
    "        nerf_next_multi_view_depth_path = replay_sample['nerf_next_multi_view_depth']\n",
    "        nerf_next_multi_view_camera_path = replay_sample['nerf_next_multi_view_camera']\n",
    "        nerf_next_multi_view_mask_path = replay_sample['nerf_next_multi_view_mask']\n",
    "\n",
    "        # cprint(nerf_multi_view_rgb_path, 'red')\n",
    "\n",
    "        if nerf_multi_view_rgb_path is None or nerf_multi_view_rgb_path[0,0] is None:\n",
    "            cprint(nerf_multi_view_rgb_path, 'red')\n",
    "            cprint(replay_sample['indices'], 'red')\n",
    "            nerf_target_rgb = None\n",
    "            nerf_target_camera_extrinsic = None\n",
    "            print(colored('warn: one iter not use additional multi view', 'cyan'))\n",
    "            raise ValueError('nerf_multi_view_rgb_path is None')\n",
    "        else:\n",
    "            # control the number of views by the following code\n",
    "            num_view = nerf_multi_view_rgb_path.shape[-1]\n",
    "            num_view_by_user = self.cfg.num_view_for_nerf\n",
    "            # compute interval first\n",
    "            assert num_view_by_user <= num_view, f'num_view_by_user {num_view_by_user} should be less than num_view {num_view}'\n",
    "            interval = num_view // num_view_by_user # 21//20?\n",
    "            nerf_multi_view_rgb_path = nerf_multi_view_rgb_path[:, ::interval]\n",
    "            \n",
    "            # sample one target img\n",
    "            view_dix = np.random.randint(0, num_view_by_user)\n",
    "            nerf_multi_view_rgb_path = nerf_multi_view_rgb_path[:, view_dix]\n",
    "            nerf_multi_view_depth_path = nerf_multi_view_depth_path[:, view_dix]\n",
    "            nerf_multi_view_camera_path = nerf_multi_view_camera_path[:, view_dix]\n",
    "            nerf_multi_view_mask_path = nerf_multi_view_mask_path[:, view_dix]\n",
    "\n",
    "            next_view_dix = np.random.randint(0, num_view_by_user)\n",
    "            nerf_next_multi_view_rgb_path = nerf_next_multi_view_rgb_path[:, next_view_dix]\n",
    "            nerf_next_multi_view_depth_path = nerf_next_multi_view_depth_path[:, next_view_dix]\n",
    "            nerf_next_multi_view_camera_path = nerf_next_multi_view_camera_path[:, next_view_dix]\n",
    "            nerf_next_multi_view_mask_path = nerf_next_multi_view_mask_path[:, view_dix]\n",
    "\n",
    "            # load img and camera (support bs>1)\n",
    "            nerf_target_rgbs, nerf_target_depths, nerf_target_masks, nerf_target_camera_extrinsics, nerf_target_camera_intrinsics = [], [], [], [], []\n",
    "            nerf_next_target_rgbs, nerf_next_target_depths, nerf_next_target_masks, nerf_next_target_camera_extrinsics, nerf_next_target_camera_intrinsics = [], [], [], [], []\n",
    "            for i in range(bs):\n",
    "                nerf_target_rgbs.append(parse_img_file(nerf_multi_view_rgb_path[i], mask_gt_rgb=self._mask_gt_rgb))#, session=self._rembg_session))    # FIXME: file_path 'NoneType' object has no attribute 'read'\n",
    "                nerf_target_masks.append(parse_mask_file(nerf_multi_view_mask_path[i]))\n",
    "                nerf_target_depths.append(parse_depth_file(nerf_multi_view_depth_path[i]))\n",
    "                nerf_target_camera_extrinsic, nerf_target_camera_intrinsic, nerf_target_focal = parse_camera_file(nerf_multi_view_camera_path[i])\n",
    "                # nerf_target_camera_extrinsic = self.sim_to_colmap(nerf_target_camera_extrinsic)\n",
    "                nerf_target_camera_extrinsics.append(nerf_target_camera_extrinsic)\n",
    "                nerf_target_camera_intrinsics.append(nerf_target_camera_intrinsic)\n",
    "\n",
    "                nerf_next_target_rgbs.append(parse_img_file(nerf_next_multi_view_rgb_path[i], mask_gt_rgb=self._mask_gt_rgb))#, session=self._rembg_session))    # FIXME: file_path 'NoneType' object has no attribute 'read'\n",
    "                nerf_next_target_masks.append(parse_mask_file(nerf_next_multi_view_mask_path[i]))\n",
    "                nerf_next_target_depths.append(parse_depth_file(nerf_next_multi_view_depth_path[i]))\n",
    "                nerf_next_target_camera_extrinsic, nerf_next_target_camera_intrinsic, nerf_next_target_focal = parse_camera_file(nerf_next_multi_view_camera_path[i])\n",
    "                # nerf_next_target_camera_extrinsic = self.sim_to_colmap(nerf_next_target_camera_extrinsic)\n",
    "                nerf_next_target_camera_extrinsics.append(nerf_next_target_camera_extrinsic)\n",
    "                nerf_next_target_camera_intrinsics.append(nerf_next_target_camera_intrinsic)\n",
    "\n",
    "            nerf_target_rgb = torch.from_numpy(np.stack(nerf_target_rgbs)).float().to(device) # [bs, H, W, 3], [0,1]\n",
    "            nerf_target_depth = torch.from_numpy(np.stack(nerf_target_depths)).float().to(device) # [bs, H, W, 1], no normalization\n",
    "            nerf_target_mask = torch.from_numpy(np.stack(nerf_target_masks)).float().to(device)\n",
    "            nerf_target_camera_extrinsic = torch.from_numpy(np.stack(nerf_target_camera_extrinsics)).float().to(device)\n",
    "            nerf_target_camera_intrinsic = torch.from_numpy(np.stack(nerf_target_camera_intrinsics)).float().to(device)\n",
    "\n",
    "            nerf_next_target_rgb = torch.from_numpy(np.stack(nerf_next_target_rgbs)).float().to(device) # [bs, H, W, 3], [0,1]\n",
    "            nerf_next_target_depth = torch.from_numpy(np.stack(nerf_next_target_depths)).float().to(device) # [bs, H, W, 1], no normalization\n",
    "            nerf_next_target_mask = torch.from_numpy(np.stack(nerf_next_target_masks)).float().to(device)\n",
    "            nerf_next_target_camera_extrinsic = torch.from_numpy(np.stack(nerf_next_target_camera_extrinsics)).float().to(device)\n",
    "            nerf_next_target_camera_intrinsic = torch.from_numpy(np.stack(nerf_next_target_camera_intrinsics)).float().to(device)\n",
    "            \n",
    "\n",
    "\n",
    "        # fig, ax = plt.subplots(1,2,figsize=(15,15))\n",
    "        # ax[0].set_title('rgb')\n",
    "        # ax[0].imshow(nerf_target_rgb.cpu()[0])\n",
    "        # ax[1].set_title('mask')\n",
    "        # ax[1].imshow(nerf_target_mask.cpu()[0])\n",
    "        # plt.show()\n",
    "\n",
    "        \n",
    "        bounds = self._coordinate_bounds.to(device)\n",
    "        if self._layer > 0:\n",
    "            cp = replay_sample['attention_coordinate_layer_%d' % (self._layer - 1)]\n",
    "            bounds = torch.cat([cp - self._bounds_offset, cp + self._bounds_offset], dim=1)\n",
    "\n",
    "        proprio = None\n",
    "        if self._include_low_dim_state:\n",
    "            proprio = replay_sample['low_dim_state']\n",
    "\n",
    "        # SE(3) augmentation of point clouds and actions\n",
    "        if self._transform_augmentation:    # default: True\n",
    "            action_trans, \\\n",
    "            action_rot_grip, \\\n",
    "            pcd,\\\n",
    "            extrinsics = apply_se3_augmentation_with_camera_pose(pcd,\n",
    "                                         extrinsics,\n",
    "                                         action_gripper_pose,\n",
    "                                         action_trans,\n",
    "                                         action_rot_grip,\n",
    "                                         bounds,\n",
    "                                         self._layer,\n",
    "                                         self._transform_augmentation_xyz,\n",
    "                                         self._transform_augmentation_rpy,\n",
    "                                         self._transform_augmentation_rot_resolution,\n",
    "                                         self._voxel_size,\n",
    "                                         self._rotation_resolution,\n",
    "                                         self._device)\n",
    "\n",
    "        # forward pass\n",
    "        q_trans, q_rot_grip, \\\n",
    "        q_collision, \\\n",
    "        voxel_grid, \\\n",
    "        rendering_loss_dict = self._q(obs,\n",
    "                                depth,\n",
    "                                proprio,\n",
    "                                pcd,\n",
    "                                mask,\n",
    "                                extrinsics, # although augmented, not used\n",
    "                                intrinsics,\n",
    "                                lang_goal_emb,\n",
    "                                lang_token_embs,\n",
    "                                bounds,\n",
    "                                prev_layer_bounds,\n",
    "                                prev_layer_voxel_grid,\n",
    "                                use_neural_rendering=self.use_neural_rendering,\n",
    "                                nerf_target_rgb=nerf_target_rgb,\n",
    "                                nerf_target_depth=nerf_target_depth,\n",
    "                                nerf_target_mask = nerf_target_mask,\n",
    "                                nerf_target_pose=nerf_target_camera_extrinsic,\n",
    "                                nerf_target_camera_intrinsic=nerf_target_camera_intrinsic,\n",
    "                                lang_goal=lang_goal,\n",
    "                                nerf_next_target_rgb=nerf_next_target_rgb,\n",
    "                                nerf_next_target_depth=nerf_next_target_depth,\n",
    "                                nerf_next_target_mask = nerf_next_target_mask,\n",
    "                                nerf_next_target_pose=nerf_next_target_camera_extrinsic,\n",
    "                                nerf_next_target_camera_intrinsic=nerf_next_target_camera_intrinsic,\n",
    "                                step=step,\n",
    "                                action=action_gt,\n",
    "                                )\n",
    "        # argmax to choose best action\n",
    "        # coords, \\\n",
    "        # rot_and_grip_indicies, \\\n",
    "        # ignore_collision_indicies = self._q.choose_highest_action(q_trans, q_rot_grip, q_collision)\n",
    "\n",
    "        # q_trans_loss, q_rot_loss, q_grip_loss, q_collision_loss = 0., 0., 0., 0.\n",
    "\n",
    "        # # translation one-hot\n",
    "        # action_trans_one_hot = self._action_trans_one_hot_zeros.clone()\n",
    "        # # action_trans: [bs, 3]\n",
    "        # for b in range(bs):\n",
    "        #     gt_coord = action_trans[b, :].int()\n",
    "        #     action_trans_one_hot[b, :, gt_coord[0], gt_coord[1], gt_coord[2]] = 1\n",
    "            \n",
    "        # # translation loss\n",
    "        # q_trans_flat = q_trans.view(bs, -1)\n",
    "        # action_trans_one_hot_flat = action_trans_one_hot.view(bs, -1)   # [1,1,100,100,100]\n",
    "        # q_trans_loss = self._celoss(q_trans_flat, action_trans_one_hot_flat)\n",
    "\n",
    "        # with_rot_and_grip = rot_and_grip_indicies is not None\n",
    "        # if with_rot_and_grip:\n",
    "        #     # rotation, gripper, and collision one-hots\n",
    "        #     action_rot_x_one_hot = self._action_rot_x_one_hot_zeros.clone()\n",
    "        #     action_rot_y_one_hot = self._action_rot_y_one_hot_zeros.clone()\n",
    "        #     action_rot_z_one_hot = self._action_rot_z_one_hot_zeros.clone()\n",
    "        #     action_grip_one_hot = self._action_grip_one_hot_zeros.clone()\n",
    "        #     action_ignore_collisions_one_hot = self._action_ignore_collisions_one_hot_zeros.clone()\n",
    "\n",
    "        #     for b in range(bs):\n",
    "        #         gt_rot_grip = action_rot_grip[b, :].int()\n",
    "        #         action_rot_x_one_hot[b, gt_rot_grip[0]] = 1\n",
    "        #         action_rot_y_one_hot[b, gt_rot_grip[1]] = 1\n",
    "        #         action_rot_z_one_hot[b, gt_rot_grip[2]] = 1\n",
    "        #         action_grip_one_hot[b, gt_rot_grip[3]] = 1\n",
    "\n",
    "        #         gt_ignore_collisions = action_ignore_collisions[b, :].int()\n",
    "        #         action_ignore_collisions_one_hot[b, gt_ignore_collisions[0]] = 1\n",
    "\n",
    "        #     # flatten predictions\n",
    "        #     q_rot_x_flat = q_rot_grip[:, 0*self._num_rotation_classes:1*self._num_rotation_classes]\n",
    "        #     q_rot_y_flat = q_rot_grip[:, 1*self._num_rotation_classes:2*self._num_rotation_classes]\n",
    "        #     q_rot_z_flat = q_rot_grip[:, 2*self._num_rotation_classes:3*self._num_rotation_classes]\n",
    "        #     q_grip_flat =  q_rot_grip[:, 3*self._num_rotation_classes:]\n",
    "        #     q_ignore_collisions_flat = q_collision\n",
    "\n",
    "        #     # rotation loss\n",
    "        #     q_rot_loss += self._celoss(q_rot_x_flat, action_rot_x_one_hot)\n",
    "        #     q_rot_loss += self._celoss(q_rot_y_flat, action_rot_y_one_hot)\n",
    "        #     q_rot_loss += self._celoss(q_rot_z_flat, action_rot_z_one_hot)\n",
    "\n",
    "        #     # gripper loss\n",
    "        #     q_grip_loss += self._celoss(q_grip_flat, action_grip_one_hot)\n",
    "\n",
    "        #     # collision loss\n",
    "        #     q_collision_loss += self._celoss(q_ignore_collisions_flat, action_ignore_collisions_one_hot)\n",
    "\n",
    "        # combined_losses = (q_trans_loss * self._trans_loss_weight) + \\\n",
    "        #                   (q_rot_loss * self._rot_loss_weight) + \\\n",
    "        #                   (q_grip_loss * self._grip_loss_weight) + \\\n",
    "        #                   (q_collision_loss * self._collision_loss_weight)\n",
    "        # total_loss = combined_losses.mean()\n",
    "\n",
    "        if self.use_neural_rendering:   # eval default: False; train default: True\n",
    "            \n",
    "            lambda_nerf = self.cfg.neural_renderer.lambda_nerf\n",
    "            lambda_BC = self.cfg.lambda_bc\n",
    "            # print(f'use_neural_rendering: {self.use_neural_rendering}')\n",
    "            # total_loss = lambda_BC * total_loss + lambda_nerf * rendering_loss_dict['loss']\n",
    "            total_loss = lambda_nerf * rendering_loss_dict['loss']\n",
    "\n",
    "            # for print\n",
    "            loss_rgb_item = rendering_loss_dict['loss_rgb']\n",
    "            loss_embed_item = rendering_loss_dict['loss_embed']\n",
    "            loss_dyna_item = rendering_loss_dict['loss_dyna']\n",
    "            loss_reg_item = rendering_loss_dict['loss_reg']\n",
    "            psnr = rendering_loss_dict['psnr']\n",
    "\n",
    "            lambda_embed = self.cfg.neural_renderer.lambda_embed * lambda_nerf  # 0.0001\n",
    "            lambda_rgb = self.cfg.neural_renderer.lambda_rgb * lambda_nerf  # 0.01\n",
    "            lambda_dyna = (self.cfg.neural_renderer.lambda_dyna if step >= self.cfg.neural_renderer.next_mlp.warm_up else 0.) * lambda_nerf  # 0.01\n",
    "            lambda_reg = (self.cfg.neural_renderer.lambda_reg if step >= self.cfg.neural_renderer.next_mlp.warm_up else 0.) * lambda_nerf  # 0.01\n",
    "\n",
    "            if step % 10 == 0: # and rank == 0:\n",
    "                # cprint(f'total L: {total_loss.item():.4f} | \\\n",
    "                # L_rgb: {loss_rgb_item:.3f} x {lambda_rgb:.3f} | \\\n",
    "                # L_embed: {loss_embed_item:.3f} x {lambda_embed:.4f} | \\\n",
    "                # L_dyna: {loss_dyna_item:.3f} x {lambda_dyna:.4f} | \\\n",
    "                # L_reg: {loss_reg_item:.3f} x {lambda_reg:.4f} | \\\n",
    "                # psnr: {psnr:.3f}', 'green')\n",
    "\n",
    "                # if self.cfg.use_wandb:\n",
    "                wandb.log({\n",
    "                    # 'train/BC_loss':combined_losses.item(), \n",
    "                    'train/psnr':psnr, \n",
    "                    'train/rgb_loss':loss_rgb_item,\n",
    "                    'train/embed_loss':loss_embed_item,\n",
    "                    'train/dyna_loss':loss_dyna_item,\n",
    "                    }, step=step)\n",
    "            \n",
    "\n",
    "      \n",
    "\n",
    "        self._optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        # use fabric ddp\n",
    "        # fabric.backward(total_loss)\n",
    "        self._optimizer.step()\n",
    "\n",
    "        ############### Render in training process #################\n",
    "\n",
    "        render_freq = self.cfg.neural_renderer.render_freq\n",
    "        to_render = (step % render_freq == 0 and self.use_neural_rendering and nerf_target_camera_extrinsic is not None)\n",
    "        if to_render:\n",
    "            rgb_render, next_rgb_render, embed_render, gt_embed_render = self._q.render(\n",
    "                rgb_pcd=obs,\n",
    "                proprio=proprio,\n",
    "                pcd=pcd,\n",
    "                mask=mask,\n",
    "                lang_goal_emb=lang_goal_emb,\n",
    "                lang_token_embs=lang_token_embs,\n",
    "                bounds=bounds,\n",
    "                prev_bounds=prev_layer_bounds,\n",
    "                prev_layer_voxel_grid=prev_layer_voxel_grid,\n",
    "                tgt_pose=nerf_target_camera_extrinsic,\n",
    "                tgt_intrinsic=nerf_target_camera_intrinsic,\n",
    "                nerf_target_rgb=nerf_target_rgb,\n",
    "                lang_goal=lang_goal,\n",
    "                nerf_next_target_rgb=nerf_next_target_rgb,\n",
    "                nerf_next_target_depth=nerf_next_target_depth,\n",
    "                nerf_next_target_pose=nerf_next_target_camera_extrinsic,\n",
    "                nerf_next_target_camera_intrinsic=nerf_next_target_camera_intrinsic,\n",
    "                step=step,\n",
    "                action=action_gt,\n",
    "                \n",
    "                )\n",
    "            \n",
    "            # NOTE: [1, h, w, 3]\n",
    "            rgb_gt = nerf_target_rgb[0]\n",
    "            rgb_render = rgb_render[0]\n",
    "            psnr = PSNR_torch(rgb_render, rgb_gt)\n",
    "            if next_rgb_render is not None:\n",
    "                next_rgb_gt = nerf_next_target_rgb[0]\n",
    "                next_rgb_render = next_rgb_render[0]\n",
    "                psnr_dyna = PSNR_torch(next_rgb_render, next_rgb_gt)\n",
    "\n",
    "            os.makedirs('recon', exist_ok=True)\n",
    "\n",
    "            # plot three images in one row with subplots:\n",
    "            # src, tgt, pred\n",
    "            rgb_src =  obs[0][0].squeeze(0).permute(1, 2, 0)  / 2 + 0.5\n",
    "            fig, axs = plt.subplots(1, 7, figsize=(15, 3))\n",
    "            # src\n",
    "            axs[0].imshow(rgb_src.cpu().numpy())\n",
    "            axs[0].title.set_text('src')\n",
    "            # tgt\n",
    "            axs[1].imshow(rgb_gt.cpu().numpy())\n",
    "            axs[1].title.set_text('tgt')\n",
    "            # pred rgb\n",
    "            axs[2].imshow(rgb_render.cpu().numpy())\n",
    "            axs[2].title.set_text('psnr={:.2f}'.format(psnr))\n",
    "            # pred embed\n",
    "            # embed_render = visualize_feature_map_by_clustering(embed_render.permute(0,3,1,2), num_cluster=4)\n",
    "            embed_render = visualize_feature_map_by_normalization(embed_render.permute(0,3,1,2))    # range from -1 to 1\n",
    "            axs[3].imshow(embed_render)\n",
    "            axs[3].title.set_text('embed seg')\n",
    "            # gt embed\n",
    "            if gt_embed_render is not None:\n",
    "                # gt_embed_render = visualize_feature_map_by_clustering(gt_embed_render, num_cluster=4)\n",
    "                gt_embed_render = visualize_feature_map_by_normalization(gt_embed_render)    # range from -1 to 1\n",
    "                axs[4].imshow(gt_embed_render)\n",
    "                axs[4].title.set_text('gt embed seg')\n",
    "            if next_rgb_render is not None:\n",
    "                # gt next rgb frame\n",
    "                axs[6].imshow(next_rgb_gt.cpu().numpy())\n",
    "                axs[6].title.set_text('next tgt')\n",
    "                # Ours\n",
    "                axs[5].imshow(next_rgb_render.cpu().numpy())\n",
    "                axs[5].title.set_text('next psnr={:.2f}'.format(psnr_dyna))\n",
    "            # remove axis\n",
    "            for ax in axs:\n",
    "                ax.axis('off')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # if rank == 0:\n",
    "            #     if self.cfg.use_wandb:\n",
    "            #         # save to buffer and write to wandb\n",
    "            buf = io.BytesIO()\n",
    "            plt.savefig(buf, format='png')\n",
    "            buf.seek(0)\n",
    "\n",
    "            image = Image.open(buf)\n",
    "            wandb.log({\"eval/recon_img\": wandb.Image(image)}, step=step)\n",
    "\n",
    "            #         buf.close()\n",
    "            #         workdir = os.getcwd()\n",
    "            #         cprint(f'Saved {workdir}/recon/{step}_rgb.png to wandb', 'cyan')\n",
    "            #     else:\n",
    "            # print(f'step: {step}')\n",
    "            \n",
    "            plt.savefig(f'recon_gs/{step}_rgb.png')\n",
    "            workdir = os.getcwd()\n",
    "            cprint(f'Saved {workdir}/recon_gs/{step}_rgb.png locally', 'cyan')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        # self._summaries = {\n",
    "        #     'losses/total_loss': total_loss.item(),\n",
    "        #     'losses/trans_loss': q_trans_loss.mean(),\n",
    "        #     'losses/rot_loss': q_rot_loss.mean() if with_rot_and_grip else 0.,\n",
    "        #     'losses/grip_loss': q_grip_loss.mean() if with_rot_and_grip else 0.,\n",
    "        #     'losses/collision_loss': q_collision_loss.mean() if with_rot_and_grip else 0.,\n",
    "        # }\n",
    "\n",
    "        # self._wandb_summaries = {\n",
    "        #     'losses/total_loss': total_loss.item(),\n",
    "        #     'losses/trans_loss': q_trans_loss.mean(),\n",
    "        #     'losses/rot_loss': q_rot_loss.mean() if with_rot_and_grip else 0.,\n",
    "        #     'losses/grip_loss': q_grip_loss.mean() if with_rot_and_grip else 0.,\n",
    "        #     'losses/collision_loss': q_collision_loss.mean() if with_rot_and_grip else 0.,\n",
    "\n",
    "        #     # for visualization\n",
    "        #     'point_cloud': None,\n",
    "        #     'coord_pred': coords,\n",
    "        #     'coord_gt': gt_coord,\n",
    "        # }\n",
    "\n",
    "        if self._lr_scheduler:\n",
    "            self._scheduler.step()\n",
    "            # self._summaries['learning_rate'] = self._scheduler.get_last_lr()[0]\n",
    "\n",
    "        self._vis_voxel_grid = voxel_grid[0]        \n",
    "        \n",
    "        # self._vis_translation_qvalue = self._softmax_q_trans(q_trans[0])\n",
    "        # self._vis_max_coordinate = coords[0]\n",
    "        # self._vis_gt_coordinate = action_trans[0]\n",
    "\n",
    "        # Note: PerAct doesn't use multi-layer voxel grids like C2FARM\n",
    "        # stack prev_layer_voxel_grid(s) from previous layers into a list\n",
    "        if prev_layer_voxel_grid is None:\n",
    "            prev_layer_voxel_grid = [voxel_grid]\n",
    "        else:\n",
    "            prev_layer_voxel_grid = prev_layer_voxel_grid + [voxel_grid]\n",
    "\n",
    "        # stack prev_layer_bound(s) from previous layers into a list\n",
    "        if prev_layer_bounds is None:\n",
    "            prev_layer_bounds = [self._coordinate_bounds.repeat(bs, 1)]\n",
    "        else:\n",
    "            prev_layer_bounds = prev_layer_bounds + [bounds]\n",
    "\n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'prev_layer_voxel_grid': prev_layer_voxel_grid,\n",
    "            'prev_layer_bounds': prev_layer_bounds,\n",
    "        }\n",
    "\n",
    "    def act(self, step: int, observation: dict,\n",
    "            deterministic=False) -> ActResult:\n",
    "        deterministic = True\n",
    "        bounds = self._coordinate_bounds\n",
    "        prev_layer_voxel_grid = observation.get('prev_layer_voxel_grid', None)\n",
    "        prev_layer_bounds = observation.get('prev_layer_bounds', None)\n",
    "        lang_goal_tokens = observation.get('lang_goal_tokens', None).long()\n",
    "        lang_goal = observation['lang_goal']\n",
    "\n",
    "        # extract language embs\n",
    "        with torch.no_grad():\n",
    "            lang_goal_emb, lang_token_embs = self.language_model.extract(lang_goal)\n",
    "\n",
    "        # voxelization resolution\n",
    "        res = (bounds[:, 3:] - bounds[:, :3]) / self._voxel_size\n",
    "        max_rot_index = int(360 // self._rotation_resolution)\n",
    "        proprio = None\n",
    "\n",
    "        if self._include_low_dim_state:\n",
    "            proprio = observation['low_dim_state']\n",
    "\n",
    "        obs, depth, pcd, extrinsics, intrinsics = self._act_preprocess_inputs(observation)\n",
    "\n",
    "        # correct batch size and device\n",
    "        obs = [[o[0][0].to(self._device), o[1][0].to(self._device)] for o in obs]\n",
    "        proprio = proprio[0].to(self._device)\n",
    "        pcd = [p[0].to(self._device) for p in pcd]\n",
    "        lang_goal_emb = lang_goal_emb.to(self._device)\n",
    "        lang_token_embs = lang_token_embs.to(self._device)\n",
    "        bounds = torch.as_tensor(bounds, device=self._device)\n",
    "        prev_layer_voxel_grid = prev_layer_voxel_grid.to(self._device) if prev_layer_voxel_grid is not None else None\n",
    "        prev_layer_bounds = prev_layer_bounds.to(self._device) if prev_layer_bounds is not None else None\n",
    "\n",
    "        # inference\n",
    "        q_trans, \\\n",
    "        q_rot_grip, \\\n",
    "        q_ignore_collisions, \\\n",
    "        vox_grid,\\\n",
    "        rendering_loss_dict  = self._q(obs,\n",
    "                            depth,\n",
    "                            proprio,\n",
    "                            pcd,\n",
    "                            extrinsics,\n",
    "                            intrinsics,\n",
    "                            lang_goal_emb,\n",
    "                            lang_token_embs,\n",
    "                            bounds,\n",
    "                            prev_layer_bounds,\n",
    "                            prev_layer_voxel_grid, \n",
    "                            use_neural_rendering=False)\n",
    "\n",
    "        # softmax Q predictions\n",
    "        q_trans = self._softmax_q_trans(q_trans)\n",
    "        q_rot_grip =  self._softmax_q_rot_grip(q_rot_grip) if q_rot_grip is not None else q_rot_grip\n",
    "        q_ignore_collisions = self._softmax_ignore_collision(q_ignore_collisions) \\\n",
    "            if q_ignore_collisions is not None else q_ignore_collisions\n",
    "\n",
    "        # argmax Q predictions\n",
    "        coords, \\\n",
    "        rot_and_grip_indicies, \\\n",
    "        ignore_collisions = self._q.choose_highest_action(q_trans, q_rot_grip, q_ignore_collisions)\n",
    "\n",
    "        rot_grip_action = rot_and_grip_indicies if q_rot_grip is not None else None\n",
    "        ignore_collisions_action = ignore_collisions.int() if ignore_collisions is not None else None\n",
    "\n",
    "        coords = coords.int()\n",
    "        attention_coordinate = bounds[:, :3] + res * coords + res / 2\n",
    "\n",
    "        # stack prev_layer_voxel_grid(s) into a list\n",
    "        # NOTE: PerAct doesn't used multi-layer voxel grids like C2FARM\n",
    "        if prev_layer_voxel_grid is None:\n",
    "            prev_layer_voxel_grid = [vox_grid]\n",
    "        else:\n",
    "            prev_layer_voxel_grid = prev_layer_voxel_grid + [vox_grid]\n",
    "\n",
    "        if prev_layer_bounds is None:\n",
    "            prev_layer_bounds = [bounds]\n",
    "        else:\n",
    "            prev_layer_bounds = prev_layer_bounds + [bounds]\n",
    "\n",
    "        observation_elements = {\n",
    "            'attention_coordinate': attention_coordinate,\n",
    "            'prev_layer_voxel_grid': prev_layer_voxel_grid,\n",
    "            'prev_layer_bounds': prev_layer_bounds,\n",
    "        }\n",
    "        info = {\n",
    "            'voxel_grid_depth%d' % self._layer: vox_grid,\n",
    "            'q_depth%d' % self._layer: q_trans,\n",
    "            'voxel_idx_depth%d' % self._layer: coords\n",
    "        }\n",
    "        self._act_voxel_grid = vox_grid[0]\n",
    "        self._act_max_coordinate = coords[0]\n",
    "        self._act_qvalues = q_trans[0].detach()\n",
    "        return ActResult((coords, rot_grip_action, ignore_collisions_action),\n",
    "                         observation_elements=observation_elements,\n",
    "                         info=info)\n",
    "\n",
    "    def update_summaries(self) -> List[Summary]:\n",
    "        summaries = []\n",
    "\n",
    "        for n, v in self._summaries.items():\n",
    "            summaries.append(ScalarSummary('%s/%s' % (self._name, n), v))\n",
    "\n",
    "        for (name, crop) in (self._crop_summary):\n",
    "            crops = (torch.cat(torch.split(crop, 3, dim=1), dim=3) + 1.0) / 2.0\n",
    "            summaries.extend([\n",
    "                ImageSummary('%s/crops/%s' % (self._name, name), crops)])\n",
    "\n",
    "        for tag, param in self._q.named_parameters():\n",
    "            # assert not torch.isnan(param.grad.abs() <= 1.0).all()\n",
    "            if param.grad is None:\n",
    "                continue\n",
    "\n",
    "            summaries.append(\n",
    "                HistogramSummary('%s/gradient/%s' % (self._name, tag),\n",
    "                                 param.grad))\n",
    "            summaries.append(\n",
    "                HistogramSummary('%s/weight/%s' % (self._name, tag),\n",
    "                                 param.data))\n",
    "\n",
    "        return summaries\n",
    "    \n",
    "    \n",
    "    def update_wandb_summaries(self):\n",
    "        summaries = dict()\n",
    "\n",
    "        for k, v in self._wandb_summaries.items():\n",
    "            summaries[k] = v\n",
    "        return summaries\n",
    "\n",
    "\n",
    "    def act_summaries(self) -> List[Summary]:\n",
    "        return [\n",
    "            ImageSummary('%s/act_Qattention' % self._name,\n",
    "                         transforms.ToTensor()(visualise_voxel(\n",
    "                             self._act_voxel_grid.cpu().numpy(),\n",
    "                             self._act_qvalues.cpu().numpy(),\n",
    "                             self._act_max_coordinate.cpu().numpy())))]\n",
    "        # return []\n",
    "\n",
    "\n",
    "    def load_weights(self, savedir: str):\n",
    "        device = self._device if not self._training else torch.device('cuda:%d' % self._device)\n",
    "        # if device is str, convert it to torch.device\n",
    "        if isinstance(device, int):\n",
    "            device = torch.device('cuda:%d' % self._device)\n",
    "\n",
    "        weight_file = os.path.join(savedir, '%s.pt' % self._name)\n",
    "        state_dict = torch.load(weight_file, map_location=device)\n",
    "\n",
    "        # load only keys that are in the current model\n",
    "        merged_state_dict = self._q.state_dict()\n",
    "        for k, v in state_dict.items():\n",
    "            if not self._training:\n",
    "                k = k.replace('_qnet.module', '_qnet')\n",
    "                k = k.replace('_neural_renderer.module', '_neural_renderer')\n",
    "            if k in merged_state_dict:\n",
    "                merged_state_dict[k] = v\n",
    "            else:\n",
    "                if '_voxelizer' not in k: #and '_neural_renderer' not in k:\n",
    "                    logging.warning(f\"key {k} is found in checkpoint, but not found in current model.\")\n",
    "        msg = self._q.load_state_dict(merged_state_dict, strict=False)\n",
    "        # msg = self._q.load_state_dict(merged_state_dict, strict=True)\n",
    "        if msg.missing_keys:\n",
    "            print(\"missing some keys...\")\n",
    "        if msg.unexpected_keys:\n",
    "            print(\"unexpected some keys...\")\n",
    "        print(\"loaded weights from %s\" % weight_file)\n",
    "\n",
    "\n",
    "    def save_weights(self, savedir: str):\n",
    "        torch.save(\n",
    "            self._q.state_dict(), os.path.join(savedir, '%s.pt' % self._name))\n",
    "    \n",
    "    \n",
    "    def load_clip(self):\n",
    "        model, _ = load_clip(\"RN50\", jit=False)\n",
    "        self._clip_rn50 = build_model(model.state_dict())\n",
    "        self._clip_rn50 = self._clip_rn50.float().to(self._device)\n",
    "        self._clip_rn50.eval() \n",
    "        del model\n",
    "\n",
    "\n",
    "    def unload_clip(self):\n",
    "        del self._clip_rn50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qattention stack agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from yarr.agents.agent import Agent, ActResult, Summary\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from helpers import utils\n",
    "# from agents.manigaussian_bc.qattention_manigaussian_bc_agent import QAttentionPerActBCAgent\n",
    "\n",
    "from termcolor import cprint\n",
    "\n",
    "NAME = 'QAttentionStackAgent'\n",
    "\n",
    "\n",
    "class QAttentionStackAgent(Agent):\n",
    "\n",
    "    def __init__(self,\n",
    "                 qattention_agents: List[QAttentionPerActBCAgent],\n",
    "                 rotation_resolution: float,\n",
    "                 camera_names: List[str],\n",
    "                 rotation_prediction_depth: int = 0):\n",
    "        super(QAttentionStackAgent, self).__init__()\n",
    "        self._qattention_agents = qattention_agents\n",
    "        self._rotation_resolution = rotation_resolution\n",
    "        self._camera_names = camera_names\n",
    "        self._rotation_prediction_depth = rotation_prediction_depth\n",
    "\n",
    "    def build(self, training: bool, device=None, use_ddp=True, **kwargs) -> None:\n",
    "        self._device = device\n",
    "        if self._device is None:\n",
    "            self._device = torch.device('cpu')\n",
    "        for qa in self._qattention_agents:\n",
    "            qa.build(training, device, use_ddp, **kwargs)\n",
    "\n",
    "    def update(self, step: int, replay_sample: dict, **kwargs) -> dict:\n",
    "        priorities = 0\n",
    "        total_losses = 0.\n",
    "\n",
    "                \n",
    "        if replay_sample['nerf_multi_view_rgb'] is None or replay_sample['nerf_multi_view_rgb'][0,0] is None:\n",
    "            cprint(\"stack agent no nerf rgb\", \"red\")\n",
    "\n",
    "\n",
    "        for qa in self._qattention_agents:\n",
    "            update_dict = qa.update(step, replay_sample, **kwargs)\n",
    "            replay_sample.update(update_dict)\n",
    "            total_losses += update_dict['total_loss']\n",
    "        return {\n",
    "            'total_losses': total_losses,\n",
    "        }\n",
    "\n",
    "    def act(self, step: int, observation: dict,\n",
    "            deterministic=False) -> ActResult:\n",
    "\n",
    "        observation_elements = {}\n",
    "        translation_results, rot_grip_results, ignore_collisions_results = [], [], []\n",
    "        infos = {}\n",
    "        for depth, qagent in enumerate(self._qattention_agents):\n",
    "            act_results = qagent.act(step, observation, deterministic)\n",
    "            \n",
    "            attention_coordinate = act_results.observation_elements['attention_coordinate'].cpu().numpy()\n",
    "\n",
    "            observation_elements['attention_coordinate_layer_%d' % depth] = attention_coordinate[0]\n",
    "\n",
    "            translation_idxs, rot_grip_idxs, ignore_collisions_idxs = act_results.action\n",
    "            translation_results.append(translation_idxs)\n",
    "            if rot_grip_idxs is not None:\n",
    "                rot_grip_results.append(rot_grip_idxs)\n",
    "            if ignore_collisions_idxs is not None:\n",
    "                ignore_collisions_results.append(ignore_collisions_idxs)\n",
    "\n",
    "            observation['attention_coordinate'] = act_results.observation_elements['attention_coordinate']\n",
    "            observation['prev_layer_voxel_grid'] = act_results.observation_elements['prev_layer_voxel_grid']\n",
    "            observation['prev_layer_bounds'] = act_results.observation_elements['prev_layer_bounds']\n",
    "\n",
    "            for n in self._camera_names:\n",
    "                px, py = utils.point_to_pixel_index(\n",
    "                    attention_coordinate[0],\n",
    "                    observation['%s_camera_extrinsics' % n][0, 0].cpu().numpy(),\n",
    "                    observation['%s_camera_intrinsics' % n][0, 0].cpu().numpy())\n",
    "                pc_t = torch.tensor([[[py, px]]], dtype=torch.float32, device=self._device)\n",
    "                observation['%s_pixel_coord' % n] = pc_t\n",
    "                observation_elements['%s_pixel_coord' % n] = [py, px]\n",
    "\n",
    "            infos.update(act_results.info)\n",
    "\n",
    "        rgai = torch.cat(rot_grip_results, 1)[0].cpu().numpy()\n",
    "        ignore_collisions = float(torch.cat(ignore_collisions_results, 1)[0].cpu().numpy())\n",
    "        observation_elements['trans_action_indicies'] = torch.cat(translation_results, 1)[0].cpu().numpy()\n",
    "        observation_elements['rot_grip_action_indicies'] = rgai\n",
    "        continuous_action = np.concatenate([\n",
    "            act_results.observation_elements['attention_coordinate'].cpu().numpy()[0],\n",
    "            utils.discrete_euler_to_quaternion(rgai[-4:-1], self._rotation_resolution),\n",
    "            rgai[-1:],\n",
    "            [ignore_collisions],\n",
    "        ])\n",
    "        return ActResult(\n",
    "            continuous_action,\n",
    "            observation_elements=observation_elements,\n",
    "            info=infos\n",
    "        )\n",
    "\n",
    "    def update_summaries(self) -> List[Summary]:\n",
    "        summaries = []\n",
    "        for qa in self._qattention_agents:\n",
    "            summaries.extend(qa.update_summaries())\n",
    "        return summaries\n",
    "    \n",
    "    def update_wandb_summaries(self):\n",
    "        summaries = {}\n",
    "        for qa in self._qattention_agents:\n",
    "            summaries.update(qa.update_wandb_summaries())\n",
    "        return summaries\n",
    "\n",
    "    def act_summaries(self) -> List[Summary]:\n",
    "        s = []\n",
    "        for qa in self._qattention_agents:\n",
    "            s.extend(qa.act_summaries())\n",
    "        return s\n",
    "\n",
    "    def load_weights(self, savedir: str):\n",
    "        for qa in self._qattention_agents:\n",
    "            qa.load_weights(savedir)\n",
    "\n",
    "    def save_weights(self, savedir: str):\n",
    "        for qa in self._qattention_agents:\n",
    "            qa.save_weights(savedir)\n",
    "    \n",
    "    def load_clip(self):\n",
    "        for qa in self._qattention_agents:\n",
    "            qa.load_clip()\n",
    "    \n",
    "    def unload_clip(self):\n",
    "        for qa in self._qattention_agents:\n",
    "            qa.unload_clip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreprocessAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List\n",
    "from yarr.agents.agent import Agent, Summary, ActResult, \\\n",
    "    ScalarSummary, HistogramSummary, ImageSummary\n",
    "from termcolor import cprint\n",
    "\n",
    "\n",
    "class PreprocessAgent(Agent):\n",
    "    '''\n",
    "    normalize rgb, logging\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 pose_agent: Agent,\n",
    "                 norm_rgb: bool = True):\n",
    "        self._pose_agent = pose_agent\n",
    "        self._norm_rgb = norm_rgb\n",
    "\n",
    "    def build(self, training: bool, device: torch.device = None, use_ddp: bool = True, **kwargs):\n",
    "        # try:\n",
    "        self._pose_agent.build(training, device, use_ddp, **kwargs)\n",
    "        # except Exception as e:\n",
    "        #     cprint(e, \"red\")\n",
    "        #     self._pose_agent.build(training, device, **kwargs)\n",
    "\n",
    "    def _norm_rgb_(self, x):\n",
    "        return (x.float() / 255.0) * 2.0 - 1.0\n",
    "\n",
    "    def update(self, step: int, replay_sample: dict, **kwargs) -> dict:\n",
    "\n",
    "\n",
    "        nerf_multi_view_rgb = replay_sample['nerf_multi_view_rgb']\n",
    "        nerf_multi_view_depth = replay_sample['nerf_multi_view_depth']\n",
    "        nerf_multi_view_camera = replay_sample['nerf_multi_view_camera']\n",
    "        nerf_multi_view_mask = replay_sample['nerf_multi_view_mask']\n",
    "\n",
    "        if 'nerf_next_multi_view_rgb' in replay_sample:\n",
    "            nerf_next_multi_view_rgb = replay_sample['nerf_next_multi_view_rgb']\n",
    "            nerf_next_multi_view_depth = replay_sample['nerf_next_multi_view_depth']\n",
    "            nerf_next_multi_view_camera = replay_sample['nerf_next_multi_view_camera']\n",
    "            nerf_next_multi_view_mask = replay_sample['nerf_next_multi_view_mask']\n",
    "        lang_goal = replay_sample['lang_goal']\n",
    "\n",
    "        if replay_sample['nerf_multi_view_rgb'] is None or replay_sample['nerf_multi_view_rgb'][0,0] is None:\n",
    "            cprint(\"preprocess agent no nerf rgb 1\", \"red\")\n",
    "\n",
    "        replay_sample = {k: v[:, 0] if len(v.shape) > 2 else v for k, v in replay_sample.items()}\n",
    "\n",
    "        for k, v in replay_sample.items():\n",
    "            if self._norm_rgb and 'rgb' in k and 'nerf' not in k:\n",
    "                replay_sample[k] = self._norm_rgb_(v)\n",
    "            elif 'nerf' in k:\n",
    "                replay_sample[k] = v\n",
    "            else:\n",
    "                try:\n",
    "                    replay_sample[k] = v.float()\n",
    "                except:\n",
    "                    replay_sample[k] = v\n",
    "                    pass # some elements are not tensors/arrays\n",
    "        replay_sample['nerf_multi_view_rgb'] = nerf_multi_view_rgb\n",
    "        replay_sample['nerf_multi_view_depth'] = nerf_multi_view_depth\n",
    "        replay_sample['nerf_multi_view_camera'] = nerf_multi_view_camera\n",
    "        replay_sample['nerf_multi_view_mask'] = nerf_multi_view_mask\n",
    "\n",
    "        if 'nerf_next_multi_view_rgb' in replay_sample:\n",
    "            replay_sample['nerf_next_multi_view_rgb'] = nerf_next_multi_view_rgb\n",
    "            replay_sample['nerf_next_multi_view_depth'] = nerf_next_multi_view_depth\n",
    "            replay_sample['nerf_next_multi_view_camera'] = nerf_next_multi_view_camera\n",
    "            replay_sample['nerf_next_multi_view_mask'] = nerf_next_multi_view_mask\n",
    "        \n",
    "        replay_sample['lang_goal'] = lang_goal\n",
    "        self._replay_sample = replay_sample\n",
    "\n",
    "\n",
    "        if replay_sample['nerf_multi_view_rgb'] is None or replay_sample['nerf_multi_view_rgb'][0,0] is None:\n",
    "            cprint(\"preprocess agent no nerf rgb 2\", \"red\")\n",
    "\n",
    "        return self._pose_agent.update(step, replay_sample, **kwargs)\n",
    "\n",
    "    def act(self, step: int, observation: dict,\n",
    "            deterministic=False) -> ActResult:\n",
    "\n",
    "        for k, v in observation.items():\n",
    "            if self._norm_rgb and 'rgb' in k:\n",
    "                observation[k] = self._norm_rgb_(v)\n",
    "            else:\n",
    "                try:\n",
    "                    observation[k] = v.float()\n",
    "                except:\n",
    "                    observation[k] = v\n",
    "                    pass # some elements are not tensors/arrays\n",
    "        act_res = self._pose_agent.act(step, observation, deterministic)\n",
    "        act_res.replay_elements.update({'demo': False})\n",
    "        return act_res\n",
    "\n",
    "    def update_summaries(self) -> List[Summary]:\n",
    "        prefix = 'inputs'\n",
    "        demo_f = self._replay_sample['demo'].float()\n",
    "        demo_proportion = demo_f.mean()\n",
    "        tile = lambda x: torch.squeeze(\n",
    "            torch.cat(x.split(1, dim=1), dim=-1), dim=1)\n",
    "        sums = [\n",
    "            ScalarSummary('%s/demo_proportion' % prefix, demo_proportion),\n",
    "            HistogramSummary('%s/low_dim_state' % prefix,\n",
    "                    self._replay_sample['low_dim_state']),\n",
    "            HistogramSummary('%s/low_dim_state_tp1' % prefix,\n",
    "                    self._replay_sample['low_dim_state_tp1']),\n",
    "            ScalarSummary('%s/low_dim_state_mean' % prefix,\n",
    "                    self._replay_sample['low_dim_state'].mean()),\n",
    "            ScalarSummary('%s/low_dim_state_min' % prefix,\n",
    "                    self._replay_sample['low_dim_state'].min()),\n",
    "            ScalarSummary('%s/low_dim_state_max' % prefix,\n",
    "                    self._replay_sample['low_dim_state'].max()),\n",
    "            ScalarSummary('%s/timeouts' % prefix,\n",
    "                    self._replay_sample['timeout'].float().mean()),\n",
    "        ]\n",
    "\n",
    "        if 'sampling_probabilities' in self._replay_sample:\n",
    "            sums.extend([\n",
    "                HistogramSummary('replay/priority',\n",
    "                                 self._replay_sample['sampling_probabilities']),\n",
    "            ])\n",
    "        sums.extend(self._pose_agent.update_summaries())\n",
    "        return sums\n",
    "\n",
    "    def update_wandb_summaries(self):\n",
    "        return self._pose_agent.update_wandb_summaries()\n",
    "        \n",
    "    def act_summaries(self) -> List[Summary]:\n",
    "        return self._pose_agent.act_summaries()\n",
    "\n",
    "    def load_weights(self, savedir: str):\n",
    "        self._pose_agent.load_weights(savedir)\n",
    "\n",
    "    def save_weights(self, savedir: str):\n",
    "        self._pose_agent.save_weights(savedir)\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._pose_agent.reset()\n",
    "\n",
    "    def load_clip(self):\n",
    "        self._pose_agent.load_clip()\n",
    "\n",
    "    def unload_clip(self):\n",
    "        self._pose_agent.unload_clip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceiver lang io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceiver IO implementation adpated for manipulation\n",
    "# Source: https://github.com/lucidrains/perceiver-pytorch\n",
    "# License: https://github.com/lucidrains/perceiver-pytorch/blob/main/LICENSE\n",
    "\n",
    "from math import pi, log\n",
    "from functools import wraps\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Reduce\n",
    "from helpers.network_utils import DenseBlock, SpatialSoftmax3D, Conv3DBlock, \\\n",
    "            Conv3DUpsampleBlock, MultiLayer3DEncoderShallow\n",
    "from termcolor import colored, cprint\n",
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "\n",
    "def cache_fn(f):\n",
    "    cache = None\n",
    "\n",
    "    @wraps(f)\n",
    "    def cached_fn(*args, _cache=True, **kwargs):\n",
    "        if not _cache:\n",
    "            return f(*args, **kwargs)\n",
    "        nonlocal cache\n",
    "        if cache is not None:\n",
    "            return cache\n",
    "        cache = f(*args, **kwargs)\n",
    "        return cache\n",
    "\n",
    "    return cached_fn\n",
    "\n",
    "def fourier_encode(x, max_freq, num_bands = 4):\n",
    "    x = x.unsqueeze(-1)\n",
    "    device, dtype, orig_x = x.device, x.dtype, x\n",
    "\n",
    "    scales = torch.linspace(1., max_freq / 2, num_bands, device = device, dtype = dtype)\n",
    "    scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]\n",
    "\n",
    "    x = x * scales * pi\n",
    "    x = torch.cat([x.sin(), x.cos()], dim = -1)\n",
    "    x = torch.cat((x, orig_x), dim = -1)\n",
    "    return x\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context=normed_context)\n",
    "\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "    def get_attention_matrix(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context=normed_context)\n",
    "        kwargs['return_attention_weights'] = True\n",
    "        return self.fn(x, **kwargs) \n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None, return_attention_weights=False):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        \n",
    "        if return_attention_weights:\n",
    "            return attn\n",
    "\n",
    "        # dropout\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "\n",
    "# PerceiverIO adapted for 6-DoF manipulation\n",
    "class PerceiverVoxelLangEncoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            depth,                    # number of self-attention layers\n",
    "            iterations,               # number cross-attention iterations (PerceiverIO uses just 1)\n",
    "            voxel_size,               # N voxels per side (size: N*N*N)\n",
    "            initial_dim,              # 10 dimensions - dimension of the input sequence to be encoded\n",
    "            low_dim_size,             # 4 dimensions - proprioception: {gripper_open, left_finger, right_finger, timestep}\n",
    "            layer=0,\n",
    "            num_rotation_classes=72,  # 5 degree increments (5*72=360) for each of the 3-axis\n",
    "            num_grip_classes=2,       # open or not open\n",
    "            num_collision_classes=2,  # collisions allowed or not allowed\n",
    "            input_axis=3,             # 3D tensors have 3 axes\n",
    "            num_latents=2048,          # number of latent vectors\n",
    "            im_channels=64,           # intermediate channel size\n",
    "            latent_dim=512,           # dimensions of latent vectors\n",
    "            cross_heads=1,            # number of cross-attention heads\n",
    "            latent_heads=8,           # number of latent heads\n",
    "            cross_dim_head=64,\n",
    "            latent_dim_head=64,\n",
    "            activation='relu',\n",
    "            weight_tie_layers=False,\n",
    "            pos_encoding_with_lang=True,\n",
    "            input_dropout=0.1,\n",
    "            attn_dropout=0.1,\n",
    "            decoder_dropout=0.0,\n",
    "            lang_fusion_type='seq',\n",
    "            voxel_patch_size=9,\n",
    "            voxel_patch_stride=8,\n",
    "            no_skip_connection=False,\n",
    "            no_perceiver=False,\n",
    "            no_language=False,\n",
    "            final_dim=64,\n",
    "            cfg=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.layer = layer\n",
    "        self.init_dim = int(initial_dim)\n",
    "        self.iterations = iterations\n",
    "        self.input_axis = input_axis\n",
    "        self.voxel_size = voxel_size\n",
    "        self.low_dim_size = low_dim_size\n",
    "        self.im_channels = im_channels\n",
    "        self.pos_encoding_with_lang = pos_encoding_with_lang\n",
    "        self.lang_fusion_type = lang_fusion_type\n",
    "        self.voxel_patch_size = voxel_patch_size\n",
    "        self.voxel_patch_stride = voxel_patch_stride\n",
    "        self.num_rotation_classes = num_rotation_classes\n",
    "        self.num_grip_classes = num_grip_classes\n",
    "        self.num_collision_classes = num_collision_classes\n",
    "        self.final_dim = final_dim\n",
    "        self.input_dropout = input_dropout\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.decoder_dropout = decoder_dropout\n",
    "        self.no_skip_connection = no_skip_connection\n",
    "        self.no_perceiver = no_perceiver\n",
    "        self.no_language = no_language\n",
    "\n",
    "        # patchified input dimensions\n",
    "        spatial_size = voxel_size // self.voxel_patch_stride  # 100/5 = 20\n",
    "\n",
    "        # 64 voxel features + 64 proprio features (+ 64 lang goal features if concattenated)\n",
    "        self.input_dim_before_seq = self.im_channels * 3 if self.lang_fusion_type == 'concat' else self.im_channels * 2\n",
    "\n",
    "        # CLIP language feature dimensions\n",
    "        lang_feat_dim, lang_emb_dim, lang_max_seq_len = 1024, cfg.method.language_model_dim, 77\n",
    "\n",
    "        # learnable positional encoding\n",
    "        if self.pos_encoding_with_lang:\n",
    "            self.pos_encoding = nn.Parameter(torch.randn(1,\n",
    "                                                         lang_max_seq_len + spatial_size ** 3,\n",
    "                                                         self.input_dim_before_seq))\n",
    "        else:\n",
    "            # assert self.lang_fusion_type == 'concat', 'Only concat is supported for pos encoding without lang.'\n",
    "            self.pos_encoding = nn.Parameter(torch.randn(1,\n",
    "                                                         spatial_size, spatial_size, spatial_size,\n",
    "                                                         self.input_dim_before_seq))\n",
    "\n",
    "        # voxel input preprocessing \n",
    "        self.encoder_3d = MultiLayer3DEncoderShallow(in_channels=self.init_dim, out_channels=self.im_channels)\n",
    "\n",
    "        # patchify conv\n",
    "        self.patchify = Conv3DBlock(\n",
    "            self.encoder_3d.out_channels, self.im_channels,\n",
    "            kernel_sizes=self.voxel_patch_size, strides=self.voxel_patch_stride,\n",
    "            norm=None, activation=activation)\n",
    "\n",
    "        # language preprocess\n",
    "        if self.lang_fusion_type == 'concat':\n",
    "            self.lang_preprocess = nn.Linear(lang_feat_dim, self.im_channels)\n",
    "        elif self.lang_fusion_type == 'seq':\n",
    "            self.lang_preprocess = nn.Linear(lang_emb_dim, self.im_channels * 2)\n",
    "\n",
    "        # proprioception\n",
    "        if self.low_dim_size > 0:\n",
    "            self.proprio_preprocess = DenseBlock(\n",
    "                self.low_dim_size, self.im_channels, norm=None, activation=activation,\n",
    "            )\n",
    "\n",
    "        # pooling functions\n",
    "        # self.local_maxp = nn.MaxPool3d(3, 2, padding=1)\n",
    "        self.global_maxp = nn.AdaptiveMaxPool3d(1)\n",
    "\n",
    "        # 1st 3D softmax\n",
    "        self.ss0 = SpatialSoftmax3D(\n",
    "            self.voxel_size, self.voxel_size, self.voxel_size, self.im_channels)\n",
    "        flat_size = self.im_channels * 4\n",
    "\n",
    "        # latent vectors (that are randomly initialized)\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, latent_dim))\n",
    "\n",
    "        # encoder cross attention\n",
    "        self.cross_attend_blocks = nn.ModuleList([\n",
    "            PreNorm(latent_dim, Attention(latent_dim,\n",
    "                                          self.input_dim_before_seq,\n",
    "                                          heads=cross_heads,\n",
    "                                          dim_head=cross_dim_head,\n",
    "                                          dropout=input_dropout),\n",
    "                    context_dim=self.input_dim_before_seq),\n",
    "            PreNorm(latent_dim, FeedForward(latent_dim))\n",
    "        ])\n",
    "\n",
    "        get_latent_attn = lambda: PreNorm(latent_dim,\n",
    "                                          Attention(latent_dim, heads=latent_heads,\n",
    "                                                    dim_head=latent_dim_head, dropout=attn_dropout))\n",
    "        get_latent_ff = lambda: PreNorm(latent_dim, FeedForward(latent_dim))\n",
    "        get_latent_attn, get_latent_ff = map(cache_fn, (get_latent_attn, get_latent_ff))\n",
    "\n",
    "        # self attention layers\n",
    "        self.layers = nn.ModuleList([])\n",
    "        cache_args = {'_cache': weight_tie_layers}\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                get_latent_attn(**cache_args),\n",
    "                get_latent_ff(**cache_args)\n",
    "            ]))\n",
    "\n",
    "        # decoder cross attention\n",
    "        self.decoder_cross_attn = PreNorm(self.input_dim_before_seq, Attention(self.input_dim_before_seq,\n",
    "                                                                               latent_dim,\n",
    "                                                                               heads=cross_heads,\n",
    "                                                                               dim_head=cross_dim_head,\n",
    "                                                                               dropout=decoder_dropout),\n",
    "                                          context_dim=latent_dim)\n",
    "\n",
    "        # upsample conv\n",
    "        self.up0 = Conv3DUpsampleBlock(\n",
    "            self.input_dim_before_seq, self.final_dim,\n",
    "            kernel_sizes=self.voxel_patch_size, strides=self.voxel_patch_stride,\n",
    "            norm=None, activation=activation,\n",
    "        )\n",
    "        # voxel_patch_size: 5, voxel_patch_stride: 5\n",
    "\n",
    "\n",
    "        # 2nd 3D softmax\n",
    "        self.ss1 = SpatialSoftmax3D(\n",
    "            spatial_size, spatial_size, spatial_size,\n",
    "            self.input_dim_before_seq)\n",
    "\n",
    "        flat_size += self.input_dim_before_seq * 4\n",
    "\n",
    "        # final 3D softmax\n",
    "        self.final = Conv3DBlock(\n",
    "            self.im_channels if (self.no_perceiver or self.no_skip_connection) else self.im_channels * 2,\n",
    "            self.im_channels,\n",
    "            kernel_sizes=3,\n",
    "            strides=1, norm=None, activation=activation)\n",
    "\n",
    "        self.trans_decoder = Conv3DBlock(\n",
    "            self.final_dim, 1, kernel_sizes=3, strides=1,\n",
    "            norm=None, activation=None,\n",
    "        )\n",
    "\n",
    "        # rotation, gripper, and collision MLP layers\n",
    "        if self.num_rotation_classes > 0:\n",
    "            self.ss_final = SpatialSoftmax3D(\n",
    "                self.voxel_size, self.voxel_size, self.voxel_size,\n",
    "                self.im_channels)\n",
    "\n",
    "            flat_size += self.im_channels * 4\n",
    "            self.dense0 =  DenseBlock(flat_size, 256, None, activation)\n",
    "            self.dense1 = DenseBlock(256, self.final_dim, None, activation)\n",
    "\n",
    "            self.rot_grip_collision_ff = DenseBlock(self.final_dim,\n",
    "                                                    self.num_rotation_classes * 3 + \\\n",
    "                                                    self.num_grip_classes + \\\n",
    "                                                    self.num_collision_classes,\n",
    "                                                    None, None)\n",
    "\n",
    "\n",
    "    def encode_text(self, x):\n",
    "        with torch.no_grad():\n",
    "            text_feat, text_emb = self._clip_rn50.encode_text_with_embeddings(x)\n",
    "\n",
    "        text_feat = text_feat.detach()\n",
    "        text_emb = text_emb.detach()\n",
    "        text_mask = torch.where(x==0, x, 1)  # [1, max_token_len]\n",
    "        return text_feat, text_emb\n",
    "\n",
    "    def save_tensor(self, x, save_path):\n",
    "        import pickle\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(x, f)\n",
    "        print(f'save tensor with shape {x.shape} to {save_path}')\n",
    "    \n",
    "    def counter(self):\n",
    "        if not hasattr(self, '_counter'):\n",
    "            self._counter = 0\n",
    "        else:\n",
    "            self._counter += 1\n",
    "        return self._counter\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            ins, # voxel\n",
    "            proprio,\n",
    "            lang_goal_emb,\n",
    "            lang_token_embs,\n",
    "            prev_layer_voxel_grid,\n",
    "            bounds,\n",
    "            prev_layer_bounds,\n",
    "            mask=None,\n",
    "    ):  \n",
    "\n",
    "        # preprocess input\n",
    "        d0, multi_scale_voxel_list = self.encoder_3d(ins) # [B,10,100,100,100] -> [B,128,100,100,100]\n",
    "        # d0: [1, 128, 100, 100, 100]\n",
    "        # multi_scale_voxel_list: [torch.Size([1, 10, 100, 100, 100]), torch.Size([1, 32, 25, 25, 25]), torch.Size([1, 16, 50, 50, 50])]\n",
    "        \n",
    "        # aggregated features from 1st softmax and maxpool for MLP decoders\n",
    "        feats = [self.ss0(d0.contiguous()), self.global_maxp(d0).view(ins.shape[0], -1)]\n",
    "        # feats: [torch.Size([1, 384]), torch.Size([1, 128])]\n",
    "\n",
    "        # patchify input (5x5x5 patches)\n",
    "        ins = self.patchify(d0)                               # [B,128,100,100,100] -> [B,128,20,20,20]\n",
    "        # voxel_grid = ins.clone() # not working\n",
    "\n",
    "        b, c, d, h, w, device = *ins.shape, ins.device\n",
    "        axis = [d, h, w]\n",
    "        assert len(axis) == self.input_axis, 'input must have the same number of axis as input_axis'\n",
    "\n",
    "        # concat proprio\n",
    "        if self.low_dim_size > 0:\n",
    "            p = self.proprio_preprocess(proprio)              # [B,4] -> [B,64]\n",
    "            p = p.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, d, h, w)\n",
    "            ins = torch.cat([ins, p], dim=1)                  # [B,128,20,20,20]\n",
    "\n",
    "        # language ablation\n",
    "        if self.no_language:\n",
    "            lang_goal_emb = torch.zeros_like(lang_goal_emb)\n",
    "            lang_token_embs = torch.zeros_like(lang_token_embs)\n",
    "        # option 1: tile and concat lang goal to input\n",
    "        if self.lang_fusion_type == 'concat':\n",
    "            lang_emb = lang_goal_emb\n",
    "            lang_emb = lang_emb.to(dtype=ins.dtype)\n",
    "            l = self.lang_preprocess(lang_emb)\n",
    "            l = l.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).repeat(1, 1, d, h, w)\n",
    "            ins = torch.cat([ins, l], dim=1)\n",
    "\n",
    "        # channel last\n",
    "        ins = rearrange(ins, 'b d ... -> b ... d')            # [B,20,20,20,128]\n",
    "\n",
    "        # add pos encoding to grid\n",
    "        if not self.pos_encoding_with_lang:\n",
    "            ins = ins + self.pos_encoding\n",
    "\n",
    "        # voxel_grid = ins.clone()\n",
    "\n",
    "        ######################## NOTE #############################\n",
    "        # NOTE: If you add positional encodings ^here the lang embs\n",
    "        # won't have positional encodings. I accidently forgot\n",
    "        # to turn this off for all the experiments in the paper.\n",
    "        # So I guess those models were using language embs\n",
    "        # as a bag of words :( But it doesn't matter much for\n",
    "        # RLBench tasks since we don't test for novel instructions\n",
    "        # at test time anyway. The recommend way is to add\n",
    "        # positional encodings to the final input sequence\n",
    "        # fed into the Perceiver Transformer, as done below\n",
    "        # (and also in the Colab tutorial).\n",
    "        ###########################################################\n",
    "\n",
    "        # concat to channels of and flatten axis\n",
    "        queries_orig_shape = ins.shape\n",
    "\n",
    "        # rearrange input to be channel last\n",
    "        ins = rearrange(ins, 'b ... d -> b (...) d')          # [B,8000,128]\n",
    "        ins_wo_prev_layers = ins\n",
    "\n",
    "        # option 2: add lang token embs as a sequence\n",
    "        if self.lang_fusion_type == 'seq':\n",
    "            l = self.lang_preprocess(lang_token_embs.float())         # [B,77,512] -> [B,77,256]\n",
    "            ins = torch.cat((l, ins), dim=1)                  # [B,8077,256]\n",
    "\n",
    "        # add pos encoding to language + flattened grid (the recommended way)\n",
    "        if self.pos_encoding_with_lang:\n",
    "            ins = ins + self.pos_encoding\n",
    "\n",
    "        # batchify latents\n",
    "        x = repeat(self.latents, 'n d -> b n d', b=b)\n",
    "\n",
    "        cross_attn, cross_ff = self.cross_attend_blocks\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            # encoder cross attention\n",
    "            x = cross_attn(x, context=ins, mask=mask) + x # x: 1,2048,512, ins: 1,8077,128\n",
    "            \n",
    "            x = cross_ff(x) + x\n",
    "\n",
    "            # self-attention layers\n",
    "            layer_counter = 1\n",
    "            for self_attn, self_ff in self.layers:\n",
    "                x = self_attn(x) + x\n",
    "                layer_counter += 1\n",
    "                \n",
    "                x = self_ff(x) + x\n",
    "                \n",
    "\n",
    "        # decoder cross attention\n",
    "        # ins: [B,8077,128], x: [B,2048,512]\n",
    "        latents = self.decoder_cross_attn(ins, context=x)\n",
    "        # latents: [B,8077,128]\n",
    "\n",
    "        # crop out the language part of the output sequence\n",
    "        if self.lang_fusion_type == 'seq':\n",
    "            latents = latents[:, l.shape[1]:]\n",
    "\n",
    "        # reshape back to voxel grid\n",
    "        latents = latents.view(b, *queries_orig_shape[1:-1], latents.shape[-1]) # [B,20,20,20,128]\n",
    "        latents = rearrange(latents, 'b ... d -> b d ...')                      # [B,128,20,20,20]\n",
    "\n",
    "        # aggregated features from 2nd softmax and maxpool for MLP decoders\n",
    "        feats.extend([self.ss1(latents.contiguous()), self.global_maxp(latents).view(b, -1)])\n",
    "\n",
    "        # upsample\n",
    "        latents = self.up0(latents) # [B,256,20,20,20] -> [B,128,100,100,100]\n",
    "        \n",
    "        # ablations\n",
    "        if self.no_skip_connection:\n",
    "            latents = self.final(latents)\n",
    "        elif self.no_perceiver:\n",
    "            latents = self.final(d0)\n",
    "        else:\n",
    "            latents = self.final(torch.cat([d0, latents], dim=1)) # [1, 128, 100, 100, 100]\n",
    "\n",
    "        # translation decoder\n",
    "        trans = self.trans_decoder(latents) # [1, 1, 100, 100, 100]\n",
    "\n",
    "        # rotation, gripper, and collision MLPs\n",
    "        rot_and_grip_out = None\n",
    "        if self.num_rotation_classes > 0:\n",
    "            feats.extend([self.ss_final(latents.contiguous()), self.global_maxp(latents).view(b, -1)])\n",
    "            \n",
    "            feats = self.dense0(torch.cat(feats, dim=1))   # [1,2048]->[1,256]\n",
    "            feats = self.dense1(feats)                     # [B,72*3+2+2]\n",
    "\n",
    "            rot_and_grip_collision_out = self.rot_grip_collision_ff(feats) # [1,220]\n",
    "            rot_and_grip_out = rot_and_grip_collision_out[:, :-self.num_collision_classes]  # [1,218]\n",
    "            collision_out = rot_and_grip_collision_out[:, -self.num_collision_classes:] # [1,2]\n",
    "\n",
    "        return trans, rot_and_grip_out, collision_out, d0, multi_scale_voxel_list, l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceiver IO implementation adpated for manipulation\n",
    "# Source: https://github.com/lucidrains/perceiver-pytorch\n",
    "# License: https://github.com/lucidrains/perceiver-pytorch/blob/main/LICENSE\n",
    "\n",
    "from math import pi, log\n",
    "from functools import wraps\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat, reduce\n",
    "from einops.layers.torch import Reduce\n",
    "from helpers.network_utils import DenseBlock, SpatialSoftmax3D, Conv3DBlock, \\\n",
    "            Conv3DUpsampleBlock, MultiLayer3DEncoderShallow\n",
    "from termcolor import colored, cprint\n",
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "\n",
    "def cache_fn(f):\n",
    "    cache = None\n",
    "\n",
    "    @wraps(f)\n",
    "    def cached_fn(*args, _cache=True, **kwargs):\n",
    "        if not _cache:\n",
    "            return f(*args, **kwargs)\n",
    "        nonlocal cache\n",
    "        if cache is not None:\n",
    "            return cache\n",
    "        cache = f(*args, **kwargs)\n",
    "        return cache\n",
    "\n",
    "    return cached_fn\n",
    "\n",
    "def fourier_encode(x, max_freq, num_bands = 4):\n",
    "    x = x.unsqueeze(-1)\n",
    "    device, dtype, orig_x = x.device, x.dtype, x\n",
    "\n",
    "    scales = torch.linspace(1., max_freq / 2, num_bands, device = device, dtype = dtype)\n",
    "    scales = scales[(*((None,) * (len(x.shape) - 1)), Ellipsis)]\n",
    "\n",
    "    x = x * scales * pi\n",
    "    x = torch.cat([x.sin(), x.cos()], dim = -1)\n",
    "    x = torch.cat((x, orig_x), dim = -1)\n",
    "    return x\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, context_dim=None):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.norm_context = nn.LayerNorm(context_dim) if exists(context_dim) else None\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context=normed_context)\n",
    "\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "    def get_attention_matrix(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(self.norm_context):\n",
    "            context = kwargs['context']\n",
    "            normed_context = self.norm_context(context)\n",
    "            kwargs.update(context=normed_context)\n",
    "        kwargs['return_attention_weights'] = True\n",
    "        return self.fn(x, **kwargs) \n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim=-1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult=4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(context_dim, query_dim)\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias=False)\n",
    "        self.to_out = nn.Linear(inner_dim, query_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None, return_attention_weights=False):\n",
    "        h = self.heads\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim=-1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        \n",
    "        if return_attention_weights:\n",
    "            return attn\n",
    "\n",
    "        # dropout\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
    "        \n",
    "        return self.to_out(out)\n",
    "\n",
    "\n",
    "\n",
    "# PerceiverIO adapted for 6-DoF manipulation\n",
    "class VoxelFeature(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            depth,                    # number of self-attention layers\n",
    "            iterations,               # number cross-attention iterations (PerceiverIO uses just 1)\n",
    "            voxel_size,               # N voxels per side (size: N*N*N)\n",
    "            initial_dim,              # 10 dimensions - dimension of the input sequence to be encoded\n",
    "            low_dim_size,             # 4 dimensions - proprioception: {gripper_open, left_finger, right_finger, timestep}\n",
    "            layer=0,\n",
    "            num_rotation_classes=72,  # 5 degree increments (5*72=360) for each of the 3-axis\n",
    "            num_grip_classes=2,       # open or not open\n",
    "            num_collision_classes=2,  # collisions allowed or not allowed\n",
    "            input_axis=3,             # 3D tensors have 3 axes\n",
    "            num_latents=2048,          # number of latent vectors\n",
    "            im_channels=64,           # intermediate channel size\n",
    "            latent_dim=512,           # dimensions of latent vectors\n",
    "            cross_heads=1,            # number of cross-attention heads\n",
    "            latent_heads=8,           # number of latent heads\n",
    "            cross_dim_head=64,\n",
    "            latent_dim_head=64,\n",
    "            activation='relu',\n",
    "            weight_tie_layers=False,\n",
    "            pos_encoding_with_lang=True,\n",
    "            input_dropout=0.1,\n",
    "            attn_dropout=0.1,\n",
    "            decoder_dropout=0.0,\n",
    "            lang_fusion_type='seq',\n",
    "            voxel_patch_size=9,\n",
    "            voxel_patch_stride=8,\n",
    "            no_skip_connection=False,\n",
    "            no_perceiver=False,\n",
    "            no_language=False,\n",
    "            final_dim=64,\n",
    "            cfg=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.layer = layer\n",
    "        self.init_dim = int(initial_dim)\n",
    "        self.iterations = iterations\n",
    "        self.input_axis = input_axis\n",
    "        self.voxel_size = voxel_size\n",
    "        self.low_dim_size = low_dim_size\n",
    "        self.im_channels = im_channels\n",
    "        self.pos_encoding_with_lang = pos_encoding_with_lang\n",
    "        self.lang_fusion_type = lang_fusion_type\n",
    "        self.voxel_patch_size = voxel_patch_size\n",
    "        self.voxel_patch_stride = voxel_patch_stride\n",
    "        self.num_rotation_classes = num_rotation_classes\n",
    "        self.num_grip_classes = num_grip_classes\n",
    "        self.num_collision_classes = num_collision_classes\n",
    "        self.final_dim = final_dim\n",
    "        self.input_dropout = input_dropout\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.decoder_dropout = decoder_dropout\n",
    "        self.no_skip_connection = no_skip_connection\n",
    "        self.no_perceiver = no_perceiver\n",
    "        self.no_language = no_language\n",
    "\n",
    "        # patchified input dimensions\n",
    "        spatial_size = voxel_size // self.voxel_patch_stride  # 100/5 = 20\n",
    "\n",
    "        # 64 voxel features + 64 proprio features (+ 64 lang goal features if concattenated)\n",
    "        self.input_dim_before_seq = self.im_channels * 3 if self.lang_fusion_type == 'concat' else self.im_channels * 2\n",
    "\n",
    "        # CLIP language feature dimensions\n",
    "        lang_feat_dim, lang_emb_dim, lang_max_seq_len = 1024, cfg.method.language_model_dim, 77\n",
    "\n",
    "\n",
    "        # voxel input preprocessing \n",
    "        self.encoder_3d = MultiLayer3DEncoderShallow(in_channels=self.init_dim, out_channels=self.im_channels)\n",
    "        self.lang_preprocess = nn.Linear(lang_emb_dim, self.im_channels * 2)\n",
    "        \n",
    "\n",
    "\n",
    "    def encode_text(self, x):\n",
    "        with torch.no_grad():\n",
    "            text_feat, text_emb = self._clip_rn50.encode_text_with_embeddings(x)\n",
    "\n",
    "        text_feat = text_feat.detach()\n",
    "        text_emb = text_emb.detach()\n",
    "        text_mask = torch.where(x==0, x, 1)  # [1, max_token_len]\n",
    "        return text_feat, text_emb\n",
    "\n",
    "    def save_tensor(self, x, save_path):\n",
    "        import pickle\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(x, f)\n",
    "        print(f'save tensor with shape {x.shape} to {save_path}')\n",
    "    \n",
    "    def counter(self):\n",
    "        if not hasattr(self, '_counter'):\n",
    "            self._counter = 0\n",
    "        else:\n",
    "            self._counter += 1\n",
    "        return self._counter\n",
    "    \n",
    "    def forward(\n",
    "            self,\n",
    "            ins, # voxel\n",
    "            proprio,\n",
    "            lang_goal_emb,\n",
    "            lang_token_embs,\n",
    "            prev_layer_voxel_grid,\n",
    "            bounds,\n",
    "            prev_layer_bounds,\n",
    "            mask=None,\n",
    "    ):  \n",
    "\n",
    "        # preprocess input\n",
    "        d0, multi_scale_voxel_list = self.encoder_3d(ins) # [B,10,100,100,100] -> [B,128,100,100,100]\n",
    "        l = self.lang_preprocess(lang_token_embs.float())         # [B,77,512] -> [B,77,256]\n",
    "\n",
    "        return None, None, None, d0, multi_scale_voxel_list, l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_agent(cfg: DictConfig):\n",
    "    LATENT_SIZE = 64\n",
    "    depth_0bounds = cfg.rlbench.scene_bounds\n",
    "    cam_resolution = cfg.rlbench.camera_resolution\n",
    "\n",
    "    num_rotation_classes = int(360. // cfg.method.rotation_resolution)\n",
    "    qattention_agents = []\n",
    "    for depth, vox_size in enumerate(cfg.method.voxel_sizes):   # default: [100]\n",
    "        last = depth == len(cfg.method.voxel_sizes) - 1\n",
    "        # perceiver_encoder = PerceiverVoxelLangEncoder(\n",
    "        #     depth=cfg.method.transformer_depth, # 6\n",
    "        #     iterations=cfg.method.transformer_iterations,\n",
    "        #     voxel_size=vox_size,\n",
    "        #     initial_dim=3 + 3 + 1 + 3,\n",
    "        #     low_dim_size=4,\n",
    "        #     layer=depth,\n",
    "        #     num_rotation_classes=num_rotation_classes if last else 0,\n",
    "        #     num_grip_classes=2 if last else 0,\n",
    "        #     num_collision_classes=2 if last else 0,\n",
    "        #     input_axis=3,\n",
    "        #     num_latents = cfg.method.num_latents,\n",
    "        #     latent_dim = cfg.method.latent_dim,\n",
    "        #     cross_heads = cfg.method.cross_heads,\n",
    "        #     latent_heads = cfg.method.latent_heads,\n",
    "        #     cross_dim_head = cfg.method.cross_dim_head,\n",
    "        #     latent_dim_head = cfg.method.latent_dim_head,\n",
    "        #     weight_tie_layers = False,\n",
    "        #     activation = cfg.method.activation,\n",
    "        #     pos_encoding_with_lang=cfg.method.pos_encoding_with_lang,\n",
    "        #     input_dropout=cfg.method.input_dropout,\n",
    "        #     attn_dropout=cfg.method.attn_dropout,\n",
    "        #     decoder_dropout=cfg.method.decoder_dropout,\n",
    "        #     lang_fusion_type=cfg.method.lang_fusion_type,\n",
    "        #     voxel_patch_size=cfg.method.voxel_patch_size,\n",
    "        #     voxel_patch_stride=cfg.method.voxel_patch_stride,\n",
    "        #     no_skip_connection=cfg.method.no_skip_connection,\n",
    "        #     no_perceiver=cfg.method.no_perceiver,\n",
    "        #     no_language=cfg.method.no_language,\n",
    "        #     final_dim=cfg.method.final_dim,\n",
    "        #     im_channels=cfg.method.final_dim,\n",
    "        #     cfg=cfg,\n",
    "        # )\n",
    "        perceiver_encoder = VoxelFeature(\n",
    "            depth=cfg.method.transformer_depth, # 6\n",
    "            iterations=cfg.method.transformer_iterations,\n",
    "            voxel_size=vox_size,\n",
    "            initial_dim=3 + 3 + 1 + 3,\n",
    "            low_dim_size=4,\n",
    "            layer=depth,\n",
    "            num_rotation_classes=num_rotation_classes if last else 0,\n",
    "            num_grip_classes=2 if last else 0,\n",
    "            num_collision_classes=2 if last else 0,\n",
    "            input_axis=3,\n",
    "            num_latents = cfg.method.num_latents,\n",
    "            latent_dim = cfg.method.latent_dim,\n",
    "            cross_heads = cfg.method.cross_heads,\n",
    "            latent_heads = cfg.method.latent_heads,\n",
    "            cross_dim_head = cfg.method.cross_dim_head,\n",
    "            latent_dim_head = cfg.method.latent_dim_head,\n",
    "            weight_tie_layers = False,\n",
    "            activation = cfg.method.activation,\n",
    "            pos_encoding_with_lang=cfg.method.pos_encoding_with_lang,\n",
    "            input_dropout=cfg.method.input_dropout,\n",
    "            attn_dropout=cfg.method.attn_dropout,\n",
    "            decoder_dropout=cfg.method.decoder_dropout,\n",
    "            lang_fusion_type=cfg.method.lang_fusion_type,\n",
    "            voxel_patch_size=cfg.method.voxel_patch_size,\n",
    "            voxel_patch_stride=cfg.method.voxel_patch_stride,\n",
    "            no_skip_connection=cfg.method.no_skip_connection,\n",
    "            no_perceiver=cfg.method.no_perceiver,\n",
    "            no_language=cfg.method.no_language,\n",
    "            final_dim=cfg.method.final_dim,\n",
    "            im_channels=cfg.method.final_dim,\n",
    "            cfg=cfg,\n",
    "        )\n",
    "        qattention_agent = QAttentionPerActBCAgent(\n",
    "            layer=depth,\n",
    "            coordinate_bounds=depth_0bounds,\n",
    "            perceiver_encoder=perceiver_encoder,\n",
    "            camera_names=cfg.rlbench.cameras,\n",
    "            voxel_size=vox_size,\n",
    "            bounds_offset=cfg.method.bounds_offset[depth - 1] if depth > 0 else None,\n",
    "            image_crop_size=cfg.method.image_crop_size,\n",
    "            lr=cfg.method.lr,\n",
    "            training_iterations=cfg.framework.training_iterations,\n",
    "            lr_scheduler=cfg.method.lr_scheduler,\n",
    "            num_warmup_steps=cfg.method.num_warmup_steps,\n",
    "            trans_loss_weight=cfg.method.trans_loss_weight,\n",
    "            rot_loss_weight=cfg.method.rot_loss_weight,\n",
    "            grip_loss_weight=cfg.method.grip_loss_weight,\n",
    "            collision_loss_weight=cfg.method.collision_loss_weight,\n",
    "            include_low_dim_state=True,\n",
    "            image_resolution=cam_resolution,\n",
    "            batch_size=cfg.replay.batch_size,\n",
    "            voxel_feature_size=3,\n",
    "            lambda_weight_l2=cfg.method.lambda_weight_l2,\n",
    "            num_rotation_classes=num_rotation_classes,\n",
    "            rotation_resolution=cfg.method.rotation_resolution,\n",
    "            transform_augmentation=cfg.method.transform_augmentation.apply_se3,\n",
    "            transform_augmentation_xyz=cfg.method.transform_augmentation.aug_xyz,\n",
    "            transform_augmentation_rpy=cfg.method.transform_augmentation.aug_rpy,\n",
    "            transform_augmentation_rot_resolution=cfg.method.transform_augmentation.aug_rot_resolution,\n",
    "            optimizer_type=cfg.method.optimizer,\n",
    "            num_devices=cfg.ddp.num_devices,\n",
    "            cfg=cfg.method,\n",
    "        )\n",
    "        qattention_agents.append(qattention_agent)\n",
    "\n",
    "    rotation_agent = QAttentionStackAgent(\n",
    "        qattention_agents=qattention_agents,\n",
    "        rotation_resolution=cfg.method.rotation_resolution,\n",
    "        camera_names=cfg.rlbench.cameras,\n",
    "    )\n",
    "    preprocess_agent = PreprocessAgent(\n",
    "        pose_agent=rotation_agent,\n",
    "        \n",
    "    )\n",
    "    return preprocess_agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "\n",
    "\n",
    "agent = create_agent(cfg)\n",
    "agent = copy.deepcopy(agent)\n",
    "agent.build(training=True, device=device, use_ddp=False,fabric = None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(data_iter, SILENT=True):\n",
    "    # try:\n",
    "    sampled_batch = next(data_iter) # may raise StopIteration\n",
    "    # print error and restart data iter\n",
    "    # except Exception as e:\n",
    "    #     cprint(e, 'red')\n",
    "    #     # FIXME: this is a pretty bad hack...\n",
    "    #     cprint(\"restarting data iter...\", 'red')\n",
    "    #     return self.preprocess_data(data_iter)\n",
    "    \n",
    "    batch = {k: v.to(device) for k, v in sampled_batch.items() if type(v) == torch.Tensor}\n",
    "    batch['nerf_multi_view_rgb'] = sampled_batch['nerf_multi_view_rgb'] # [bs, 1, 21]\n",
    "    batch['nerf_multi_view_depth'] = sampled_batch['nerf_multi_view_depth']\n",
    "    batch['nerf_multi_view_camera'] = sampled_batch['nerf_multi_view_camera'] # must!!!\n",
    "    batch['nerf_multi_view_mask'] = sampled_batch['nerf_multi_view_mask']\n",
    "    batch['lang_goal'] = sampled_batch['lang_goal']\n",
    "\n",
    "    if 'nerf_next_multi_view_rgb' in sampled_batch:\n",
    "        batch['nerf_next_multi_view_rgb'] = sampled_batch['nerf_next_multi_view_rgb']\n",
    "        batch['nerf_next_multi_view_depth'] = sampled_batch['nerf_next_multi_view_depth']\n",
    "        batch['nerf_next_multi_view_camera'] = sampled_batch['nerf_next_multi_view_camera']\n",
    "        batch['nerf_next_multi_view_mask'] = sampled_batch['nerf_next_multi_view_mask']\n",
    "    \n",
    "    if len(batch['nerf_multi_view_rgb'].shape) == 3:\n",
    "        batch['nerf_multi_view_rgb'] = batch['nerf_multi_view_rgb'].squeeze(1)\n",
    "        batch['nerf_multi_view_depth'] = batch['nerf_multi_view_depth'].squeeze(1)\n",
    "        batch['nerf_multi_view_camera'] = batch['nerf_multi_view_camera'].squeeze(1)\n",
    "        batch['nerf_multi_view_mask'] = batch['nerf_multi_view_mask'].squeeze(1)\n",
    "\n",
    "        if 'nerf_next_multi_view_rgb' in batch and batch['nerf_next_multi_view_rgb'] is not None:\n",
    "            batch['nerf_next_multi_view_rgb'] = batch['nerf_next_multi_view_rgb'].squeeze(1)\n",
    "            batch['nerf_next_multi_view_depth'] = batch['nerf_next_multi_view_depth'].squeeze(1)\n",
    "            batch['nerf_next_multi_view_camera'] = batch['nerf_next_multi_view_camera'].squeeze(1)\n",
    "            batch['nerf_next_multi_view_mask'] = batch['nerf_next_multi_view_mask'].squeeze(1)\n",
    "\n",
    "    # print(batch['nerf_multi_view_rgb'])\n",
    "    # print(batch['nerf_multi_view_rgb'].shape)\n",
    "\n",
    "\n",
    "    if batch['nerf_multi_view_rgb'] is None or batch['nerf_multi_view_rgb'][0,0] is None:\n",
    "        if not SILENT:\n",
    "            cprint('batch[nerf_multi_view_rgb] is None. find next data iter', 'red')\n",
    "        return preprocess_data(data_iter)\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13578/101000 [14:06<1:30:49, 16.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m     data_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(dataset)  \u001b[38;5;66;03m# recreate the iterator\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     batch \u001b[38;5;241m=\u001b[39m preprocess_data(data_iter)\n\u001b[0;32m---> 38\u001b[0m update_dict \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     41\u001b[0m     total_losses \u001b[38;5;241m=\u001b[39m update_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_losses\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[0;32mIn[12], line 78\u001b[0m, in \u001b[0;36mPreprocessAgent.update\u001b[0;34m(self, step, replay_sample, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m replay_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnerf_multi_view_rgb\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m replay_sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnerf_multi_view_rgb\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     cprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocess agent no nerf rgb 2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pose_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 46\u001b[0m, in \u001b[0;36mQAttentionStackAgent.update\u001b[0;34m(self, step, replay_sample, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     cprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack agent no nerf rgb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m qa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qattention_agents:\n\u001b[0;32m---> 46\u001b[0m     update_dict \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     replay_sample\u001b[38;5;241m.\u001b[39mupdate(update_dict)\n\u001b[1;32m     48\u001b[0m     total_losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m update_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[10], line 765\u001b[0m, in \u001b[0;36mQAttentionPerActBCAgent.update\u001b[0;34m(self, step, replay_sample, fabric)\u001b[0m\n\u001b[1;32m    762\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    763\u001b[0m \u001b[38;5;66;03m# use fabric ddp\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;66;03m# fabric.backward(total_loss)\u001b[39;00m\n\u001b[0;32m--> 765\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m############### Render in training process #################\u001b[39;00m\n\u001b[1;32m    769\u001b[0m render_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mneural_renderer\u001b[38;5;241m.\u001b[39mrender_freq\n",
      "File \u001b[0;32m~/anaconda3/envs/manigaussian/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Project/ManiGaussian/helpers/optim/lamb.py:99\u001b[0m, in \u001b[0;36mLamb.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     96\u001b[0m     adam_step\u001b[38;5;241m.\u001b[39madd_(p\u001b[38;5;241m.\u001b[39mdata, alpha\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     98\u001b[0m adam_norm \u001b[38;5;241m=\u001b[39m adam_step\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight_norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m adam_norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    100\u001b[0m     trust_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x709d8d6c1c10>> (for post_run_cell), with arguments args (<ExecutionResult object at 709d9148a820, execution_count=18 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 709d9148a5b0, raw_cell=\"\n",
      "import time\n",
      "import psutil\n",
      "from tqdm import tqdm\n",
      "i..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/home/bruce/Project/ManiGaussian/train_gs.ipynb#X52sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "wandb_name = 'gs_model_mask_10_demo_sim_to_colmap'\n",
    "\n",
    "wandb.init(project='gs', group=str(cfg.framework.wandb_group), name=wandb_name, config=cfg)\n",
    "\n",
    "dataset = wrapped_replay.dataset()    # <class 'torch.utils.data.dataloader.DataLoader'>\n",
    "\n",
    "\n",
    "data_iter = iter(dataset)\n",
    "\n",
    "num_cpu = psutil.cpu_count()    # e.g., 255\n",
    "print(f'num_cpu: {num_cpu}')\n",
    "\n",
    "start_iter = 0\n",
    "\n",
    "for i in tqdm(range(start_iter, 101000), mininterval=1):\n",
    "    # log_iteration = i % self._log_freq == 0 and i > 0\n",
    "\n",
    "#     if log_iteration:\n",
    "#         process.cpu_percent(interval=None)\n",
    "\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    try:\n",
    "        batch = preprocess_data(data_iter)\n",
    "    except StopIteration:\n",
    "        cprint('StopIteration', 'red')\n",
    "        data_iter = iter(dataset)  # recreate the iterator\n",
    "        batch = preprocess_data(data_iter)\n",
    "        \n",
    "\n",
    "\n",
    "    update_dict = agent.update(i, batch)\n",
    "    \n",
    "    if i % 100 == 0 and i > 0:\n",
    "        total_losses = update_dict['total_losses'].item()\n",
    "        # print(f'total_losses: {total_losses}')\n",
    "        # wandb.log({'total_losses':total_losses, \n",
    "        #         }, step=i)\n",
    "    # print('Batch:-----------------------------')\n",
    "    # for key, value in batch.items():\n",
    "    #     print(key, value.shape\n",
    "    #           )\n",
    "    # print(batch['front_point_cloud'].shape)\n",
    "    # plt.imshow(batch['front_point_cloud'][0][0].permute(1,2,0).cpu().detach().numpy())\n",
    "    # plt.show()\n",
    "    # print(batch['front_depth'].shape)\n",
    "    # plt.imshow(batch['front_depth'][0][0].permute(1,2,0).cpu().detach().numpy())\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# return total_losses\n",
    "    # step_time = time.time() - t\n",
    "\n",
    "    # if self._rank == 0:\n",
    "    #     if log_iteration:\n",
    "\n",
    "    #         logging.info(f\"Train Step {i:06d} | Loss: {loss:0.5f} | Step time: {step_time:0.4f} | CWD: {os.getcwd()}\")\n",
    "            \n",
    "\n",
    "    #     if i % self._save_freq == 0 and self._weightsdir is not None:\n",
    "    #         self._save_model(i)\n",
    "\n",
    "# if self._rank == 0:\n",
    "#     logging.info('Stopping envs ...')\n",
    "\n",
    "#     self._wrapped_buffer.replay_buffer.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OfflineTrainRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import signal\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from typing import Optional, List\n",
    "from typing import Union\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "import gc\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch\n",
    "import pandas as pd\n",
    "from yarr.agents.agent import Agent\n",
    "from yarr.replay_buffer.wrappers.pytorch_replay_buffer import \\\n",
    "    PyTorchReplayBuffer\n",
    "from yarr.runners.env_runner import EnvRunner\n",
    "from yarr.runners.train_runner import TrainRunner\n",
    "from yarr.utils.log_writer import LogWriter\n",
    "from yarr.utils.stat_accumulator import StatAccumulator\n",
    "from yarr.replay_buffer.prioritized_replay_buffer import PrioritizedReplayBuffer\n",
    "\n",
    "from termcolor import cprint\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from lightning.fabric import Fabric\n",
    "\n",
    "\n",
    "class OfflineTrainRunner():\n",
    "\n",
    "    def __init__(self,\n",
    "                 agent: Agent,\n",
    "                 wrapped_replay_buffer: PyTorchReplayBuffer,\n",
    "                 train_device: torch.device,\n",
    "                 stat_accumulator: Union[StatAccumulator, None] = None,\n",
    "                 iterations: int = int(6e6),\n",
    "                 logdir: str = '/tmp/yarr/logs',\n",
    "                 logging_level: int = logging.INFO,\n",
    "                 log_freq: int = 10,\n",
    "                 weightsdir: str = '/tmp/yarr/weights',\n",
    "                 num_weights_to_keep: int = 60,\n",
    "                 save_freq: int = 100,\n",
    "                 tensorboard_logging: bool = True,\n",
    "                 csv_logging: bool = False,\n",
    "                 load_existing_weights: bool = True,\n",
    "                 rank: int = None,\n",
    "                 world_size: int = None,\n",
    "                 cfg: DictConfig = None,\n",
    "                 fabric: Fabric = None):\n",
    "        self._agent = agent\n",
    "        self._wrapped_buffer = wrapped_replay_buffer\n",
    "        self._stat_accumulator = stat_accumulator\n",
    "        self._iterations = iterations\n",
    "        self._logdir = logdir\n",
    "        self._logging_level = logging_level\n",
    "        self._log_freq = log_freq\n",
    "        self._weightsdir = weightsdir\n",
    "        self._num_weights_to_keep = num_weights_to_keep\n",
    "        self._save_freq = save_freq\n",
    "\n",
    "        self._wrapped_buffer = wrapped_replay_buffer\n",
    "        self._train_device = train_device\n",
    "        self._tensorboard_logging = tensorboard_logging\n",
    "        self._csv_logging = csv_logging\n",
    "        self._load_existing_weights = load_existing_weights\n",
    "        self._rank = rank\n",
    "        self._world_size = world_size\n",
    "        self._fabric = fabric\n",
    "        \n",
    "        self.tqdm_mininterval = cfg.framework.tqdm_mininterval\n",
    "        self.use_wandb = cfg.framework.use_wandb\n",
    "\n",
    "        if self.use_wandb and rank == 0:\n",
    "            print(f\"wandb init in {cfg.framework.wandb_project}/{cfg.framework.wandb_group}/{cfg.framework.seed}\")\n",
    "            # wandb.init(project=cfg.framework.wandb_project, group=str(cfg.framework.wandb_group), name=str(cfg.framework.seed), \n",
    "            #         config=cfg)\n",
    "            wandb_name = str(cfg.framework.seed) if cfg.framework.wandb_name is None else cfg.framework.wandb_name\n",
    "            wandb.init(project=cfg.framework.wandb_project, group=str(cfg.framework.wandb_group), name=wandb_name, \n",
    "                    config=cfg)\n",
    "            cprint(f'[wandb] init in {cfg.framework.wandb_project}/{cfg.framework.wandb_group}/{wandb_name}', 'cyan')\n",
    "\n",
    "    \n",
    "        if weightsdir is None:\n",
    "            logging.info(\n",
    "                \"'weightsdir' was None. No weight saving will take place.\")\n",
    "        else:\n",
    "            os.makedirs(self._weightsdir, exist_ok=True)\n",
    "\n",
    "    def _save_model(self, i):\n",
    "        d = os.path.join(self._weightsdir, str(i))\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "        self._agent.save_weights(d)\n",
    "\n",
    "        # remove oldest save\n",
    "        prev_dir = os.path.join(self._weightsdir, str(\n",
    "            i - self._save_freq * self._num_weights_to_keep))\n",
    "        if os.path.exists(prev_dir):\n",
    "            shutil.rmtree(prev_dir)\n",
    "\n",
    "    def _step(self, i, sampled_batch, **kwargs):\n",
    "        update_dict = self._agent.update(i, sampled_batch, **kwargs)\n",
    "        total_losses = update_dict['total_losses'].item()\n",
    "        return total_losses\n",
    "\n",
    "    def _get_resume_eval_epoch(self):\n",
    "        starting_epoch = 0\n",
    "        eval_csv_file = self._weightsdir.replace('weights', 'eval_data.csv') # TODO(mohit): check if it's supposed be 'env_data.csv'\n",
    "        if os.path.exists(eval_csv_file):\n",
    "             eval_dict = pd.read_csv(eval_csv_file).to_dict()\n",
    "             epochs = list(eval_dict['step'].values())\n",
    "             return epochs[-1] if len(epochs) > 0 else starting_epoch\n",
    "        else:\n",
    "            return starting_epoch\n",
    "\n",
    "    def preprocess_data(self, data_iter, SILENT=True):\n",
    "        # try:\n",
    "        sampled_batch = next(data_iter) # may raise StopIteration\n",
    "        # print error and restart data iter\n",
    "        # except Exception as e:\n",
    "        #     cprint(e, 'red')\n",
    "        #     # FIXME: this is a pretty bad hack...\n",
    "        #     cprint(\"restarting data iter...\", 'red')\n",
    "        #     return self.preprocess_data(data_iter)\n",
    "        \n",
    "        batch = {k: v.to(self._train_device) for k, v in sampled_batch.items() if type(v) == torch.Tensor}\n",
    "        batch['nerf_multi_view_rgb'] = sampled_batch['nerf_multi_view_rgb'] # [bs, 1, 21]\n",
    "        batch['nerf_multi_view_depth'] = sampled_batch['nerf_multi_view_depth']\n",
    "        batch['nerf_multi_view_camera'] = sampled_batch['nerf_multi_view_camera'] # must!!!\n",
    "        batch['lang_goal'] = sampled_batch['lang_goal']\n",
    "\n",
    "        if 'nerf_next_multi_view_rgb' in sampled_batch:\n",
    "            batch['nerf_next_multi_view_rgb'] = sampled_batch['nerf_next_multi_view_rgb']\n",
    "            batch['nerf_next_multi_view_depth'] = sampled_batch['nerf_next_multi_view_depth']\n",
    "            batch['nerf_next_multi_view_camera'] = sampled_batch['nerf_next_multi_view_camera']\n",
    "        \n",
    "        if len(batch['nerf_multi_view_rgb'].shape) == 3:\n",
    "            batch['nerf_multi_view_rgb'] = batch['nerf_multi_view_rgb'].squeeze(1)\n",
    "            batch['nerf_multi_view_depth'] = batch['nerf_multi_view_depth'].squeeze(1)\n",
    "            batch['nerf_multi_view_camera'] = batch['nerf_multi_view_camera'].squeeze(1)\n",
    "\n",
    "            if 'nerf_next_multi_view_rgb' in batch and batch['nerf_next_multi_view_rgb'] is not None:\n",
    "                batch['nerf_next_multi_view_rgb'] = batch['nerf_next_multi_view_rgb'].squeeze(1)\n",
    "                batch['nerf_next_multi_view_depth'] = batch['nerf_next_multi_view_depth'].squeeze(1)\n",
    "                batch['nerf_next_multi_view_camera'] = batch['nerf_next_multi_view_camera'].squeeze(1)\n",
    "\n",
    "        # print(batch['nerf_multi_view_rgb'])\n",
    "        # print(batch['nerf_multi_view_rgb'].shape)\n",
    "\n",
    "\n",
    "        if batch['nerf_multi_view_rgb'] is None or batch['nerf_multi_view_rgb'][0,0] is None:\n",
    "            if not SILENT:\n",
    "                cprint('batch[nerf_multi_view_rgb] is None. find next data iter', 'red')\n",
    "            return self.preprocess_data(data_iter)\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    def start(self):\n",
    "        logging.getLogger().setLevel(self._logging_level)\n",
    "        self._agent = copy.deepcopy(self._agent)\n",
    "        # DDP setup model and optimizer\n",
    "        # print(f'_train_device:{self._train_device}')\n",
    "        if self._fabric is not None:\n",
    "            self._agent.build(training=True, device=self._train_device, fabric=self._fabric)\n",
    "        else:\n",
    "        \n",
    "            self._agent.build(training=True, device=self._train_device, use_ddp=False)\n",
    "\n",
    "        if self._weightsdir is not None:\n",
    "            existing_weights = sorted([int(f) for f in os.listdir(self._weightsdir)])\n",
    "            if (not self._load_existing_weights) or len(existing_weights) == 0:\n",
    "                self._save_model(0)\n",
    "                start_iter = 0\n",
    "            else:\n",
    "                resume_iteration = existing_weights[-1]\n",
    "                self._agent.load_weights(os.path.join(self._weightsdir, str(resume_iteration)))\n",
    "                start_iter = resume_iteration + 1\n",
    "                if self._rank == 0:\n",
    "                    logging.info(f\"load weights from {os.path.join(self._weightsdir, str(resume_iteration))} ...\")\n",
    "                    logging.info(f\"Resuming training from iteration {resume_iteration} ...\")\n",
    "\n",
    "        dataset = self._wrapped_buffer.dataset()    # <class 'torch.utils.data.dataloader.DataLoader'>\n",
    "\n",
    "        # DDP setup dataloader\n",
    "        if self._fabric is not None:\n",
    "            dataset = self._fabric.setup_dataloaders(dataset)\n",
    "\n",
    "        data_iter = iter(dataset)\n",
    "\n",
    "        process = psutil.Process(os.getpid())\n",
    "        num_cpu = psutil.cpu_count()    # e.g., 255\n",
    "\n",
    "        for i in tqdm(range(start_iter, self._iterations), mininterval=self.tqdm_mininterval):\n",
    "            log_iteration = i % self._log_freq == 0 and i > 0\n",
    "\n",
    "        #     if log_iteration:\n",
    "        #         process.cpu_percent(interval=None)\n",
    "\n",
    "\n",
    "            t = time.time()\n",
    "\n",
    "            try:\n",
    "                batch = self.preprocess_data(data_iter)\n",
    "            except StopIteration:\n",
    "                cprint('StopIteration', 'red')\n",
    "                data_iter = iter(dataset)  # recreate the iterator\n",
    "                batch = self.preprocess_data(data_iter)\n",
    "\n",
    "            loss = self._step(0, batch)\n",
    "            if log_iteration:\n",
    "                print(f'total_losses: {loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = create_agent(cfg)\n",
    "\n",
    "\n",
    "train_runner = OfflineTrainRunner(\n",
    "    agent=agent,\n",
    "    wrapped_replay_buffer=wrapped_replay,\n",
    "    train_device=rank,\n",
    "    stat_accumulator=stat_accum,\n",
    "    iterations=cfg.framework.training_iterations,\n",
    "    logdir=logdir,\n",
    "    logging_level=cfg.framework.logging_level,\n",
    "    log_freq=cfg.framework.log_freq,\n",
    "    weightsdir=weightsdir,\n",
    "    num_weights_to_keep=cfg.framework.num_weights_to_keep,\n",
    "    save_freq=cfg.framework.save_freq,\n",
    "    tensorboard_logging=cfg.framework.tensorboard_logging,\n",
    "    csv_logging=cfg.framework.csv_logging,\n",
    "    load_existing_weights=cfg.framework.load_existing_weights,\n",
    "    rank=rank,\n",
    "    world_size=world_size,\n",
    "    cfg=cfg,\n",
    "    fabric=fabric)\n",
    "cprint('Starting training!!', 'green')\n",
    "train_runner.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_runner\n",
    "del agent\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fabric = L.Fabric(devices=world_size, strategy='ddp')\n",
    "fabric.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manigaussian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
